{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Muhammad Luqman Hakim","text":"<p>\ud83c\udf93 Computer Science Student | \ud83e\udd16 Machine Learning Enthusiast \ud83d\udccd IPB University | \ud83d\udca1 Lifelong Learner  </p> <p>Welcome to my Machine Learning Journey \u2014 a space where I document the projects, lessons, and experiments I've explored as I deepen my understanding of data-driven systems.</p> <p>My focus lies in: - Building intelligent systems using neural networks and deep learning. - Solving real-world problems with predictive models. - Bridging theory and practice through hands-on experimentation.</p>"},{"location":"#highlights","title":"\ud83d\udd0d Highlights","text":"<ul> <li>\ud83d\udcc8 Applied time series models for financial data forecasting.</li> <li>\ud83d\udcdd Conducted sentiment analysis using NLP and TF-IDF/Embeddings.</li> </ul>"},{"location":"#deep-learning-but-linear-regression-logistic-regression-and-svm","title":"\ud83d\udcc4 Deep Learning but Linear Regression, Logistic Regression, and SVM,","text":"<p>\ud83d\udccc # Deep Learning but Linear Regression, Logistic Regression, and SVM This time we will: - build simple Linear Regression, Logistic Regression, and SVM but using neural network</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#default-dataset-estimation-with-lightning-pytorch","title":"\ud83d\udcc4 Default Dataset Estimation with Lightning Pytorch","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#islp-mpg-dataset-logistic-regression-lda-qda-knn-naive-bayes","title":"\ud83d\udcc4 ISLP mpg dataset - Logistic regression, LDA, QDA, KNN, Naive Bayes","text":"<p>\ud83d\udccc 14. in this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set. - (a) Create a binary variable, mpg01, that contains a 1 if mpg contains</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#linear-polynomial-radial-svm","title":"\ud83d\udcc4 Linear, polynomial, radial SVM","text":"<p>\ud83d\udccc 4. Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation be- tween the two classes. Show that in this setting, a support vector</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#linear-regression-from-scratch","title":"\ud83d\udcc4 Linear Regression from scratch","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#linear-regression-sklearn","title":"\ud83d\udcc4 Linear Regression sklearn","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#linearsvm-and-polynomsvm-from-scratch","title":"\ud83d\udcc4 LinearSVM and PolynomSVM from scratch","text":"<p>\ud83d\udccc # Linear and Polynom SVM from scratch We will create SVM using the loss function and do the gradient descend manually </p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#nyse-trading-volume-estimation-with-autoregression-rnn-and-fusion-of-lstm-dense","title":"\ud83d\udcc4 NYSE trading volume estimation with AutoRegression, RNN, and fusion of LSTM + Dense","text":"<p>\ud83d\udccc # NYSE trading volume estimation with AutoRegression, RNN, and fusion of LSTM + Dense This exercise is from Introduction of statistical analysis (ISLP) Section DeepLearning. </p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#polynomial-logistic-regression","title":"\ud83d\udcc4 Polynomial Logistic Regression","text":"<p>\ud83d\udccc 5. We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#regularization-with-lasso-and-ridge","title":"\ud83d\udcc4 Regularization with Lasso and Ridge","text":"<p>\ud83d\udccc # Regularization with Lasso and Ridge This is an exercise inspired from \"An Introduction to Statistical Learning\" book on section 6 problem 8. 1. First we will create our own data with uniform distribution and select our own coefficient Y = c1x + c2x^2 + c3x^3 + noise</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#regularizaton-hyperparameter-tuning-ridge-lasso-pcr-pls","title":"\ud83d\udcc4 Regularizaton Hyperparameter Tuning - Ridge, Lasso, PCR, PLS","text":"<p>\ud83d\udccc 9. In this exercise, we will predict the number of applications received using the other variables in the College data set. - (a) Split the data set into a training set and a test set.</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#sentiment-analysis-with-svm-and-binomial-nb-on-presidential-debate","title":"\ud83d\udcc4 Sentiment analysis with SVM and Binomial NB on Presidential Debate","text":"<p>\ud83d\udccc # Sentiment analysis with Logistic Regression, SVM, Binomial Naive Bayes, and Deep Learning on Presidential Debate This exercise derived from Data Mining project in my college but i remaster it with SVM and Deep Learning. My team scraped and manually label the data before, so we just need to do a little preprocess here. </p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#sklearn-logistic-regression-lda-qda-knn-naive-bayes","title":"\ud83d\udcc4 Sklearn - Logistic regression, LDA, QDA, KNN, Naive Bayes","text":"<p>\ud83d\udccc 13. This question should be answered using the Weekly data set, which is part of the ISLP package. This data is similar in nature to the Smarket data from this chapter\u2019s lab, except that it contains 1, 089</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#tree-bagging-boosting-with-hyperparameter-tuning","title":"\ud83d\udcc4 Tree, Bagging, Boosting with Hyperparameter tuning","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-applied4-14-cv","title":"\ud83d\udcc4 z applied4 14 CV","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-islp-demo10","title":"\ud83d\udcc4 z ISLP demo10","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-islp-demo4","title":"\ud83d\udcc4 z ISLP demo4","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-islp-demo6-2","title":"\ud83d\udcc4 z ISLP demo6 2","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-islp-demo6-3","title":"\ud83d\udcc4 z ISLP demo6 3","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-islp-demo6","title":"\ud83d\udcc4 z ISLP demo6","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-islp-demo8","title":"\ud83d\udcc4 z ISLP demo8","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"#z-islp-demo9","title":"\ud83d\udcc4 z ISLP demo9","text":"<p>\ud83d\udccc ```python import pandas as pd import numpy as np</p> <p>\ud83d\udd17 view markdown</p>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/","title":"Deep Learning but Linear Regression, Logistic Regression, and SVM","text":"<p>This time we will: - build simple Linear Regression, Logistic Regression, and SVM but using neural network - comparing the result from sklearn library and our own neural network version of those models - add hidden layer as 'feature extraction' to see any improvement (and justify the title xd)</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib.pyplot import subplots\nfrom ISLP import load_data\n</code></pre> <pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import Adam, RMSprop, SGD\nfrom torchmetrics import MeanSquaredError, R2Score\nfrom torchmetrics.classification import BinaryAccuracy, MulticlassAccuracy\nfrom torchinfo import summary\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n</code></pre> <pre><code>import sklearn.model_selection as skm\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score, accuracy_score, mean_absolute_error\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#dataset-for-regression-and-classification","title":"Dataset for regression and classification","text":"<p>We will be using hitters dataset from ISLP library which about baseball player game statistic: - regression: predict salary based on player statistic - classification: predict if salary is above or below median</p> <p>Preprocessing: - remove null value - handle categorical data with dummy variable - it IMPORTANT to drop the first category because of perfect multicolinearity - standarization</p> <pre><code>hitters = load_data('Hitters')\nhitters.head()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague 0 293 66 1 30 29 14 1 293 66 1 30 29 14 A E 446 33 20 NaN A 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N 4 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N <pre><code>hitters = hitters.dropna()\nhitters = pd.get_dummies(hitters, columns=['League', 'Division', 'NewLeague'], drop_first=True)\nhitters.head()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks PutOuts Assists Errors Salary League_N Division_W NewLeague_N 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 632 43 10 475.0 True True True 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 880 82 14 480.0 False True False 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 200 11 3 500.0 True False True 4 321 87 10 39 42 30 2 396 101 12 48 46 33 805 40 4 91.5 True False True 5 594 169 4 74 51 35 11 4408 1133 19 501 336 194 282 421 25 750.0 False True False <pre><code>X_reg = hitters.drop(columns=[\"Salary\"])\ny_reg = hitters[\"Salary\"]\nX_train, X_test, y_train, y_test = skm.train_test_split(X_reg, y_reg, shuffle=True, random_state=0)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#generic-model-for-our-task","title":"Generic Model for our task","text":"<p>We will use this model for our task, this is very straightforward model with only input layer and one output unit. The only main thing that we need to think about is the different loss function for Linear Regression, Logistic Regression, and SVM. There is also simple hidden layer with 32 unit that we will compare the performance later.</p> <pre><code>class Model(pl.LightningModule):\n    def __init__(self, loss_fn, input_size, hidden_layer=False, lr=0.01):\n        super().__init__()\n        self.loss_fn = loss_fn\n        self.input_size = input_size\n        self.lr = lr\n        if hidden_layer:\n            self.model = nn.Sequential(\n                nn.Linear(input_size, 32),\n                nn.ReLU(),\n                nn.Linear(32, 1)\n            )\n        else:\n            self.model = nn.Sequential(\n                nn.Linear(input_size, 1)\n            )\n\n    def forward(self, x):\n        return self.model(x).squeeze()\n\n    def _shared_step(self, batch, stage):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y.float())\n        self.log(f'{stage}_loss', loss, on_epoch=True, on_step=False, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self._shared_step(batch, 'train')\n\n    def validation_step(self, batch, batch_idx):\n        self._shared_step(batch, 'val')\n\n    def predict_step(self, batch, batch_idx):\n        x, _ = batch\n        return self(x)\n\n    def configure_optimizers(self):\n        return SGD(self.parameters(), lr=self.lr)\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, train_td, val_td, batch_size=64, num_workers=8):\n        super().__init__()\n        self.train_td = train_td\n        self.val_td = val_td\n        self.batch_size = batch_size\n        self.num_workers=num_workers\n\n    def train_dataloader(self):\n        return DataLoader(self.train_td, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_td, batch_size=self.batch_size, num_workers=self.num_workers)    \n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#linear-regression-with-sklearn","title":"Linear Regression with sklearn","text":"<p>This is the default linear regression model from sklearn. It got MAE about 265 and 0.55 R2 score. For reference the std of our response is around 451 so it did pretty good with MAE almost half of std.</p> <pre><code>lr = LinearRegression()\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\nmean_absolute_error(y_test, lr_pred), r2_score(y_test, lr_pred)\n</code></pre> <pre><code>(265.87464986111604, 0.5531784057871476)\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#linear-regression-with-our-nn-model","title":"Linear Regression with our NN model","text":"<p>This is the our NN model which use MSELoss. The result is bad here, the validation and train loss get stuck at some point which adding hidden layer maybe helpful.</p> <pre><code>X_train_t = torch.tensor(X_train.astype(np.float32))\nX_test_t = torch.tensor(X_test.astype(np.float32))\ny_train_t = torch.tensor(y_train.to_numpy().astype(np.float32))\ny_test_t = torch.tensor(y_test.to_numpy().astype(np.float32))\n\ntrain_lr_ds = TensorDataset(X_train_t, y_train_t)\ntest_lr_ds = TensorDataset(X_test_t, y_test_t)\n</code></pre> <pre><code>lr_loss_fn = nn.MSELoss()\n# lr_logger = CSVLogger('logs', name='linear_regression')\nlr_model = Model(lr_loss_fn, X_train.shape[1])\nlr_dm = DataModule(train_lr_ds, test_lr_ds, batch_size=32)\nlr_trainer = Trainer(max_epochs=100)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>lr_trainer.fit(lr_model, lr_dm)\n</code></pre> <pre><code>  | Name    | Type       | Params | Mode \n-----------------------------------------------\n0 | loss_fn | MSELoss    | 0      | train\n1 | model   | Sequential | 20     | train\n-----------------------------------------------\n20        Trainable params\n0         Non-trainable params\n20        Total params\n0.000     Total estimated model params size (MB)\n3         Modules in train mode\n0         Modules in eval mode\n\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n`Trainer.fit` stopped: `max_epochs=100` reached.\n</code></pre> <pre><code>lr_dl_pred = np.concatenate(lr_trainer.predict(lr_model, lr_dm.val_dataloader()))\nmean_absolute_error(y_test, lr_dl_pred), r2_score(y_test, lr_dl_pred)\n</code></pre> <pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]\n\n(314.38981907700054, 0.4528384642986234)\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#not-so-linear-regression-with-hidden-layer","title":"Not so 'Linear' Regression with hidden layer","text":"<p>We will try adding hidden layer here to compromise with bad result that we get previously. I learn about exploding gradient while running this, so i set the learning rate very low but with more epoch. Sometime the loss function get stuck around the same point loss value with our previous model. But this time the model escape that local minimum and get much better result than sklearn model at around 224 MAE and 0.68 R2 score.</p> <pre><code>lr_loss_fn = nn.MSELoss()\nlr_model = Model(lr_loss_fn, X_train.shape[1], hidden_layer=True, lr=0.00005)\nlr_dm = DataModule(train_lr_ds, test_lr_ds, batch_size=64)\nlr_trainer = Trainer(max_epochs=300)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>lr_trainer.fit(lr_model, lr_dm)\n</code></pre> <pre><code>  | Name    | Type       | Params | Mode \n-----------------------------------------------\n0 | loss_fn | MSELoss    | 0      | train\n1 | model   | Sequential | 673    | train\n-----------------------------------------------\n673       Trainable params\n0         Non-trainable params\n673       Total params\n0.003     Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n\n`Trainer.fit` stopped: `max_epochs=300` reached.\n</code></pre> <pre><code>lr_dlh_pred = np.concatenate(lr_trainer.predict(lr_model, lr_dm.val_dataloader()))\nmean_absolute_error(y_test, lr_dlh_pred), r2_score(y_test, lr_dlh_pred)\n</code></pre> <pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]\n\n(224.00030996264832, 0.6806740806956346)\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#logistic-regression-with-sklearn","title":"Logistic Regression with sklearn","text":"<p>This time we will change the response to True or False, is the salary is higher or lower than median salary. The sklearn Logistic Regression model already giving a good start at around 81.8% accuracy.</p> <pre><code>X_cls = hitters.drop(columns=[\"Salary\"])\ny_cls = hitters[\"Salary\"] &gt; hitters[\"Salary\"].median()\nX_train, X_test, y_train, y_test = skm.train_test_split(X_cls, y_cls, shuffle=True, random_state=0)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n</code></pre> <pre><code>lg = LogisticRegression().fit(X_train, y_train)\nlg_pred = lg.predict(X_test)\naccuracy_score(y_test, lg_pred)\n</code></pre> <pre><code>0.8181818181818182\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#logistic-regression-with-our-nn-model","title":"Logistic Regression with our NN model","text":"<p>We will try to recreate the idea of Logistic Regression model with our NN model. The key is to use this BCEWithLogitLoss which will do sigmoid for us and use binary cross entropy as loss function. We only provide the logits in output unit so to predict our test data we need to do sigmoid first to map the logit range to probability.</p> <p>The result is a bit better with 83.3% accuracy</p> <pre><code>X_train_t = torch.tensor(X_train.astype(np.float32), dtype=torch.float32)\nX_test_t = torch.tensor(X_test.astype(np.float32), dtype=torch.float32)\ny_train_t = torch.tensor(y_train.to_numpy())\ny_test_t = torch.tensor(y_test.to_numpy())\n\ntrain_lg_ds = TensorDataset(X_train_t, y_train_t)\ntest_lg_ds = TensorDataset(X_test_t, y_test_t)\n</code></pre> <pre><code>lg_loss_fn = nn.BCEWithLogitsLoss()\nlg_model = Model(lg_loss_fn, X_train.shape[1], lr=0.01)\nlg_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)\nlg_trainer = Trainer(max_epochs=100)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>lg_trainer.fit(lg_model, lg_dm)\n</code></pre> <pre><code>  | Name    | Type              | Params | Mode \n------------------------------------------------------\n0 | loss_fn | BCEWithLogitsLoss | 0      | train\n1 | model   | Sequential        | 20     | train\n------------------------------------------------------\n20        Trainable params\n0         Non-trainable params\n20        Total params\n0.000     Total estimated model params size (MB)\n3         Modules in train mode\n0         Modules in eval mode\n\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n`Trainer.fit` stopped: `max_epochs=100` reached.\n</code></pre> <pre><code>lg_dl_pred = np.concatenate(lg_trainer.predict(lg_model, lg_dm.val_dataloader()))\nlg_dl_pred = torch.sigmoid(torch.tensor(lg_dl_pred)).numpy()\naccuracy_score(y_test, np.where(lg_dl_pred&gt;0.5, 1, 0))\n</code></pre> <pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]\n\n0.8333333333333334\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#adding-hidden-layer-to-our-logistic-nn","title":"Adding hidden layer to our Logistic NN","text":"<p>We will add hidden layer this time and the result is the same as before. We will try to do another method for this classification task, that is using SVM</p> <pre><code>lg_loss_fn = nn.BCEWithLogitsLoss()\nlg_metrics = BinaryAccuracy()\nlg_logger = CSVLogger('logs', name='logistic_regression')\nlg_model = Model(lg_loss_fn, X_train.shape[1], hidden_layer=True, lr=0.01)\nlg_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)\nlg_trainer = Trainer(max_epochs=100)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>lg_trainer.fit(lg_model, lg_dm)\n</code></pre> <pre><code>  | Name    | Type              | Params | Mode \n------------------------------------------------------\n0 | loss_fn | BCEWithLogitsLoss | 0      | train\n1 | model   | Sequential        | 673    | train\n------------------------------------------------------\n673       Trainable params\n0         Non-trainable params\n673       Total params\n0.003     Total estimated model params size (MB)\n5         Modules in train mode\n0         Modules in eval mode\n\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n`Trainer.fit` stopped: `max_epochs=100` reached.\n</code></pre> <pre><code>lg_dl_pred = np.concatenate(lg_trainer.predict(lg_model, lg_dm.val_dataloader()))\nlg_dl_pred = torch.sigmoid(torch.tensor(lg_dl_pred)).numpy()\naccuracy_score(y_test, np.where(lg_dl_pred&gt;0.5, 1, 0))\n</code></pre> <pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]\n\n0.8333333333333334\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#svm-with-sklearn","title":"SVM with sklearn","text":"<p>Logistic regression for this classification task result around 83%. This time we will take different approach that is using support vector machine to split the feature space to two different region. The sklearn linear SVM result around 80 accuracy.</p> <pre><code>svc = SVC(kernel='linear').fit(X_train, y_train)\nsvc_pred = svc.predict(X_test)\naccuracy_score(y_test, np.where(svc_pred&gt;0.5, 1, 0))\n</code></pre> <pre><code>0.803030303030303\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#svm-with-our-nn-model","title":"SVM with our NN model","text":"<p>We will use our NN to build SVM, but we need to define our own hinge loss as the loss function for this task. One thing that i learn is torch can do backprop even without their nn.loss function. The decision is the same with SMV, if the output is positive then it's above median and vice versa. The result without hidden layer is 81.8% accuracy.</p> <pre><code>def hinge_loss(outputs, labels):\n    labels = labels.float()\n    labels = labels * 2 - 1\n    loss = torch.mean(torch.clamp(1 - outputs.view(-1) * labels, min=0))\n    return loss\n</code></pre> <pre><code>svm_loss_fn = hinge_loss\nsvm_model = Model(svm_loss_fn, X_train.shape[1], lr=0.01)\nsvm_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)\nsvm_trainer = Trainer(max_epochs=100)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>svm_trainer.fit(svm_model, svm_dm)\n</code></pre> <pre><code>  | Name  | Type       | Params | Mode \n---------------------------------------------\n0 | model | Sequential | 20     | train\n---------------------------------------------\n20        Trainable params\n0         Non-trainable params\n20        Total params\n0.000     Total estimated model params size (MB)\n2         Modules in train mode\n0         Modules in eval mode\n\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n`Trainer.fit` stopped: `max_epochs=100` reached.\n</code></pre> <pre><code>svm_dl_pred = np.concatenate(svm_trainer.predict(svm_model, svm_dm.val_dataloader()))\nsvm_dl_pred = torch.tensor(svm_dl_pred).numpy()\naccuracy_score(y_test, np.where(svm_dl_pred&gt;0, 1, 0))\n</code></pre> <pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]\n\n0.8181818181818182\n</code></pre>"},{"location":"Deep%20Learning%20but%20Linear%20Regression%2C%20Logistic%20Regression%2C%20and%20SVM%2C%20/#adding-hidden-layer-to-our-svm-model","title":"Adding hidden layer to our SVM model","text":"<p>Adding hidden layer in this case doesn't improve our model just like in logistic regression case. Overfitting may be the cause, as we dont do any regularization in our model</p> <pre><code>svm_loss_fn = hinge_loss\nsvm_model = Model(svm_loss_fn, X_train.shape[1], lr=0.01, hidden_layer=True)\nsvm_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)\nsvm_trainer = Trainer(max_epochs=100)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>svm_trainer.fit(svm_model, svm_dm)\n</code></pre> <pre><code>  | Name  | Type       | Params | Mode \n---------------------------------------------\n0 | model | Sequential | 673    | train\n---------------------------------------------\n673       Trainable params\n0         Non-trainable params\n673       Total params\n0.003     Total estimated model params size (MB)\n4         Modules in train mode\n0         Modules in eval mode\n\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n`Trainer.fit` stopped: `max_epochs=100` reached.\n</code></pre> <pre><code>svm_dl_pred = np.concatenate(svm_trainer.predict(svm_model, svm_dm.val_dataloader()))\nsvm_dl_pred = torch.tensor(svm_dl_pred).numpy()\naccuracy_score(y_test, np.where(svm_dl_pred&gt;0, 1, 0))\n</code></pre> <pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]\n\n0.803030303030303\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Default%20Dataset%20Estimation%20with%20Lightning%20Pytorch/","title":"Default Dataset Estimation with Lightning Pytorch","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib.pyplot import subplots\nfrom ISLP import load_data\n</code></pre> <pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import Adam, RMSprop\nfrom torchmetrics import MeanSquaredError\nfrom torchmetrics.classification import BinaryAccuracy, MulticlassAccuracy\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n</code></pre> <pre><code>import sklearn.model_selection as skm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code>d = load_data('Default')\nd.head(), d.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 4 columns):\n #   Column   Non-Null Count  Dtype   \n---  ------   --------------  -----   \n 0   default  10000 non-null  category\n 1   student  10000 non-null  category\n 2   balance  10000 non-null  float64 \n 3   income   10000 non-null  float64 \ndtypes: category(2), float64(2)\nmemory usage: 176.2 KB\n\n(  default student      balance        income\n 0      No      No   729.526495  44361.625074\n 1      No     Yes   817.180407  12106.134700\n 2      No      No  1073.549164  31767.138947\n 3      No      No   529.250605  35704.493935\n 4      No      No   785.655883  38463.495879,\n None)\n</code></pre> <pre><code>default = pd.get_dummies(d, columns=['default', 'student'], drop_first=True)\nX = default.drop(columns=['default_Yes'])\ny = default['default_Yes']\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, shuffle=True, random_state=0)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n</code></pre> <pre><code>lr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\n(lr_pred == y_test).mean()\n</code></pre> <pre><code>0.9708\n</code></pre> <pre><code>X_train_t = torch.tensor(X_train.astype(np.float32))\nX_test_t = torch.tensor(X_test.astype(np.float32))\ny_train_t = torch.tensor(y_train.to_numpy().astype(np.float32))\ny_test_t = torch.tensor(y_test.to_numpy().astype(np.float32))\n\ndefault_train = TensorDataset(X_train_t, y_train_t)\ndefault_test = TensorDataset(X_test_t, y_test_t)\n</code></pre> <pre><code>class DefaultModel(pl.LightningModule):\n    def __init__(self, input_size):\n        super().__init__()\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = BinaryAccuracy()\n        self.learning_rate = 0.001\n        self.model = nn.Sequential(\n            nn.Linear(input_size, 10),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(10, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x).squeeze()\n\n    def step(self, batch, batch_idx, mode='train'):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y)\n        self.log(f'{mode}_loss', loss, on_epoch=True, on_step=False, prog_bar=True)\n        self.log(f'{mode}_acc', self.accuracy(preds, y), on_epoch=True, on_step=False, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx)\n\n    def validation_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'val')\n\n    def test_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'test')\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=self.learning_rate)\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nNameError                                 Traceback (most recent call last)\n\nCell In[1], line 1\n----&gt; 1 class DefaultModel(pl.LightningModule):\n      2     def __init__(self, input_size):\n      3         super().__init__()\n\nNameError: name 'pl' is not defined\n</code></pre> <pre><code>class DefaultDataModule(pl.LightningDataModule):\n    def __init__(self, train_td, test_td, batch_size=32, num_workers=8):\n        super().__init__()\n        self.train_td = train_td\n        self.test_td = test_td\n        self.batch_size = batch_size\n        self.num_workers=num_workers\n\n    def train_dataloader(self):\n        return DataLoader(self.train_td, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self):\n        return self.test_dataloader()\n\n    def test_dataloader(self):\n        return DataLoader(self.test_td, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre> <pre><code>default_model = DefaultModel(X_train_t.shape[1])\ndefault_dm = DefaultDataModule(default_train, default_test)\ndefault_logger = TensorBoardLogger('logs', name='default')\ndefault_trainer = Trainer(\n    deterministic=True, \n    max_epochs=30, \n    logger=default_logger,\n    callbacks=EarlyStopping(monitor='val_acc', patience=5, mode='min'))\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>default_trainer.fit(default_model, datamodule=default_dm) \n</code></pre> <pre><code>  | Name     | Type              | Params | Mode \n-------------------------------------------------------\n0 | loss_fn  | BCEWithLogitsLoss | 0      | train\n1 | accuracy | BinaryAccuracy    | 0      | train\n2 | model    | Sequential        | 51     | train\n-------------------------------------------------------\n51        Trainable params\n0         Non-trainable params\n51        Total params\n0.000     Total estimated model params size (MB)\n7         Modules in train mode\n0         Modules in eval mode\n</code></pre> <pre><code>default_trainer.test(default_model, datamodule=default_dm)\n</code></pre> <pre><code>Testing: |                                                | 0/? [00:00&lt;?, ?it/s]\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_acc            0.9616000056266785\n        test_loss           0.10796656459569931\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'test_loss': 0.10796656459569931, 'test_acc': 0.9616000056266785}]\n</code></pre> <pre><code>\n</code></pre> balance income student_Yes 0 729.526495 44361.625074 False 1 817.180407 12106.134700 True 2 1073.549164 31767.138947 False 3 529.250605 35704.493935 False 4 785.655883 38463.495879 False ... ... ... ... 9995 711.555020 52992.378914 False 9996 757.962918 19660.721768 False 9997 845.411989 58636.156984 False 9998 1569.009053 36669.112365 False 9999 200.922183 16862.952321 True <p>10000 rows \u00d7 3 columns</p> <pre><code>\n</code></pre>"},{"location":"ISLP%20mpg%20dataset%20-%20Logistic%20regression%2C%20LDA%2C%20QDA%2C%20KNN%2C%20Naive%20Bayes/","title":"ISLP mpg dataset   Logistic regression, LDA, QDA, KNN, Naive Bayes","text":"<ol> <li>in this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.</li> <li>(a) Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() method of the data frame. Note you may find it helpful to add a column mpg01 to the data frame by assignment. Assuming you have stored the data frame as Auto, this can be done as follows: Auto['mpg01 '] = mpg01</li> <li>(b) Explore the data graphically in order to investigate the associ- ation between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scat- terplots and boxplots may be useful tools to answer this ques- tion. Describe your findings.</li> <li>(c) Split the data into a training set and a test set.</li> <li>(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?</li> <li>(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?</li> <li>(f) Perform logistic regression on the training data in order to pre- dict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?</li> <li>(g) Perform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?</li> <li>(h) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?</li> </ol> <pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\n</code></pre> <pre><code>from ISLP import load_data, confusion_table\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n</code></pre> <pre><code>auto = load_data(\"Auto\")\nauto = auto.reset_index()\nauto\n</code></pre> name mpg cylinders displacement horsepower weight acceleration year origin 0 chevrolet chevelle malibu 18.0 8 307.0 130 3504 12.0 70 1 1 buick skylark 320 15.0 8 350.0 165 3693 11.5 70 1 2 plymouth satellite 18.0 8 318.0 150 3436 11.0 70 1 3 amc rebel sst 16.0 8 304.0 150 3433 12.0 70 1 4 ford torino 17.0 8 302.0 140 3449 10.5 70 1 ... ... ... ... ... ... ... ... ... ... 387 ford mustang gl 27.0 4 140.0 86 2790 15.6 82 1 388 vw pickup 44.0 4 97.0 52 2130 24.6 82 2 389 dodge rampage 32.0 4 135.0 84 2295 11.6 82 1 390 ford ranger 28.0 4 120.0 79 2625 18.6 82 1 391 chevy s-10 31.0 4 119.0 82 2720 19.4 82 1 <p>392 rows \u00d7 9 columns</p> <pre><code>auto['mpg01'] = np.where(auto['mpg'] &gt; auto['mpg'].median(), 1, 0)\n</code></pre> <pre><code>sns.pairplot(auto, hue='mpg01')\n</code></pre> <pre><code>&lt;seaborn.axisgrid.PairGrid at 0x73ee4c894f50&gt;\n</code></pre> <p></p> <pre><code>useful_features = [ 'displacement', 'weight']\nscaler = StandardScaler()\nscaler.fit(auto[useful_features])\nX = scaler.transform(auto[useful_features])\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = train_test_split(X, auto['mpg01'], stratify=auto['mpg01'], random_state=42)\n# X_train, X_test, y_train, y_test = train_test_split(auto[useful_features], auto['mpg01'])\n</code></pre> <pre><code>lda = LDA()\nlda.fit(X_train, y_train)\nlda_pred = lda.predict(X_test)\nconfusion_table(lda_pred, y_test)\n</code></pre> Truth 0 1 Predicted 0 42 1 1 7 48 <pre><code>qda = QDA()\nqda.fit(X_train, y_train)\nqda_pred = qda.predict(X_test)\nconfusion_table(qda_pred, y_test)\n</code></pre> Truth 0 1 Predicted 0 45 1 1 4 48 <pre><code>lr = LogisticRegression(C=10e5)\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\nconfusion_table(lr_pred, y_test)\n</code></pre> Truth 0 1 Predicted 0 44 3 1 5 46 <pre><code>nb = GaussianNB()\nnb.fit(X_train, y_train)\nnb_pred = nb.predict(X_test)\nconfusion_table(nb_pred, y_test)\n</code></pre> Truth 0 1 Predicted 0 43 2 1 6 47 <pre><code>for i in range(1, 2):\n    print(i)\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    knn_pred = knn.predict(X_test)\n    print(confusion_table(knn_pred, y_test))\n    print()\n</code></pre> <pre><code>1\nTruth       0   1\nPredicted        \n0          43  12\n1           6  37\n</code></pre> <pre><code>print(\"LDA\", (lda_pred == y_test).mean())\nprint(\"QDA\", (qda_pred == y_test).mean())\nprint(\"NB\", (nb_pred == y_test).mean())\nprint(\"LR\", (lr_pred == y_test).mean())\n</code></pre> <pre><code>LDA 0.9183673469387755\nQDA 0.9489795918367347\nNB 0.9183673469387755\nLR 0.9183673469387755\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Linear%20Regression%20from%20scratch/","title":"Linear Regression from scratch","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\n</code></pre> <pre><code>data = fetch_california_housing()\n</code></pre> <pre><code>df = pd.DataFrame(data.data, columns=data.feature_names)\ndf[\"Target\"] = data.target\n</code></pre> <pre><code>df = df.head(10000)\ndf\n</code></pre> MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude Target 0 8.3252 41.0 6.984127 1.023810 322.0 2.555556 37.88 -122.23 4.526 1 8.3014 21.0 6.238137 0.971880 2401.0 2.109842 37.86 -122.22 3.585 2 7.2574 52.0 8.288136 1.073446 496.0 2.802260 37.85 -122.24 3.521 3 5.6431 52.0 5.817352 1.073059 558.0 2.547945 37.85 -122.25 3.413 4 3.8462 52.0 6.281853 1.081081 565.0 2.181467 37.85 -122.25 3.422 ... ... ... ... ... ... ... ... ... ... 9995 4.0775 10.0 6.140900 1.025440 1275.0 2.495108 39.14 -121.03 1.645 9996 4.0848 8.0 6.350394 1.091864 1977.0 2.594488 39.13 -121.07 1.559 9997 3.6333 7.0 7.243455 1.107330 1143.0 2.992147 39.11 -121.05 1.702 9998 3.4630 8.0 6.363636 1.166297 1307.0 2.898004 39.08 -121.04 2.017 9999 3.0781 7.0 5.487500 1.050000 246.0 3.075000 39.09 -121.00 1.625 <p>10000 rows \u00d7 9 columns</p> <pre><code>df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      10000 non-null  float64\n 1   HouseAge    10000 non-null  float64\n 2   AveRooms    10000 non-null  float64\n 3   AveBedrms   10000 non-null  float64\n 4   Population  10000 non-null  float64\n 5   AveOccup    10000 non-null  float64\n 6   Latitude    10000 non-null  float64\n 7   Longitude   10000 non-null  float64\n 8   Target      10000 non-null  float64\ndtypes: float64(9)\nmemory usage: 703.3 KB\n</code></pre> <pre><code>df.describe()\n</code></pre> MedInc HouseAge AveRooms AveBedrms Population AveOccup Latitude Longitude Target count 10000.000000 10000.000000 10000.000000 10000.000000 10000.000000 10000.000000 10000.000000 10000.000000 10000.00000 mean 3.718040 31.847100 5.212102 1.090389 1395.588700 3.061855 35.493820 -119.472328 2.04949 std 1.916912 11.821967 2.752832 0.547035 1090.838717 6.098183 1.959545 1.808913 1.16595 min 0.499900 1.000000 0.846154 0.500000 3.000000 0.750000 32.670000 -124.350000 0.14999 25% 2.411900 23.000000 4.253385 1.007078 779.750000 2.452830 34.010000 -121.590000 1.17975 50% 3.328900 33.000000 5.031476 1.049645 1137.500000 2.851168 34.170000 -118.410000 1.76600 75% 4.544825 40.000000 5.830935 1.097466 1687.000000 3.373184 37.630000 -118.210000 2.58025 max 15.000100 52.000000 141.909091 34.066667 28566.000000 599.714286 41.950000 -114.550000 5.00001 <pre><code>sns.pairplot(df.head(1000), hue=\"Target\", diag_kind=\"kde\")\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(figsize=(10, 6))\nsns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\nplt.show()\n</code></pre> <p></p> <pre><code>class LinearRegression:\n    def __init__(self, lmbd=0.1, lr=0.1):\n        self.lmbd = lmbd\n        self.theta = 0\n        self.theta_0 = 0\n        self.lr = lr\n\n    def gradient_fit(self, X_train, Y_train, max_iter=1000):\n        self.theta = np.zeros(X_train.shape[1])\n\n        history = []\n        for _ in range(max_iter):\n            pred = np.dot(X_train, self.theta) + self.theta_0\n            err = Y_train - pred\n            gradient_theta_0 = -np.mean(err)\n            gradient_theta = -np.dot(X_train.T, err) / X_train.shape[0] + self.lmbd * self.theta\n\n            self.theta_0 -= self.lr * gradient_theta_0\n            self.theta -= self.lr * gradient_theta\n            # history.append([self.theta_0] + self.theta)\n        return self.theta_0, self.theta\n\n    def closed_form_fit(self, X_train, Y_train):\n        X = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) \n        XTX = np.matmul(X.T, X)\n        R = np.identity(XTX.shape[0]) * self.lmbd\n        R[0, 0] = 0\n        inv = np.linalg.inv(XTX + R)\n        XTY = np.matmul(X.T, Y_train)\n        theta = np.matmul(inv, XTY)\n        self.theta_0 = theta[:1]\n        self.theta = theta[1:]\n        return self.theta_0, self.theta\n\n    def predict(self, X_test):\n        pred = np.dot(X_test, self.theta) + self.theta_0\n        return pred\n        # print(Y_test - pred)\n        # return np.mean(np.square(Y_test - pred))\n\n    def MSE(self, X_test, Y_test):\n        # print(Y_test - pred)\n        return np.mean(np.square(Y_test - self.predict(X_test)))\n\n    def score(self, X_train, Y_train):\n        pred = self.predict(X_train)\n        TSS = np.sum(np.square(Y_train - np.mean(Y_train)))\n        RSS = np.sum(np.square(pred - Y_train))\n        return 1 - RSS/TSS\n\n    def residual_plot(self, X_test, Y_test):\n        pred = self.predict(X_test)\n        residuals = Y_test - pred\n\n        fig, ax = plt.subplots(figsize=(8, 6))\n        ax.scatter(pred, residuals, color=\"blue\", alpha=0.5, label=\"Residuals\", s=1)\n        ax.axhline(y=0, color='red', linestyle=\"--\") \n        ax.set_xlabel(\"Predicted Values\")\n        ax.set_ylabel(\"Residuals\")\n        ax.set_title(\"Residual Plot\")\n        ax.legend()\n        plt.show()\n</code></pre> <pre><code># X = df[[\"MedInc\"]]\nX = df.drop(columns=\"Target\")\nY = df[\"Target\"]\n</code></pre> <pre><code>X = (X - X.mean()) / X.std()\nY = (Y - Y.mean()) / Y.std()\n</code></pre> <pre><code>train = X.index % 2 == 0\ntest = X.index % 2 == 1\nX_train = X[train]\nY_train = Y[train]\nX_test = X[test]\nY_test = Y[test]\n</code></pre> <pre><code>X_train = np.array(X_train)\nY_train = np.array(Y_train)\nX_test = np.array(X_test)\nY_test = np.array(Y_test)\n</code></pre> <pre><code>model = LinearRegression(0.1)\n</code></pre> <pre><code>model.closed_form_fit(X_train, Y_train)\nprint(model.MSE(X_test, Y_test))\nprint(model.score(X_train, Y_train))\nmodel.residual_plot(X_test, Y_test)\n</code></pre> <pre><code>0.41151109506529826\n0.6059167915575893\n</code></pre> <p></p> <pre><code>model.gradient_fit(X_train, Y_train)\nmodel.MSE(X_test, Y_test)\nprint(model.MSE(X_test, Y_test))\nprint(model.score(X_train, Y_train))\n</code></pre> <pre><code>0.43518215741620114\n0.5710105260707274\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Linear%20Regression%20sklearn/","title":"Linear Regression sklearn","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n</code></pre> <pre><code>from statsmodels.stats.outliers_influence \\\n     import variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\n\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\n                         summarize,\n                         poly)\n</code></pre> <pre><code>df = pd.read_csv(\"../dataset/Auto.csv\", na_values='?')\ndf = df.dropna()\ndf = df.iloc[20:,:]\ndf\n</code></pre> mpg cylinders displacement horsepower weight acceleration year origin name 20 25.0 4 110.0 87.0 2672 17.5 70 2 peugeot 504 21 24.0 4 107.0 90.0 2430 14.5 70 2 audi 100 ls 22 25.0 4 104.0 95.0 2375 17.5 70 2 saab 99e 23 26.0 4 121.0 113.0 2234 12.5 70 2 bmw 2002 24 21.0 6 199.0 90.0 2648 15.0 70 1 amc gremlin ... ... ... ... ... ... ... ... ... ... 392 27.0 4 140.0 86.0 2790 15.6 82 1 ford mustang gl 393 44.0 4 97.0 52.0 2130 24.6 82 2 vw pickup 394 32.0 4 135.0 84.0 2295 11.6 82 1 dodge rampage 395 28.0 4 120.0 79.0 2625 18.6 82 1 ford ranger 396 31.0 4 119.0 82.0 2720 19.4 82 1 chevy s-10 <p>372 rows \u00d7 9 columns</p> <pre><code>print(df.info())\nprint(df.describe())\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 392 entries, 0 to 396\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           392 non-null    float64\n 1   cylinders     392 non-null    int64  \n 2   displacement  392 non-null    float64\n 3   horsepower    392 non-null    float64\n 4   weight        392 non-null    int64  \n 5   acceleration  392 non-null    float64\n 6   year          392 non-null    int64  \n 7   origin        392 non-null    int64  \n 8   name          392 non-null    object \ndtypes: float64(4), int64(4), object(1)\nmemory usage: 30.6+ KB\nNone\n              mpg   cylinders  displacement  horsepower       weight  \\\ncount  392.000000  392.000000    392.000000  392.000000   392.000000   \nmean    23.445918    5.471939    194.411990  104.469388  2977.584184   \nstd      7.805007    1.705783    104.644004   38.491160   849.402560   \nmin      9.000000    3.000000     68.000000   46.000000  1613.000000   \n25%     17.000000    4.000000    105.000000   75.000000  2225.250000   \n50%     22.750000    4.000000    151.000000   93.500000  2803.500000   \n75%     29.000000    8.000000    275.750000  126.000000  3614.750000   \nmax     46.600000    8.000000    455.000000  230.000000  5140.000000\n\n       acceleration        year      origin  \ncount    392.000000  392.000000  392.000000  \nmean      15.541327   75.979592    1.576531  \nstd        2.758864    3.683737    0.805518  \nmin        8.000000   70.000000    1.000000  \n25%       13.775000   73.000000    1.000000  \n50%       15.500000   76.000000    1.000000  \n75%       17.025000   79.000000    2.000000  \nmax       24.800000   82.000000    3.000000\n</code></pre> <pre><code>plt.scatter(df[\"horsepower\"], df[\"mpg\"])\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x7482ad90a9c0&gt;\n</code></pre> <p></p> <pre><code>X = pd.DataFrame({\"intercept\": np.ones(df.shape[0]), \"horsepower\": df[\"horsepower\"]})\nX.head()\n</code></pre> intercept horsepower 20 1.0 87.0 21 1.0 90.0 22 1.0 95.0 23 1.0 113.0 24 1.0 90.0 <pre><code>model = sm.OLS(df[\"mpg\"], X)\nresult = model.fit()\nsummarize(result)\n</code></pre> coef std err t P&gt;|t| intercept 41.1259 0.758 54.222 0.0 horsepower -0.1702 0.007 -24.281 0.0 <pre><code>result.params\n</code></pre> <pre><code>intercept     41.125860\nhorsepower    -0.170189\ndtype: float64\n</code></pre> <pre><code>fig, ax = plt.subplots()\nax.scatter(df[\"horsepower\"], df[\"mpg\"], s=5)\n\ncoef = result.params\nxlim = ax.get_xlim()\nylim = (coef[0] + coef[1] * xlim[0], coef[0] + coef[1] * xlim[1])\nax.plot(xlim, ylim, 'r-')\nax.set_title(coef)\n</code></pre> <pre><code>/tmp/ipykernel_152811/4053741418.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  ylim = (coef[0] + coef[1] * xlim[0], coef[0] + coef[1] * xlim[1])\n\nText(0.5, 1.0, 'intercept     41.125860\\nhorsepower    -0.170189\\ndtype: float64')\n</code></pre> <p></p> <pre><code># confidence interval\nparams = np.array(result.params)\nstderr = np.array(summarize(result)['std err'])\ndef conf_interval(params, stderr, x):\n    low = np.dot((params - stderr), x)\n    high = np.dot((params + stderr), x)\n    return low, high\n\nconf_interval(params, stderr, [1, 98])\n</code></pre> <pre><code>(23.003374498935447, 25.891374498935456)\n</code></pre> <pre><code>pred = result.get_prediction([1, 98])\npred.conf_int(alpha=0.05)\n</code></pre> <pre><code>array([[23.947671, 24.947078]])\n</code></pre> <pre><code>ax = plt.subplots(figsize=(8,8))[1]\nax.scatter(result.fittedvalues, result.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n</code></pre> <p></p> <pre><code>df = df.drop(columns=['name'])\ndf.corr()\n</code></pre> mpg cylinders displacement horsepower weight acceleration year origin mpg 1.000000 -0.768159 -0.803673 -0.783845 -0.832270 0.383589 0.568716 0.555810 cylinders -0.768159 1.000000 0.953304 0.842559 0.902638 -0.449479 -0.296280 -0.554618 displacement -0.803673 0.953304 1.000000 0.884850 0.946904 -0.475429 -0.315380 -0.612197 horsepower -0.783845 0.842559 0.884850 1.000000 0.880128 -0.641154 -0.370743 -0.448932 weight -0.832270 0.902638 0.946904 0.880128 1.000000 -0.381225 -0.297841 -0.578263 acceleration 0.383589 -0.449479 -0.475429 -0.641154 -0.381225 1.000000 0.205426 0.179090 year 0.568716 -0.296280 -0.315380 -0.370743 -0.297841 0.205426 1.000000 0.160888 origin 0.555810 -0.554618 -0.612197 -0.448932 -0.578263 0.179090 0.160888 1.000000 <pre><code>X = MS(df.drop(columns=['mpg'])).fit_transform(df)\n</code></pre> <pre><code>model = sm.OLS(df['mpg'], X)\nresult2 = model.fit()\nsummarize(result2)\n</code></pre> coef std err t P&gt;|t| intercept -19.7169 4.963 -3.973 0.000 cylinders -0.4081 0.349 -1.170 0.243 displacement 0.0168 0.009 1.963 0.050 horsepower -0.0174 0.015 -1.159 0.247 weight -0.0063 0.001 -8.213 0.000 acceleration 0.1110 0.105 1.059 0.290 year 0.7734 0.054 14.294 0.000 origin 1.3909 0.285 4.886 0.000 <pre><code>anova_lm(result, result2)\n</code></pre> df_resid ssr df_diff ss_diff F Pr(&gt;F) 0 370.0 8778.242950 0.0 NaN NaN NaN 1 364.0 4119.951551 6.0 4658.291399 68.593771 7.767048e-57 <pre><code>ax = plt.subplots(figsize=(8,8))[1]\nax.scatter(result2.fittedvalues, result2.resid)\nax.set_xlabel('Fitted value')\nax.set_ylabel('Residual')\nax.axhline(0, c='k', ls='--');\n</code></pre> <p></p> <pre><code>vals = [VIF(X, i)\n        for i in range(1, X.shape[1])]\nvif = pd.DataFrame({'vif':vals},\n                   index=X.columns[1:])\nvif\n</code></pre> vif cylinders 11.149785 displacement 23.969825 horsepower 9.578183 weight 14.029209 acceleration 2.414266 year 1.177402 origin 1.745033 <pre><code>infl = result2.get_influence ()\nax = plt.subplots(figsize =(8 ,8))[1]\nax.scatter(np.arange(X.shape [0]), infl.hat_matrix_diag)\nax.set_xlabel('Index ')\nax.set_ylabel('Leverage ')\nnp.argmax(infl.hat_matrix_diag)\n</code></pre> <pre><code>8\n</code></pre> <p></p> <pre><code>df.describe()\n</code></pre> mpg cylinders displacement horsepower weight acceleration year origin count 372.000000 372.000000 372.000000 372.000000 372.000000 372.000000 372.000000 372.000000 mean 23.760215 5.384409 188.114247 102.037634 2956.629032 15.738441 76.301075 1.594086 std 7.833502 1.672699 99.994614 36.079109 850.155868 2.590570 3.502898 0.810489 min 9.000000 3.000000 68.000000 46.000000 1613.000000 9.500000 70.000000 1.000000 25% 17.600000 4.000000 100.250000 75.000000 2219.750000 14.000000 73.000000 1.000000 50% 23.000000 4.000000 141.000000 91.500000 2750.000000 15.500000 76.000000 1.000000 75% 29.575000 6.000000 258.000000 115.250000 3581.750000 17.300000 79.000000 2.000000 max 46.600000 8.000000 455.000000 230.000000 5140.000000 24.800000 82.000000 3.000000 <pre><code>result2.rsquared\n</code></pre> <pre><code>0.8190301157401636\n</code></pre> <pre><code>result.rsquared\n</code></pre> <pre><code>0.6144135213369303\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Linear%2C%20polynomial%2C%20radial%20SVM/","title":"Linear, polynomial, radial SVM","text":"<ol> <li>Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation be- tween the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the train- ing data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions</li> </ol> <pre><code>import numpy as np \nimport pandas as pd\nimport sklearn.model_selection as skm\n\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, RocCurveDisplay\nfrom matplotlib.pyplot import subplots\nfrom ISLP import load_data\nfrom ISLP.svm import plot as plot_svm\n</code></pre> <pre><code>rng = np.random.default_rng(0)\nX = rng.standard_normal((100, 2))\ny = X[:,0]**2 + X[:,1]&gt;0\nX[:,1][y] +=1\n</code></pre> <pre><code>fig, ax = subplots(figsize=(4,4))\nax.scatter(X[:,0], X[:,1], c=y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x77abced84740&gt;\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, shuffle=True, stratify=y, random_state=0)\n</code></pre> <pre><code>svm_linear = SVC(kernel='linear')\nsvm_poly = SVC(kernel='poly', degree=3)\nsvm_rbf = SVC(kernel='rbf')\n\nsvm_linear.fit(X_train, y_train)\nsvm_poly.fit(X_train, y_train)\nsvm_rbf.fit(X_train, y_train)\n\nsvm_linear_pred = svm_linear.predict(X_test)\nsvm_poly_pred = svm_poly.predict(X_test)\nsvm_rbf_pred = svm_rbf.predict(X_test)\n</code></pre> <pre><code>accuracy_score(y_test, svm_linear_pred), accuracy_score(y_test, svm_poly_pred), accuracy_score(y_test, svm_rbf_pred)\n</code></pre> <pre><code>(0.96, 0.84, 1.0)\n</code></pre> <pre><code>fig, ax = subplots(ncols=3, figsize=(12,4))\nplot_svm(X, y, svm_linear, ax=ax[0])\nplot_svm(X, y, svm_poly, ax=ax[1])\nplot_svm(X, y, svm_rbf, ax=ax[2])\n</code></pre> <ol> <li>In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.</li> <li>Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.</li> <li>Fit a support vector classifier to the data with various values of C, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different val- ues of this parameter. Comment on your results. Note you will need to fit the classifier without the gas mileage variable to pro- duce sensible results.</li> <li>Now repeat (b), this time using SVMs with radial and polyno- mial basis kernels, with different values of gamma and degree and C. Comment on your results.</li> <li>Make some plots to back up your assertions in (b) and (c). Hint: In the lab, we used the plot_svm() function for fitted SVMs. When p &gt; 2, you can use the keyword argument features to create plots displaying pairs of variables at a time.</li> </ol> <pre><code>auto = load_data(\"Auto\")\nauto.head()\n</code></pre> mpg cylinders displacement horsepower weight acceleration year origin name chevrolet chevelle malibu 18.0 8 307.0 130 3504 12.0 70 1 buick skylark 320 15.0 8 350.0 165 3693 11.5 70 1 plymouth satellite 18.0 8 318.0 150 3436 11.0 70 1 amc rebel sst 16.0 8 304.0 150 3433 12.0 70 1 ford torino 17.0 8 302.0 140 3449 10.5 70 1 <pre><code>X = auto.dropna()\nX = X.reset_index()\n\ny = X[\"mpg\"] &gt; X[\"mpg\"].median()\nX = X.drop(columns=[\"name\", \"mpg\"])\nX.head()\n</code></pre> cylinders displacement horsepower weight acceleration year origin 0 8 307.0 130 3504 12.0 70 1 1 8 350.0 165 3693 11.5 70 1 2 8 318.0 150 3436 11.0 70 1 3 8 304.0 150 3433 12.0 70 1 4 8 302.0 140 3449 10.5 70 1 <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, shuffle=True, stratify=y, random_state=0)\nkfold = skm.KFold(n_splits=3, shuffle=True, random_state=0)\n</code></pre> <pre><code>c = 10**np.linspace(2, -5, 10)\nparams = {'C': c}\nsvm_lin = skm.GridSearchCV(SVC(kernel='linear'), param_grid=params, cv=kfold, scoring=\"accuracy\")\nsvm_lin.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=3, random_state=0, shuffle=True),\n             estimator=SVC(kernel='linear'),\n             param_grid={'C': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05])},\n             scoring='accuracy')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=3, random_state=0, shuffle=True),\n             estimator=SVC(kernel='linear'),\n             param_grid={'C': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05])},\n             scoring='accuracy')</pre> best_estimator_: SVC<pre>SVC(C=0.0774263682681127, kernel='linear')</pre> SVC?Documentation for SVC<pre>SVC(C=0.0774263682681127, kernel='linear')</pre> <pre><code>svm_lin_pred = svm_lin.best_estimator_.predict(X_test)\naccuracy_score(y_test, svm_lin_pred)\n</code></pre> <pre><code>0.8775510204081632\n</code></pre> <pre><code>fig, ax = subplots(figsize=(4, 4))\nax.plot(np.log10(c), svm_lin.cv_results_[\"mean_test_score\"], marker=\"o\")\nax.axvline(np.log10(svm_lin.best_params_[\"C\"]))\nax.set_title(\"accuracy score with function of log10(C)\"), np.log10(svm_lin.best_params_[\"C\"])\n</code></pre> <pre><code>(Text(0.5, 1.0, 'accuracy score with function of log10(C)'),\n -1.1111111111111112)\n</code></pre> <pre><code>c = 10**np.linspace(2, -5, 10)\ndegree = list(range(2, 6))\nparams = {'C': c, 'degree': degree}\nsvm_poly = skm.GridSearchCV(SVC(kernel='poly'), param_grid=params, cv=kfold, scoring=\"accuracy\")\nsvm_poly.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=3, random_state=0, shuffle=True),\n             estimator=SVC(kernel='poly'),\n             param_grid={'C': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05]),\n                         'degree': [2, 3, 4, 5]},\n             scoring='accuracy')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=3, random_state=0, shuffle=True),\n             estimator=SVC(kernel='poly'),\n             param_grid={'C': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05]),\n                         'degree': [2, 3, 4, 5]},\n             scoring='accuracy')</pre> best_estimator_: SVC<pre>SVC(C=100.0, degree=2, kernel='poly')</pre> SVC?Documentation for SVC<pre>SVC(C=100.0, degree=2, kernel='poly')</pre> <pre><code>svm_poly_pred = svm_poly.best_estimator_.predict(X_test)\naccuracy_score(y_test, svm_poly_pred)\n</code></pre> <pre><code>0.8775510204081632\n</code></pre> <pre><code>df = pd.DataFrame(svm_poly.cv_results_[\"params\"])\ndf[\"accuracy\"] = svm_poly.cv_results_[\"mean_test_score\"]\n\nfig, ax = subplots(figsize=(4, 4))\nax.set_title(\"accuracy score as function of log10(C) and polynomial degree\"), np.log10(svm_poly.best_params_[\"C\"])\n\nfor degree, group in df.groupby(\"degree\"):\n    ax.plot(np.log10(group[\"C\"]), group[\"accuracy\"], label=degree, marker=\"o\")\nax.legend()\nsvm_poly.best_params_\n</code></pre> <pre><code>{'C': 100.0, 'degree': 2}\n</code></pre> <pre><code>c = 10**np.linspace(2, -5, 10)\ngamma = 10**np.linspace(2, -5, 10)\nparams = {'C': c, \"gamma\": gamma}\nsvm_poly = skm.GridSearchCV(SVC(kernel='rbf'), param_grid=params, cv=kfold, scoring=\"accuracy\")\nsvm_poly.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=3, random_state=0, shuffle=True),\n             estimator=SVC(),\n             param_grid={'C': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05]),\n                         'gamma': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05])},\n             scoring='accuracy')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=3, random_state=0, shuffle=True),\n             estimator=SVC(),\n             param_grid={'C': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05]),\n                         'gamma': array([1.00000000e+02, 1.66810054e+01, 2.78255940e+00, 4.64158883e-01,\n       7.74263683e-02, 1.29154967e-02, 2.15443469e-03, 3.59381366e-04,\n       5.99484250e-05, 1.00000000e-05])},\n             scoring='accuracy')</pre> best_estimator_: SVC<pre>SVC(C=16.68100537200059, gamma=1e-05)</pre> SVC?Documentation for SVC<pre>SVC(C=16.68100537200059, gamma=1e-05)</pre> <pre><code>\n</code></pre>"},{"location":"LinearSVM%20and%20PolynomSVM%20from%20scratch/","title":"Linear and Polynom SVM from scratch","text":"<p>We will create SVM using the loss function and do the gradient descend manually</p> <pre><code>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>df = pd.read_csv(\"dataset/Iris.csv\", index_col=\"Id\")\n</code></pre>"},{"location":"LinearSVM%20and%20PolynomSVM%20from%20scratch/#we-use-the-iris-dataset","title":"We use the iris dataset","text":"<p>But to make things simple we only classify is it iris setosa or not, because iris versicolor and iris virginica is'nt linearly separable</p> <pre><code>sns.pairplot(df, hue=\"Species\")\nplt.show()\n</code></pre> <p></p> <pre><code>x = np.array(df.drop(columns=[\"Species\", \"SepalLengthCm\", \"SepalWidthCm\"]))\ny = np.array([s == 'Iris-setosa' for s in df[\"Species\"]])\n</code></pre> <pre><code>from sklearn.preprocessing import StandardScaler\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, stratify=y, random_state=7)\n# scaler = StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n</code></pre> <pre><code>def perceptron(lx, ly, maxiter=1000):\n    theta = np.zeros(len(lx[0]), dtype=float)\n    theta_0 = 0\n\n    history = []\n    for _ in range(maxiter):\n        count = 0\n        for x, y in zip(lx, ly):\n            y = 1 if bool(y) else -1\n            z = y * np.dot(theta, x) + theta_0\n            if z &lt;= 0:\n                theta += y * x\n                theta_0 += y\n                history.append(np.copy(theta))\n                count += 1\n        if count == 0:\n            break\n    return theta_0, theta\n</code></pre> <pre><code>theta_0, theta = perceptron(X_train, Y_train)\n</code></pre> <pre><code>count = 0\n# X_test = scaler.transform(X_test)\nfor x, y in zip(X_test, Y_test):\n    z = theta_0 + np.dot(x, theta)\n    count += ((z &gt; 0) and y)\n    count += ((z &lt; 0) and not y)\n</code></pre> <pre><code>count/len(Y_test)\n</code></pre> <pre><code>0.5263157894736842\n</code></pre> <pre><code>theta\n</code></pre> <pre><code>array([-0.1, -2.2])\n</code></pre> <pre><code>theta_0\n</code></pre> <pre><code>5\n</code></pre> <pre><code>\n</code></pre> <pre><code>class LinearSVM:\n    def __init__(self, lx, ly):\n        self.theta = np.zeros(len(lx[0]), dtype=float)\n        self.theta_0 = 0\n        self.lmbd = 0.1\n        self.lr = 0.001\n        self.maxiter=1000\n        self.lx = lx\n        self.ly = ly\n\n    def transform_data(self, func):\n        self.lx = np.apply_along_axis(func, 1, self.lx)\n        self.theta = np.zeros(len(self.lx[0]), dtype=float)\n\n    def train(self):\n        history = []\n        for _ in range(self.maxiter):\n            count = 0\n            for x, y in zip(self.lx, self.ly):\n                y = 1 if bool(y) else -1\n                z = 1 - (y * (np.dot(self.theta, x) + self.theta_0))\n                gradient_theta = self.dregterm_dtheta(x)\n                gradient_theta_0 = 0\n                if z &gt; 0:\n                    gradient_theta += self.dlossterm_dtheta(x, y)\n                    gradient_theta_0 += self.dlossterm_dtheta_0(y)\n                    count += 1\n                self.theta -= gradient_theta * self.lr\n                self.theta_0 -= gradient_theta_0 * self.lr\n\n                history.append(np.copy(self.theta))\n            # if count == 0:\n            #     break\n        # print(history)\n        return self.theta_0, self.theta\n\n    def dregterm_dtheta(self, x):\n        return self.lmbd * self.theta\n\n    def dlossterm_dtheta(self, x, y):\n        return -y*x\n\n    def dlossterm_dtheta_0(self, y):\n        return -y\n\n    def predict(self, lx_test, ly_test):\n        count = 0\n        for x, y in zip(lx_test, ly_test):\n            z = self.theta_0 + np.dot(x, self.theta)\n            count += ((z &gt; 0) and y)\n            count += ((z &lt; 0) and not y)\n        return count / len(lx_test)\n\n    def plot(self):\n        # Plot data points\n        plt.scatter(self.lxx[:, 0], self.lxx[:, 1], c=self.lyy, cmap=plt.cm.Paired, edgecolors='k', label='Data Points')\n\n        # Plot decision boundary\n        x1 = np.linspace(min(self.lxx[:, 0]), max(self.lxx[:, 0]), 100)\n        x2 = -(self.theta[0] * x1 + self.theta_0) / self.theta[1]\n        plt.plot(x1, x2, 'k-', label='Decision Boundary')\n\n        # Plot margin boundaries\n        margin = 1 / np.linalg.norm(self.theta)\n        x2_upper = x2 + margin / np.linalg.norm(self.theta)\n        x2_lower = x2 - margin / np.linalg.norm(self.theta)\n        plt.plot(x1, x2_upper, 'k--', label='Margin Boundary', color=\"red\")\n        plt.plot(x1, x2_lower, 'k--', color=\"red\")\n\n        # Add labels and legend\n        plt.xlabel('Feature 1')\n        plt.ylabel('Feature 2')\n        plt.title('Linear SVM Decision and Margin Boundaries')\n        plt.legend()\n        plt.show()\n\n</code></pre> <pre><code>s = LinearSVM(X_train, Y_train)\n</code></pre> <pre><code>s.train()\n</code></pre> <pre><code>(-0.8530000000000006, array([-0.83165633, -0.66812085]))\n</code></pre> <pre><code>s.predict(X_test, Y_test)\n</code></pre> <pre><code>np.float64(1.0)\n</code></pre> <pre><code>s.plot()\n</code></pre> <pre><code>/tmp/ipykernel_4050/4131259921.py:68: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k--\" (-&gt; color='k'). The keyword argument will take precedence.\n  plt.plot(x1, x2_upper, 'k--', label='Margin Boundary', color=\"red\")\n/tmp/ipykernel_4050/4131259921.py:69: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k--\" (-&gt; color='k'). The keyword argument will take precedence.\n  plt.plot(x1, x2_lower, 'k--', color=\"red\")\n</code></pre> <p></p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nclass PolynomSVM:\n    def __init__(self, lx, ly):\n        self.copy_lx = lx\n        self.lx = np.apply_along_axis(self.polynom, 1, lx)\n        self.theta = np.zeros(len(self.lx[0]), dtype=float)\n        self.theta_0 = 0\n        self.lmbd = 0.9\n        self.lr = 0.001\n        self.maxiter = 1000\n        self.ly = ly\n\n    @staticmethod\n    def polynom(arr):\n        s = np.sqrt(2)\n        return np.array([s * arr[0], s * arr[1], arr[0] ** 2, arr[1] ** 2, s * arr[0] * arr[1], 1])\n\n    def train(self):\n        for _ in range(self.maxiter):\n            for x, y in zip(self.lx, self.ly):\n                y = 1 if bool(y) else -1\n                z = 1 - (y * (np.dot(self.theta, x) + self.theta_0))\n                gradient_theta = self.lmbd * self.theta\n                gradient_theta_0 = 0\n                if z &gt; 0:\n                    gradient_theta -= y * x\n                    gradient_theta_0 -= y\n                self.theta -= gradient_theta * self.lr\n                self.theta_0 -= gradient_theta_0 * self.lr\n        return self.theta_0, self.theta\n\n    def predict(self, lx_test, ly_test):\n        lx_test = np.apply_along_axis(self.polynom, 1, lx_test)\n        predictions = np.dot(lx_test, self.theta) + self.theta_0\n        correct_predictions = ((predictions &gt; 0) == ly_test).sum()\n        print((predictions&gt;0).sum())\n        print(len(lx_test))\n        return correct_predictions / len(lx_test)\n\n    def plot(self):\n        x_min, x_max = self.copy_lx[:, 0].min() - 1, self.copy_lx[:, 0].max() + 1\n        y_min, y_max = self.copy_lx[:, 1].min() - 1, self.copy_lx[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n\n        grid = np.c_[xx.ravel(), yy.ravel()]\n        grid_transformed = np.apply_along_axis(self.polynom, 1, grid)\n        zz = np.dot(grid_transformed, self.theta) + self.theta_0\n        zz = zz.reshape(xx.shape)\n\n        plt.contourf(xx, yy, zz, levels=[-1, 0, 1], alpha=0.2, colors=[\"blue\", \"black\", \"red\"])\n        plt.contour(xx, yy, zz, levels=[-1, 0, 1], colors=[\"blue\", \"black\", \"red\"], linestyles=[\"dashed\", \"solid\", \"dashed\"])\n\n        plt.scatter(self.copy_lx[:, 0], self.copy_lx[:, 1], c=self.ly, cmap=plt.cm.Paired, edgecolors='k')\n        plt.xlabel(\"Feature 1\")\n        plt.ylabel(\"Feature 2\")\n        plt.title(\"Polynomial SVM Decision Boundary\")\n        plt.show()\n</code></pre> <pre><code>s = PolynomSVM(X_train, Y_train)\n</code></pre> <pre><code>s.train()\n</code></pre> <pre><code>(-0.8980000000000007,\n array([-0.29973561, -0.27545061,  0.1579069 ,  0.10840018,  0.19147607,\n        -0.00051781]))\n</code></pre> <pre><code>s.plot()\n</code></pre> <p></p> <pre><code>s.predict(X_test, Y_test)\n</code></pre> <pre><code>13\n38\n\nnp.float64(1.0)\n</code></pre>"},{"location":"NYSE%20trading%20volume%20estimation%20with%20AutoRegression%2C%20RNN%2C%20and%20fusion%20of%20LSTM%20%2B%20Dense/","title":"NYSE trading volume estimation with AutoRegression, RNN, and fusion of LSTM + Dense","text":"<p>This exercise is from Introduction of statistical analysis (ISLP) Section DeepLearning.</p> <p>We will predict trading volume from this three time series predictor. - Log trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past - Dow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days. - Log volatility. This is based on the absolute values of daily price movements.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib.pyplot import subplots\nfrom ISLP import load_data\n</code></pre> <pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import Adam, RMSprop\nfrom torchmetrics import MeanSquaredError, R2Score\nfrom torchmetrics.classification import BinaryAccuracy, MulticlassAccuracy\nfrom torchinfo import summary\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n</code></pre> <pre><code>import sklearn.model_selection as skm\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\n</code></pre> <pre><code>NYSE = load_data('NYSE')\nNYSE.index = pd.to_datetime(NYSE.index)\ncols = ['DJ_return', 'log_volume', 'log_volatility']\nX = pd.DataFrame(\n    StandardScaler(with_mean=True, with_std=True).fit_transform(NYSE[cols]),\n    columns=NYSE[cols].columns,\n    index=NYSE.index\n)\nX.head()\n</code></pre> DJ_return log_volume log_volatility date 1962-12-03 -0.549823 0.175075 -4.357078 1962-12-04 0.905200 1.517291 -2.529058 1962-12-05 0.434813 2.283789 -2.418037 1962-12-06 -0.431397 0.935176 -2.366521 1962-12-07 0.046340 0.224779 -2.500970 <pre><code>## Preprocess the lag, adding day of week and month as hot one encoding\n</code></pre> <pre><code>for lag in range(1, 6):\n    for col in cols:\n        newcol = np.zeros(X.shape [0]) * np.nan\n        newcol[lag:] = X[col]. values[:-lag]\n        X.insert(len(X.columns), \"{0}_{1}\".format(col , lag), newcol)\nX.insert(len(X.columns), 'train', NYSE['train'])\nX = X.dropna()\n</code></pre> <pre><code>Y, train = X['log_volume'], X['train'].values\nX = X.drop(columns =['train'] + cols)\nX.columns\n</code></pre> <pre><code>Index(['DJ_return_1', 'log_volume_1', 'log_volatility_1', 'DJ_return_2',\n       'log_volume_2', 'log_volatility_2', 'DJ_return_3', 'log_volume_3',\n       'log_volatility_3', 'DJ_return_4', 'log_volume_4', 'log_volatility_4',\n       'DJ_return_5', 'log_volume_5', 'log_volatility_5'],\n      dtype='object')\n</code></pre> <pre><code>X_tmp = X.join(NYSE[\"day_of_week\"])\nX_tmp['month'] = X_tmp.index.month\nX_tmp = pd.get_dummies(X_tmp, columns=[\"day_of_week\", \"month\"])\nX_tmp.head()\n</code></pre> DJ_return_1 log_volume_1 log_volatility_1 DJ_return_2 log_volume_2 log_volatility_2 DJ_return_3 log_volume_3 log_volatility_3 DJ_return_4 ... month_3 month_4 month_5 month_6 month_7 month_8 month_9 month_10 month_11 month_12 date 1962-12-10 0.046340 0.224779 -2.500970 -0.431397 0.935176 -2.366521 0.434813 2.283789 -2.418037 0.905200 ... False False False False False False False False False True 1962-12-11 -1.304126 0.605918 -1.366028 0.046340 0.224779 -2.500970 -0.431397 0.935176 -2.366521 0.434813 ... False False False False False False False False False True 1962-12-12 -0.006294 -0.013661 -1.505667 -1.304126 0.605918 -1.366028 0.046340 0.224779 -2.500970 -0.431397 ... False False False False False False False False False True 1962-12-13 0.377081 0.042552 -1.551515 -0.006294 -0.013661 -1.505667 -1.304126 0.605918 -1.366028 0.046340 ... False False False False False False False False False True 1962-12-14 -0.411718 -0.419836 -1.597607 0.377081 0.042552 -1.551515 -0.006294 -0.013661 -1.505667 -1.304126 ... False False False False False False False False False True <p>5 rows \u00d7 32 columns</p>"},{"location":"NYSE%20trading%20volume%20estimation%20with%20AutoRegression%2C%20RNN%2C%20and%20fusion%20of%20LSTM%20%2B%20Dense/#linear-regression-as-base-measure","title":"Linear Regression as base measure","text":"<pre><code>lr = LinearRegression().fit(X[train], Y[train])\nlr_pred = lr.predict(X[~train])\n((lr_pred - Y[~train])**2).mean(), r2_score(Y[~train], lr_pred)\n</code></pre> <pre><code>(0.6186285663937958, 0.4128912938562521)\n</code></pre>"},{"location":"NYSE%20trading%20volume%20estimation%20with%20AutoRegression%2C%20RNN%2C%20and%20fusion%20of%20LSTM%20%2B%20Dense/#autoregression","title":"AutoRegression","text":"<ol> <li>Fit a lag-5 autoregressive model to the NYSE data, as described in the text and Lab 10.9.6. Refit the model with a 12-level factor repre- senting the month. Does this factor improve the performance of the model?</li> </ol> <p>Improve R2 by around 5%</p> <pre><code>datasets = []\nfor mask in [train, ~train]:\n    X_day_month_t = torch.tensor(\n        np.asarray(X_tmp[mask]).astype(np.float32)\n    )\n    Y_t = torch.tensor(\n        np.asarray(Y[mask]).astype(np.float32)\n    )\n    datasets.append(TensorDataset(X_day_month_t , Y_t))\nar_train, ar_test = datasets\n</code></pre> <pre><code>import pytorch_lightning as pl\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torchmetrics import R2Score\n\nclass AutoRegression(pl.LightningModule):\n    def __init__(self, input_size):\n        super().__init__()\n        self.learning_rate = 0.001\n\n        self.model = nn.Sequential(\n            nn.Linear(input_size, 10),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(10, 1)\n        )\n\n        self.loss_fn = nn.MSELoss()\n        self.train_r2 = R2Score()\n        self.val_r2 = R2Score()\n        self.test_r2 = R2Score()\n\n    def forward(self, x):\n        return self.model(x).squeeze()\n\n    def _shared_step(self, batch):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y)\n        return loss, preds, y\n\n    def training_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n        self.train_r2.update(preds, y)\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=False)\n        self.log('train_r2', self.train_r2, on_step=False, on_epoch=True, prog_bar=True) \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n        self.val_r2.update(preds, y)\n\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val_r2', self.val_r2, on_step=False, on_epoch=True, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n        self.test_r2.update(preds, y)\n\n        self.log('test_loss', loss, on_step=False, on_epoch=True)\n        self.log('test_r2', self.test_r2, on_step=False, on_epoch=True)\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=self.learning_rate)\n</code></pre> <pre><code>class AutoRegressionDataModule(pl.LightningDataModule):\n    def __init__(self, train_td, test_td, batch_size=32, num_workers=8):\n        super().__init__()\n        self.train_td = train_td\n        self.test_td = test_td\n        self.batch_size = batch_size\n        self.num_workers=num_workers\n\n    def train_dataloader(self):\n        return DataLoader(self.train_td, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self):\n        return self.test_dataloader()\n\n    def test_dataloader(self):\n        return DataLoader(self.test_td, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre> <pre><code>ar_model = AutoRegression(len(X_tmp.columns))\nar_dm = AutoRegressionDataModule(ar_train, ar_test)\nar_logger = TensorBoardLogger('logs', name='NYSE_AR')\nar_trainer = Trainer(\n    deterministic=True, \n    max_epochs=30, \n    logger=ar_logger,\n    callbacks=EarlyStopping(monitor='val_loss', patience=5, mode='min')\n)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>ar_trainer.fit(ar_model, datamodule=ar_dm)\n</code></pre> <pre><code>  | Name     | Type       | Params | Mode \n------------------------------------------------\n0 | model    | Sequential | 341    | train\n1 | loss_fn  | MSELoss    | 0      | train\n2 | train_r2 | R2Score    | 0      | train\n3 | val_r2   | R2Score    | 0      | train\n4 | test_r2  | R2Score    | 0      | train\n------------------------------------------------\n341       Trainable params\n0         Non-trainable params\n341       Total params\n0.001     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n</code></pre> <pre><code>ar_trainer.test(ar_model, datamodule=ar_dm)\n</code></pre> <pre><code>Testing: |                                                | 0/? [00:00&lt;?, ?it/s]\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_loss           0.5618957877159119\n         test_r2            0.46673357486724854\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'test_loss': 0.5618957877159119, 'test_r2': 0.46673357486724854}]\n</code></pre> <pre><code>ordered_cols = []\nfor lag in range (5,0,-1):\n    for col in cols:\n        ordered_cols.append('{0}_{1}'.format(col , lag))\n\nX = X.reindex(columns=ordered_cols)\nX.columns\n</code></pre> <pre><code>Index(['DJ_return_5', 'log_volume_5', 'log_volatility_5', 'DJ_return_4',\n       'log_volume_4', 'log_volatility_4', 'DJ_return_3', 'log_volume_3',\n       'log_volatility_3', 'DJ_return_2', 'log_volume_2', 'log_volatility_2',\n       'DJ_return_1', 'log_volume_1', 'log_volatility_1'],\n      dtype='object')\n</code></pre> <pre><code>X_rnn = X.to_numpy().reshape((-1,5,3))\nX_rnn.shape\n</code></pre> <pre><code>(6046, 5, 3)\n</code></pre> <pre><code>datasets = []\nfor mask in [train, ~train]:\n    X_rnn_t = torch.tensor(\n        np.asarray(X_rnn[mask]).astype(np.float32)\n    )\n    Y_t = torch.tensor(\n        np.asarray(Y[mask]).astype(np.float32)\n    )\n    datasets.append(TensorDataset(X_rnn_t , Y_t))\nnyse_train, nyse_test = datasets\n</code></pre>"},{"location":"NYSE%20trading%20volume%20estimation%20with%20AutoRegression%2C%20RNN%2C%20and%20fusion%20of%20LSTM%20%2B%20Dense/#rnn","title":"RNN","text":"<ol> <li>In Section 10.9.6, we showed how to fit a linear AR model to the NYSE data using the LinearRegression() function. However, we also mentioned that we can \u201cflatten\u201d the short sequences produced for the RNN model in order to fit a linear AR model. Use this latter approach to fit a linear AR model to the NYSE data.</li> </ol> <pre><code>class NYSEModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.MSELoss()\n        self.learning_rate = 0.001\n        self.val_r2 = R2Score()\n        self.test_r2 = R2Score()\n        self.train_r2 = R2Score()\n\n        self.rnn = nn.RNN(3, 12, batch_first=True)\n        self.dense = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(12, 24),\n            nn.ReLU(),\n            nn.Linear(24, 1)\n        )\n\n    def forward(self, x):\n        val, h_n = self.rnn(x)\n        val = self.dense(val[:,-1])\n        return torch.flatten(val)\n\n    def _shared_step(self, batch):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y)\n        return loss, preds, y\n\n    def training_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n\n        self.train_r2.update(preds, y)\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=False)\n        self.log('train_r2', self.train_r2, on_step=False, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n\n        self.val_r2.update(preds, y)\n\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val_r2', self.val_r2, on_step=False, on_epoch=True, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n        self.test_r2.update(preds, y)\n\n        self.log('test_loss', loss, on_step=False, on_epoch=True)\n        self.log('test_r2', self.test_r2, on_step=False, on_epoch=True)\n\n    def configure_optimizers(self):\n        return RMSprop(self.parameters(), lr=self.learning_rate)\n</code></pre> <pre><code>nyse_model = NYSEModel()\nnyse_dm = AutoRegressionDataModule(nyse_train, nyse_test, batch_size=64)\nnyse_logger = TensorBoardLogger('logs', name='NYSE_RNN')\nnyse_trainer = Trainer(\n    deterministic=True, \n    max_epochs=200, \n    logger=nyse_logger,\n    callbacks=EarlyStopping(monitor='val_loss', patience=5, mode='min')\n)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>nyse_trainer.fit(nyse_model, datamodule=nyse_dm)\n</code></pre> <pre><code>  | Name     | Type       | Params | Mode \n------------------------------------------------\n0 | loss_fn  | MSELoss    | 0      | train\n1 | val_r2   | R2Score    | 0      | train\n2 | test_r2  | R2Score    | 0      | train\n3 | train_r2 | R2Score    | 0      | train\n4 | rnn      | RNN        | 204    | train\n5 | dense    | Sequential | 13     | train\n------------------------------------------------\n217       Trainable params\n0         Non-trainable params\n217       Total params\n0.001     Total estimated model params size (MB)\n8         Modules in train mode\n0         Modules in eval mode\n</code></pre>"},{"location":"NYSE%20trading%20volume%20estimation%20with%20AutoRegression%2C%20RNN%2C%20and%20fusion%20of%20LSTM%20%2B%20Dense/#note","title":"Note","text":"<p>This one underperform (only 0.4 R2 score) because we dont fit the dayofweek and month in this model. The conclusion is even the more complex model cant outperform without more context/quality on data.</p> <pre><code>nyse_trainer.test(nyse_model, datamodule=nyse_dm)\n</code></pre> <pre><code>Testing: |                                                | 0/? [00:00&lt;?, ?it/s]\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_loss           0.6231500506401062\n         test_r2            0.4086001515388489\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'test_loss': 0.6231500506401062, 'test_r2': 0.4086001515388489}]\n</code></pre>"},{"location":"NYSE%20trading%20volume%20estimation%20with%20AutoRegression%2C%20RNN%2C%20and%20fusion%20of%20LSTM%20%2B%20Dense/#fusion-rnn-dense","title":"Fusion RNN + Dense","text":"<p>This model inspired from CNN that has its own conv layer and dense layer. The input in dense layer will be the output of LSTM, dayofweek, and month</p> <pre><code>from torch.utils.data import Dataset\n\nclass FusionDataset(Dataset):\n    def __init__(self, X_seq, X_tab, y):\n        self.X_seq = torch.tensor(X_seq, dtype=torch.float32)        # shape: (N, T, F)\n        self.X_tab = torch.tensor(X_tab, dtype=torch.float32)        # shape: (N, D)\n        self.y = torch.tensor(y, dtype=torch.float32)                # shape: (N,)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X_seq[idx], self.X_tab[idx], self.y[idx]\n\nclass FusionModel(pl.LightningModule):\n    def __init__(self, tabular_dim):  # jumlah fitur tabular (D)\n        super().__init__()\n        self.loss_fn = nn.MSELoss()\n        self.learning_rate = 0.001\n        self.train_r2 = R2Score()\n        self.val_r2 = R2Score()\n        self.test_r2 = R2Score()\n\n        self.lstm = nn.LSTM(3, 12, batch_first=True)\n\n        self.dense = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(12 + tabular_dim, 32),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x_seq, x_tabular):\n        val, _ = self.lstm(x_seq)              # val: (batch, seq_len, 12)\n        val_last = val[:, -1, :]              # ambil timestep terakhir\n        combined = torch.cat([val_last, x_tabular], dim=1)  # (batch, 12 + D)\n        out = self.dense(combined)\n        return torch.flatten(out)\n\n    def _shared_step(self, batch):\n        x_seq, x_tabular, y = batch\n        preds = self(x_seq, x_tabular)\n        loss = self.loss_fn(preds, y)\n        return loss, preds, y\n\n    def training_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n\n        self.train_r2.update(preds, y)\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=False)\n        self.log('train_r2', self.train_r2, on_step=False, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n        self.val_r2.update(preds, y)\n\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val_r2', self.val_r2, on_step=False, on_epoch=True, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        loss, preds, y = self._shared_step(batch)\n\n        self.test_r2.update(preds, y)\n\n        self.log('test_loss', loss, on_step=False, on_epoch=True)\n        self.log('test_r2', self.test_r2, on_step=False, on_epoch=True)\n\n    def configure_optimizers(self):\n        return RMSprop(self.parameters(), lr=self.learning_rate)\n\nclass FusionDataModule(pl.LightningDataModule):\n    def __init__(self, train_dataset, val_dataset, test_dataset=None,\n                 batch_size=32, num_workers=4):\n        super().__init__()\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n        self.test_dataset = test_dataset or val_dataset  # fallback\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n                          shuffle=True, num_workers=self.num_workers)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n                          shuffle=False, num_workers=self.num_workers)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n                          shuffle=False, num_workers=self.num_workers)\n\n</code></pre> <pre><code>X_tab = X.join(NYSE[\"day_of_week\"])\nX_tab['month'] = X_tmp.index.month\nX_tab = X_tab[['day_of_week', 'month']]\nX_tab = pd.get_dummies(X_tab, columns=[\"day_of_week\", \"month\"])\nX_tab = X_tab.to_numpy()\nX_tab.shape, X_rnn.shape\n</code></pre> <pre><code>((6046, 17), (6046, 5, 3))\n</code></pre> <pre><code>X_rnn[train].shape, X_tab[train].shape\n</code></pre> <pre><code>((4276, 5, 3), (4276, 17))\n</code></pre> <pre><code>train_ds = FusionDataset(X_rnn[train], X_tab[train], Y[train])\nval_ds = FusionDataset(X_rnn[~train], X_tab[~train], Y[~train])\n\ndm = FusionDataModule(train_ds, val_ds, batch_size=64)\n\nmodel = FusionModel(tabular_dim=X_tab.shape[1])\n\ntrainer = pl.Trainer(max_epochs=50)\ntrainer.fit(model, dm)\n</code></pre> <pre><code>/tmp/ipykernel_571680/3164401403.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  self.y = torch.tensor(y, dtype=torch.float32)                # shape: (N,)\nGPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n  | Name     | Type       | Params | Mode \n------------------------------------------------\n0 | loss_fn  | MSELoss    | 0      | train\n1 | train_r2 | R2Score    | 0      | train\n2 | val_r2   | R2Score    | 0      | train\n3 | test_r2  | R2Score    | 0      | train\n4 | lstm     | LSTM       | 816    | train\n5 | dense    | Sequential | 993    | train\n------------------------------------------------\n1.8 K     Trainable params\n0         Non-trainable params\n1.8 K     Total params\n0.007     Total estimated model params size (MB)\n11        Modules in train mode\n0         Modules in eval mode\n\n`Trainer.fit` stopped: `max_epochs=50` reached.\n</code></pre> <pre><code>trainer.test(model, dm)\n</code></pre> <pre><code>Testing: |                                                | 0/? [00:00&lt;?, ?it/s]\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_loss           0.5550546646118164\n         test_r2            0.47322607040405273\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'test_loss': 0.5550546646118164, 'test_r2': 0.47322607040405273}]\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Polynomial%20Logistic%20Regression/","title":"Polynomial Logistic Regression","text":"<ol> <li>We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features</li> <li>Generate a data set with n = 500 and p = 2, such that the obser- vations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows: rng = np.random.default_rng (5) x1 = rng.uniform(size =500) - 0.5 x2 = rng.uniform(size =500) - 0.5 y = x12 - x22 &gt; 0</li> <li>Plot the observations, colored according to their class labels. Your plot should display X1 on the x-axis, and X2 on the y- axis.</li> <li>Fit a logistic regression model to the data, using X1 and X2 as predictors.     Apply this model to the training data in order to obtain a pre- dicted class label for each training observation. Plot the ob- servations, colored according to the predicted class labels. The decision boundary should be linear.</li> <li>Now fit a logistic regression model to the data using non-linear functions of X1 and X2 as predictors (e.g. X2 1 , X1 \u00d7X2, log(X2), and so forth).</li> <li>Apply this model to the training data in order to obtain a pre- dicted class label for each training observation. Plot the ob- servations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)\u2013(e) until you come up with an example in which the predicted class labels are obviously non-linear.</li> <li>Fit a support vector classifier to the data with X1 and X2 as predictors. Obtain a class prediction for each training observa- tion. Plot the observations, colored according to the predicted class labels.</li> <li>Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.</li> <li>Comment on your results.</li> </ol> <pre><code>import numpy as np \nimport pandas as pd\nimport sklearn.model_selection as skm\n\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, RocCurveDisplay\nfrom matplotlib.pyplot import subplots\nfrom ISLP import load_data\nfrom ISLP.svm import plot as plot_svm\n</code></pre> <pre><code>rng = np.random.default_rng(0)\nx1 = rng.standard_normal(500)\nx2 = rng.standard_normal(500)\n\nX = np.array([x1, x2]).T\ny = X[:,0]**2 - X[:,1]&gt;1\nX[:,1][y] -= 1\n</code></pre> <pre><code>fig, ax = subplots(figsize=(4,4))\nax.scatter(X[:,0], X[:,1], c=y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x7038467794c0&gt;\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, shuffle=True, stratify=y, random_state=0)\nlr_linear = LogisticRegression()\nlr_linear.fit(X_train, y_train)\nlr_linear_pred = lr_linear.predict(X_test)\naccuracy_score(y_test, lr_linear_pred)\n</code></pre> <pre><code>0.896\n</code></pre> <pre><code>fig, ax = subplots(figsize=(4,4))\nax.scatter(X[:,0], X[:,1], c=lr_linear.predict(X))\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x703846912420&gt;\n</code></pre> <pre><code>X = np.array([x1, x2, x1*x2, x1**2, x2**2]).T\nX[:,1][y] -= 1\ny = X[:,0]**2 - X[:,1]&gt;1\n\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, shuffle=True, stratify=y, random_state=0)\nlr_poly = LogisticRegression()\nlr_poly.fit(X_train, y_train)\nlr_poly_pred = lr_poly.predict(X_test)\naccuracy_score(y_test, lr_poly_pred)\n</code></pre> <pre><code>1.0\n</code></pre> <pre><code>fig, ax = subplots(figsize=(4,4))\nax.scatter(X[:,0], X[:,1], c=lr_poly.predict(X))\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x703845e4c050&gt;\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Regularization%20with%20Lasso%20and%20Ridge/","title":"Regularization with Lasso and Ridge","text":"<p>This is an exercise inspired from \"An Introduction to Statistical Learning\" book on section 6 problem 8. 1. First we will create our own data with uniform distribution and select our own coefficient Y = c1x + c2x^2 + c3x^3 + noise 2. Comparing regular least square, ridge, and lasso to model the previously selected coefficient 3. Try reducing training data and increase the number of parameter to see the regularization in action</p> <pre><code>import numpy as np\nimport pandas as pd\n\nfrom matplotlib.pyplot import subplots\nimport sklearn.linear_model as skl\nimport sklearn.model_selection as skm\nfrom sklearn.metrics import r2_score as R2, mean_squared_error as MSE\n</code></pre>"},{"location":"Regularization%20with%20Lasso%20and%20Ridge/#1-here-we-create-y-c1x-c2x2-c3x3-noise","title":"1. Here we create Y = c1x + c2x^2 + c3x^3 + noise","text":"<p>We choose c1 = 0.1, c2 = 0.2, and c3 = -7</p> <pre><code>np.random.seed(0)\nx = np.random.uniform(-3, 3, size=100)\nX = np.array([x, x**2, x**3])\n\ntheta = np.array([0.1, 0.1, -7])\nnoise = np.random.normal(scale=30, size=100)\n\nY = np.dot(theta, X) + noise\nX_train, X_test, y_train, y_test = skm.train_test_split(X.T, Y, test_size=0.2)\n</code></pre> <pre><code>fig, ax = subplots()\nax.scatter(x, Y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x71ea3592b7a0&gt;\n</code></pre> <p></p>"},{"location":"Regularization%20with%20Lasso%20and%20Ridge/#next-we-fit-linear-regression-ridge-and-lasso","title":"Next we fit linear regression, ridge, and lasso.","text":"<p>for ridge and lasso we do cross validation to determine best regularization parameter (alphas). We cross validate 50 alphas ranging from 10^-4 to 10^8. We also create error bar for each alphas and see the error std from 5-fold CV models.</p> <pre><code>lr = skl.LinearRegression()\nlr.fit(X_train, y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <pre><code>lambdas = 10**np.linspace(8, -4, 50)\nridge = skl.RidgeCV(store_cv_results=True, alphas=lambdas)\nridge.fit(X_train, y_train)\n</code></pre> <pre>RidgeCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]),\n        store_cv_results=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCV?Documentation for RidgeCViFitted<pre>RidgeCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]),\n        store_cv_results=True)</pre> <pre><code>err = ridge.cv_results_.mean(0)\n\nfig, ax = subplots()\nax.set_title(\"Ridge CV error bar from 5-fold cross validation\")\nax.set_xlabel(\"-log(alphas)\")\nax.set_ylabel(\"MSE\")\nax.errorbar(-np.log10(ridge.alphas), err, yerr=ridge.cv_results_.std(0)/np.sqrt(3))\nax.axvline(-np.log10(ridge.alpha_), ls='--')\n</code></pre> <pre><code>&lt;matplotlib.lines.Line2D at 0x71ea2c051310&gt;\n</code></pre> <p></p> <pre><code>lasso = skl.LassoCV(alphas=lambdas)\nlasso.fit(X_train, y_train)\n</code></pre> <pre>LassoCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.32571137e+02, 7.54312006e+01, 4.29193426e+01, 2.44205309e+01,\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCV?Documentation for LassoCViFitted<pre>LassoCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.32571137e+02, 7.54312006e+01, 4.29193426e+01, 2.44205309e+01,\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]))</pre> <pre><code>err = lasso.mse_path_.mean(1)\nfig, ax = subplots()\nax.set_xlabel(\"-log(alphas)\")\nax.set_ylabel(\"MSE\")\nax.errorbar(-np.log10(lasso.alphas_), err, yerr=lasso.mse_path_.std(1)/np.sqrt(3))\nax.axvline(-np.log10(lasso.alpha_), ls='--')\n</code></pre> <pre><code>&lt;matplotlib.lines.Line2D at 0x71ea2bcd7620&gt;\n</code></pre> <p></p>"},{"location":"Regularization%20with%20Lasso%20and%20Ridge/#results","title":"Results","text":"<ul> <li>We see all the method doing well, with about 85% R2. Because the data are not linearly independent and there is enough training data (we use 80% of data), regularization is'nt actually needed here.</li> <li>Lasso is the closest to selected coefficient. it also do variable selection and eliminate x1 and x2 as the previously selected coefficient are close to zero (0.1)</li> </ul> <pre><code>lr.coef_, ridge.coef_, lasso.coef_\n</code></pre> <pre><code>(array([-1.34480014, -1.42828863, -6.76290371]),\n array([-1.04400461, -1.01056116, -6.62594185]),\n array([-0.        , -0.        , -6.82785986]))\n</code></pre> <pre><code>lr.intercept_, ridge.intercept_, lasso.intercept_\n</code></pre> <pre><code>(10.855330811482283, 9.743354531099754, 6.7171981562675445)\n</code></pre> <pre><code>R2(lr.predict(X_test), y_test), R2(ridge.predict(X_test), y_test), R2(lasso.predict(X_test), y_test)\n</code></pre> <pre><code>(0.8585674738446629, 0.8492477120995332, 0.8531453072140727)\n</code></pre> <pre><code>MSE(lr.predict(X_test), y_test), MSE(ridge.predict(X_test), y_test), MSE(lasso.predict(X_test), y_test)\n</code></pre> <pre><code>(709.4694629066079, 724.1507926668988, 734.2033857781576)\n</code></pre> <pre><code>ridge.alpha_, lasso.alpha_\n</code></pre> <pre><code>(232.99518105153717, 13.894954943731388)\n</code></pre>"},{"location":"Regularization%20with%20Lasso%20and%20Ridge/#3-reducing-the-training-data-to-5","title":"3. Reducing the training data to 5%","text":"<p>We will try to see regularization in action with reducing training data</p> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X.T, Y, test_size=0.95, random_state=7)\n</code></pre> <pre><code>lr = skl.LinearRegression()\nlr.fit(X_train, y_train)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <pre><code>lambdas = 10**np.linspace(8, -4, 50)\nridge = skl.RidgeCV(store_cv_results=True, alphas=lambdas)\nridge.fit(X_train, y_train)\n</code></pre> <pre>RidgeCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]),\n        store_cv_results=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeCV?Documentation for RidgeCViFitted<pre>RidgeCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]),\n        store_cv_results=True)</pre> <pre><code>lasso = skl.LassoCV(alphas=lambdas)\nlasso.fit(X_train, y_train)\n</code></pre> <pre><code>/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0755285832929076, tolerance: 2.846651969885279\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.7432737100110245, tolerance: 2.846651969885279\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.874157984353236, tolerance: 2.846651969885279\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0295505120977353, tolerance: 2.846651969885279\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.022669248395857, tolerance: 2.840656590789062\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.990998055406692, tolerance: 2.840656590789062\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.156596903410318, tolerance: 2.840656590789062\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.411234382614303, tolerance: 2.840656590789062\n  model = cd_fast.enet_coordinate_descent_gram(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.6001418198169475, tolerance: 2.840656590789062\n  model = cd_fast.enet_coordinate_descent_gram(\n</code></pre> <pre>LassoCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.32571137e+02, 7.54312006e+01, 4.29193426e+01, 2.44205309e+01,\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoCV?Documentation for LassoCViFitted<pre>LassoCV(alphas=array([1.00000000e+08, 5.68986603e+07, 3.23745754e+07, 1.84206997e+07,\n       1.04811313e+07, 5.96362332e+06, 3.39322177e+06, 1.93069773e+06,\n       1.09854114e+06, 6.25055193e+05, 3.55648031e+05, 2.02358965e+05,\n       1.15139540e+05, 6.55128557e+04, 3.72759372e+04, 2.12095089e+04,\n       1.20679264e+04, 6.86648845e+03, 3.90693994e+03, 2.22299648e+03,\n       1.26485522e+03, 7.19685673e+0...\n       1.32571137e+02, 7.54312006e+01, 4.29193426e+01, 2.44205309e+01,\n       1.38949549e+01, 7.90604321e+00, 4.49843267e+00, 2.55954792e+00,\n       1.45634848e+00, 8.28642773e-01, 4.71486636e-01, 2.68269580e-01,\n       1.52641797e-01, 8.68511374e-02, 4.94171336e-02, 2.81176870e-02,\n       1.59985872e-02, 9.10298178e-03, 5.17947468e-03, 2.94705170e-03,\n       1.67683294e-03, 9.54095476e-04, 5.42867544e-04, 3.08884360e-04,\n       1.75751062e-04, 1.00000000e-04]))</pre>"},{"location":"Regularization%20with%20Lasso%20and%20Ridge/#regularization-results","title":"Regularization results","text":"<ul> <li>There is too much variance in this model because we only using 5 points to create a line with 3 variable.</li> <li>Even linear regression model are to overfit for this extreme settings.</li> <li>regularization help us to reduce a variance with tradeoff of some bias. Both ridge and lasso did pretty good with 70% R2.</li> </ul> <pre><code>R2(lr.predict(X_test), y_test), R2(ridge.predict(X_test), y_test), R2(lasso.predict(X_test), y_test)\n</code></pre> <pre><code>(0.3761038971902325, 0.716650314401583, 0.7400452801921802)\n</code></pre> <pre><code>lr.coef_, ridge.coef_, lasso.coef_\n</code></pre> <pre><code>(array([-68.5520653 , -21.42043405,   1.78968729]),\n array([-11.68913854, -13.61503784,  -9.40388045]),\n array([ -0.        , -11.18693772, -11.5483712 ]))\n</code></pre> <pre><code>ridge.alpha_, lasso.alpha_\n</code></pre> <pre><code>(0.8286427728546859, 4.498432668969444)\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Regularizaton%20Hyperparameter%20Tuning%20-%20Ridge%2C%20Lasso%2C%20PCR%2C%20PLS/","title":"Regularizaton Hyperparameter Tuning   Ridge, Lasso, PCR, PLS","text":"<ol> <li>In this exercise, we will predict the number of applications received using the other variables in the College data set.</li> <li>(a) Split the data set into a training set and a test set.</li> <li>(b) Fit a linear model using least squares on the training set, and report the test error obtained.</li> <li>(c) Fit a ridge regression model on the training set, with \u03bb chosen by cross-validation. Report the test error obtained.</li> <li>(d) Fit a lasso model on the training set, with \u03bb chosen by cross- validation. Report the test error obtained, along with the num- ber of non-zero coefficient estimates.</li> <li>(e) Fit a PCR model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.</li> <li>(f) Fit a PLS model on the training set, with M chosen by cross- validation. Report the test error obtained, along with the value of M selected by cross-validation.</li> </ol> <pre><code>import numpy as np\nimport pandas as pd\n\nfrom matplotlib.pyplot import subplots\nimport sklearn.linear_model as skl\nimport sklearn.model_selection as skm\nimport seaborn as sns\nfrom sklearn.metrics import r2_score as R2, mean_squared_error as MSE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\nfrom ISLP import load_data\n</code></pre> <pre><code>college = load_data('College')\ncollege.info()\ncollege\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 777 entries, 0 to 776\nData columns (total 18 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   Private      777 non-null    category\n 1   Apps         777 non-null    int64   \n 2   Accept       777 non-null    int64   \n 3   Enroll       777 non-null    int64   \n 4   Top10perc    777 non-null    int64   \n 5   Top25perc    777 non-null    int64   \n 6   F.Undergrad  777 non-null    int64   \n 7   P.Undergrad  777 non-null    int64   \n 8   Outstate     777 non-null    int64   \n 9   Room.Board   777 non-null    int64   \n 10  Books        777 non-null    int64   \n 11  Personal     777 non-null    int64   \n 12  PhD          777 non-null    int64   \n 13  Terminal     777 non-null    int64   \n 14  S.F.Ratio    777 non-null    float64 \n 15  perc.alumni  777 non-null    int64   \n 16  Expend       777 non-null    int64   \n 17  Grad.Rate    777 non-null    int64   \ndtypes: category(1), float64(1), int64(16)\nmemory usage: 104.2 KB\n</code></pre> Private Apps Accept Enroll Top10perc Top25perc F.Undergrad P.Undergrad Outstate Room.Board Books Personal PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate 0 Yes 1660 1232 721 23 52 2885 537 7440 3300 450 2200 70 78 18.1 12 7041 60 1 Yes 2186 1924 512 16 29 2683 1227 12280 6450 750 1500 29 30 12.2 16 10527 56 2 Yes 1428 1097 336 22 50 1036 99 11250 3750 400 1165 53 66 12.9 30 8735 54 3 Yes 417 349 137 60 89 510 63 12960 5450 450 875 92 97 7.7 37 19016 59 4 Yes 193 146 55 16 44 249 869 7560 4120 800 1500 76 72 11.9 2 10922 15 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 772 No 2197 1515 543 4 26 3089 2029 6797 3900 500 1200 60 60 21.0 14 4469 40 773 Yes 1959 1805 695 24 47 2849 1107 11520 4960 600 1250 73 75 13.3 31 9189 83 774 Yes 2097 1915 695 34 61 2793 166 6900 4200 617 781 67 75 14.4 20 8323 49 775 Yes 10705 2453 1317 95 99 5217 83 19840 6510 630 2115 96 96 5.8 49 40386 99 776 Yes 2989 1855 691 28 63 2988 1726 4990 3560 500 1250 75 75 18.1 28 4509 99 <p>777 rows \u00d7 18 columns</p> <pre><code>college.describe()\n</code></pre> Apps Accept Enroll Top10perc Top25perc F.Undergrad P.Undergrad Outstate Room.Board Books Personal PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate count 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.000000 777.00000 mean 3001.638353 2018.804376 779.972973 27.558559 55.796654 3699.907336 855.298584 10440.669241 4357.526384 549.380952 1340.642214 72.660232 79.702703 14.089704 22.743887 9660.171171 65.46332 std 3870.201484 2451.113971 929.176190 17.640364 19.804778 4850.420531 1522.431887 4023.016484 1096.696416 165.105360 677.071454 16.328155 14.722359 3.958349 12.391801 5221.768440 17.17771 min 81.000000 72.000000 35.000000 1.000000 9.000000 139.000000 1.000000 2340.000000 1780.000000 96.000000 250.000000 8.000000 24.000000 2.500000 0.000000 3186.000000 10.00000 25% 776.000000 604.000000 242.000000 15.000000 41.000000 992.000000 95.000000 7320.000000 3597.000000 470.000000 850.000000 62.000000 71.000000 11.500000 13.000000 6751.000000 53.00000 50% 1558.000000 1110.000000 434.000000 23.000000 54.000000 1707.000000 353.000000 9990.000000 4200.000000 500.000000 1200.000000 75.000000 82.000000 13.600000 21.000000 8377.000000 65.00000 75% 3624.000000 2424.000000 902.000000 35.000000 69.000000 4005.000000 967.000000 12925.000000 5050.000000 600.000000 1700.000000 85.000000 92.000000 16.500000 31.000000 10830.000000 78.00000 max 48094.000000 26330.000000 6392.000000 96.000000 100.000000 31643.000000 21836.000000 21700.000000 8124.000000 2340.000000 6800.000000 103.000000 100.000000 39.800000 64.000000 56233.000000 118.00000 <pre><code>college[\"Private\"] = [1 if p == \"Yes\" else 0 for p in college[\"Private\"]]\n</code></pre> <pre><code>sns.heatmap(college.corr())\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code>X = college.drop(columns=['Apps'])\ny = college['Apps']\n\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, shuffle=True, random_state=0)\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nlambdas = 10**np.linspace(8, -4)\n</code></pre> <pre><code>lr = skl.LinearRegression()\n\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\n</code></pre> <pre><code>ridge = skl.RidgeCV(alphas=lambdas)\nridge.fit(X_train, y_train)\nridge_pred = ridge.predict(X_test)\n</code></pre> <pre><code>lasso = skl.LassoCV(alphas=lambdas)\nlasso.fit(X_train, y_train)\nlasso_pred = lasso.predict(X_test)\n</code></pre> <pre><code>pcr = Pipeline([\n    ('pca', PCA()),\n    ('plr', skl.LinearRegression())\n])\nparam = {'pca__n_components': range(1, X_train.shape[1])}\npcr = skm.GridSearchCV(pcr, param_grid=param, scoring='r2')\npcr.fit(X_train, y_train)\npcr_pred = pcr.predict(X_test)\n</code></pre> <pre><code>pls = PLSRegression()\nparam = {'n_components': range(1, X_train.shape[1])}\npls = skm.GridSearchCV(pls, param_grid=param, scoring='r2')\npls.fit(X_train, y_train)\npls_pred = pls.predict(X_test)\n</code></pre> <pre><code>R2(y_test, lr_pred), R2(y_test, ridge_pred), R2(y_test, ridge_pred), R2(y_test, ridge_pred), R2(y_test, ridge_pred), \n</code></pre> <pre><code>(0.9002392990734523,\n 0.9002393892199767,\n 0.9002393892199767,\n 0.9002393892199767,\n 0.9002393892199767)\n</code></pre> <pre><code>MSE(lr_pred, y_test), MSE(ridge_pred, y_test), MSE(lasso_pred, y_test), MSE(pcr_pred, y_test), MSE(pls_pred, y_test), \n</code></pre> <pre><code>(1022430.0889255423,\n 1022429.1650294779,\n 1045905.0958773881,\n 1025469.2195196956,\n 1017373.0630376363)\n</code></pre> <pre><code>lr.coef_, ridge.coef_, lasso.coef_, pcr.best_estimator_.named_steps['plr'].coef_, pls.best_estimator_.coef_\n</code></pre> <pre><code>(array([-2.51082107e+02,  4.18269191e+03, -9.96433619e+02,  1.01537623e+03,\n        -3.44032537e+02,  3.64813902e+02,  8.83658018e+01, -3.03380710e+02,\n         1.91334277e+02,  2.53797453e+00, -8.05468486e+00, -1.82788586e+02,\n        -4.51727150e+01,  1.50670386e+01,  8.14678768e+00,  2.49568955e+02,\n         1.43324300e+02]),\n array([-2.51082354e+02,  4.18268552e+03, -9.96424071e+02,  1.01537358e+03,\n        -3.44030511e+02,  3.64810149e+02,  8.83657476e+01, -3.03379720e+02,\n         1.91334635e+02,  2.53800864e+00, -8.05477455e+00, -1.82788258e+02,\n        -4.51729689e+01,  1.50671183e+01,  8.14624438e+00,  2.49569258e+02,\n         1.43324391e+02]),\n array([-177.14499451, 3615.46802497,   -0.        ,  547.81793016,\n          -0.        ,   -0.        ,    0.        ,  -54.17217488,\n         120.98445659,    0.        ,   -0.        ,  -75.25305854,\n         -32.24841232,   -0.        ,   -0.        ,  178.90491257,\n          21.07618984]),\n array([ 384.41596737, 1610.37407754, -174.73373112, -582.86012121,\n        1245.63502865, -335.64403864, -390.42036453, -371.47592549,\n        -392.95424603, -193.05636542, -136.11556742,    4.19431906,\n         125.97688297,  176.81414375, 2402.96762785, 2477.93764111]),\n array([[-2.50590747e+02,  4.13036908e+03, -6.75857579e+02,\n          1.10640670e+03, -4.05532175e+02,  7.73330234e+01,\n          1.08632225e+02, -3.31449999e+02,  1.94782720e+02,\n         -1.10102229e+01,  1.66980506e+00, -2.19589615e+02,\n         -1.05852444e+01,  2.03584653e+01,  2.05234569e+01,\n          2.45313541e+02,  1.28515658e+02]]))\n</code></pre> <pre><code>ridge.alpha_, lasso.alpha_, pcr.best_params_, pls.best_params_\n</code></pre> <pre><code>(0.0001, 42.91934260128778, {'pca__n_components': 16}, {'n_components': 10})\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/","title":"Sentiment analysis with Logistic Regression, SVM, Binomial Naive Bayes, and Deep Learning on Presidential Debate","text":"<p>This exercise derived from Data Mining project in my college but i remaster it with SVM and Deep Learning. My team scraped and manually label the data before, so we just need to do a little preprocess here.</p> <pre><code>import pandas as pd\n</code></pre> <pre><code>pd.set_option('display.max_colwidth', None)\ndf = pd.read_csv(\"dataset/president_debate.csv\")\ndf = df.dropna()\ndf.head()\n</code></pre> label full_text 0 1.0 Ganjar menyebut pernyataan itu disampaikan Jokowi saat debat Capres di tahun 2019 silam. Saat itu Ganjar merupakan salah satu tim kampanye Jokowi. TAG: Jokowi | Ganjar | anies final stage | prabowo | all in 02 | Ketua KPU | Australia | agak laen | pemilu 2024 | pemilu 2019 https://t.co/QuEav9i1Bw 1 0.0 @pikiranlugu @99propaganda @bengkeldodo @Ndons_Back @BANGSAygSUJUD @_NusantaraLand_ @florieliciouss @are_inismyname @Reskiichsan8 @P4P4B0W0_2024 @AditBandit234 @kurawa Sadar lah kamu Erik jangan nyebar in isu murahan seperti itu..saya juga nonton debat terahir V dan Pak ANIS CAPRES 01 tidak punya Niat ataupun bicara akan meruba BUMN menjadi KOPERASI..VISI MISI pak ANIS RB Jelas.. 2 1.0 @WagimanDeep212_ Dri debat terakhir kemarin nih. Makin mantep dan yakin pilih ganjar mahfud. Gaspolll ykin m3nang 3 0.0 @Dy_NPR Gak perlu kami prihatin dg modelan begini. Sdh sepuh kesehatan entah. Untungnya debat terakhir P Anies berwelas asih dg tidak membuat beliau berkaca2 lagi. Kpn nabgis massal? Bnyk yg nungguin nih https://t.co/e5A9ihPqLP 4 1.0 @tempodotco Selalu suka sama pembawaannya pak Anies yang adem apalagi pas debat semalem beliau keliatan tenang dan sudah mempersiapkan diri banget yuk bisa AMIN 1 putaran aja https://t.co/5RtE6Zop33 <pre><code>df['label'].value_counts()\n</code></pre> <pre><code>label\n1.0    681\n0.0    337\nName: count, dtype: int64\n</code></pre>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#first-we-downsample-the-data-to-stratify-the-class","title":"First we downsample the data to stratify the class","text":"<pre><code>positive = df[df['label'] == 1].sample(337, random_state=0)\nnegative = df[df['label'] == 0]\ndf = pd.concat([positive, negative])\ndf.label.value_counts()\n</code></pre> <pre><code>label\n1.0    337\n0.0    337\nName: count, dtype: int64\n</code></pre> <pre><code>from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport re\nnltk.download('punkt_tab')\nnltk.download('punkt')\nnltk.download('stopwords')\n</code></pre> <pre><code>[nltk_data] Downloading package punkt_tab to /home/luqman/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package punkt to /home/luqman/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/luqman/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\nTrue\n</code></pre>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#next-we-clean-the-data","title":"Next we clean the data","text":"<p>Tweet has username, tag, and links, we will get rid of that. We also need to remove stopword because it not helping us to determine the sentiment so basically reduce the dimension of the model.</p> <pre><code>def preprocess(text):\n    text = text.lower()\n    ## remove url\n    text = re.sub(r'http\\S+|www.\\S+', '', text)\n    ## remove username dan hastag\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#\\w+', '', text)\n    ## remove non ascii\n    text = text.encode('ascii', 'ignore').decode('utf-8')\n    ## remove number and punctuation\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    ## remove stop word\n    tokens = word_tokenize(text)\n    st = set(stopwords.words('indonesian'))\n    tokens = [word for word in tokens if word not in st]\n    return ' '.join(tokens)\n\ndf['full_text'] = df['full_text'].apply(preprocess)\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['full_text'], df['label'], stratify=df['label'], shuffle=True, random_state=0)\n</code></pre>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#next-we-do-vectorization-with-tfidf","title":"Next we do vectorization with tfidf","text":"<p>It similar to bag of word but instead of count we do tf*idf which term frequency(simplest: count of that word in that document) * inverse document frequency(how much document have that word accross all documents). When the word is rare the weight is higher, tfidf suit our need to measure different word on how important that word is, so we can decide positive or negative sentiment better. From the shape of our training data, we have 2302 word and </p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer()\nX_train = tf.fit_transform(X_train)\nX_test = tf.transform(X_test)\nX_train.shape\n</code></pre> <pre><code>(505, 2302)\n</code></pre> <pre><code>from sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n</code></pre>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#comparing-model","title":"Comparing model","text":"<p>we will compare the first 3 models then compare them with deep learning after quick evaluation  - linear SVM (draw decision line counting only closer/support data)  - naive bayes (use bayes theorem with assumption all word independent and based on bernoulli/binomial distribution)  - Logistic regression (modelling logit of sentiment with maximum likelihood estimation)  - Deep Learning (neural network with hidden layer to extract feature and do logistic regression on the extracted feature) </p> <pre><code>linsvm = SVC(kernel='linear')\nlinsvm.fit(X_train, y_train)\npred_linsvm = linsvm.predict(X_test)\nprint(classification_report(y_test, pred_linsvm), accuracy_score(y_test, pred_linsvm))\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n         0.0       0.85      0.88      0.87        85\n         1.0       0.88      0.85      0.86        84\n\n    accuracy                           0.86       169\n   macro avg       0.86      0.86      0.86       169\nweighted avg       0.86      0.86      0.86       169\n 0.863905325443787\n</code></pre> <pre><code>bnb = BernoulliNB()\nbnb.fit(X_train, y_train)\npred_bnb = bnb.predict(X_test)\nprint(classification_report(y_test, pred_bnb), accuracy_score(y_test, pred_bnb))\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n         0.0       0.95      0.66      0.78        85\n         1.0       0.74      0.96      0.84        84\n\n    accuracy                           0.81       169\n   macro avg       0.84      0.81      0.81       169\nweighted avg       0.84      0.81      0.81       169\n 0.8106508875739645\n</code></pre> <pre><code>mnb = MultinomialNB()\nmnb.fit(X_train, y_train)\npred_mnb = mnb.predict(X_test)\nprint(classification_report(y_test, pred_mnb), accuracy_score(y_test, pred_mnb))\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n         0.0       0.88      0.84      0.86        85\n         1.0       0.84      0.88      0.86        84\n\n    accuracy                           0.86       169\n   macro avg       0.86      0.86      0.86       169\nweighted avg       0.86      0.86      0.86       169\n 0.8579881656804734\n</code></pre> <pre><code>lr = LogisticRegression()\nlr.fit(X_train, y_train)\npred_lr = lr.predict(X_test)\nprint(classification_report(y_test, pred_lr), accuracy_score(y_test, pred_lr))\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n         0.0       0.85      0.86      0.85        85\n         1.0       0.86      0.85      0.85        84\n\n    accuracy                           0.85       169\n   macro avg       0.85      0.85      0.85       169\nweighted avg       0.85      0.85      0.85       169\n 0.8520710059171598\n</code></pre>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#what-we-got-before-continuing-to-deep-learning","title":"What we got, before continuing to deep learning","text":"<ul> <li>From SVM results, it looks like the sentiment is linearly separable so maybe complex model like deep learning won't necessary</li> <li>Logistic regression offer balance between accuracy and interpretation, we can select most positive/negative weight to see which word is mostly determine positve and negative sentiment.</li> </ul>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#most-sentiment-word-according-to-logistic-regression-weight","title":"Most \"sentiment\" word according to Logistic Regression weight","text":"<pre><code>feature_names = tf.get_feature_names_out()\ncoefficients = lr.coef_[0] \n\n# Top word for positif and negative sentiment\ntop_pos = sorted(zip(coefficients, feature_names), reverse=True)[:10]\ntop_neg = sorted(zip(coefficients, feature_names))[:10]\n\nprint(\"Top positive words:\")\nfor coef, word in top_pos:\n    print(f\"{word}: {coef:.4f}\")\n\nprint(\"\\nTop negative words:\")\nfor coef, word in top_neg:\n    print(f\"{word}: {coef:.4f}\")\n</code></pre> <pre><code>Top positive words:\nganjar: 2.2100\nganjarmahfud: 1.9752\npranowo: 1.6685\nmahfud: 1.3405\nmalam: 1.1633\nterbaik: 1.0706\nbanget: 1.0337\nindonesia: 0.9939\nkeren: 0.9842\nrakyat: 0.9240\n\nTop negative words:\nbansos: -1.7470\nyg: -1.2866\nga: -1.2157\njokowi: -1.0811\nprabowo: -0.9891\ncovid: -0.9811\naja: -0.9744\nbermasalah: -0.9656\ndikorupsi: -0.9656\nkaesang: -0.9656\n</code></pre>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#deep-learning","title":"Deep learning","text":"<p>We will try deep learning using 1 hidden layer with 64 hidden unit, effectively try to reduce the dimension before doing logistic regression in the output layer. We will use the test set as validation.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib.pyplot import subplots\nfrom ISLP import load_data\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import Adam, RMSprop\nfrom torchmetrics import MeanSquaredError, R2Score\nfrom torchmetrics.classification import BinaryAccuracy, MulticlassAccuracy\nfrom torchinfo import summary\nimport pytorch_lightning as pl\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n</code></pre> <pre><code>class SentimentModel(pl.LightningModule):\n    def __init__(self, input_size):\n        super().__init__()\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.train_accuracy = BinaryAccuracy()\n        self.val_accuracy = BinaryAccuracy()\n        self.test_accuracy = BinaryAccuracy()\n        self.learning_rate = 0.001\n        self.model = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x).squeeze()\n\n    def _shared_step(self, batch, stage):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y)\n\n        if stage == 'train':\n            metric = self.train_accuracy\n        elif stage == 'val':\n            metric = self.val_accuracy\n        else:\n            metric = self.test_accuracy\n        metric(preds, y.float())\n        self.log(f'{stage}_loss', loss, on_epoch=True, on_step=False)\n        self.log(f'{stage}_acc', metric, on_epoch=True, on_step=False, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self._shared_step(batch, 'train')\n\n    def validation_step(self, batch, batch_idx):\n        self._shared_step(batch, 'val')\n\n    def test_step(self, batch, batch_idx):\n        self._shared_step(batch, 'test')\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=self.learning_rate)\n\nclass SentimentDataModule(pl.LightningDataModule):\n    def __init__(self, train_td, test_td, batch_size=32, num_workers=8):\n        super().__init__()\n        self.train_td = train_td\n        self.test_td = test_td\n        self.batch_size = batch_size\n        self.num_workers=num_workers\n\n    def train_dataloader(self):\n        return DataLoader(self.train_td, batch_size=self.batch_size, num_workers=self.num_workers)\n\n    def val_dataloader(self):\n        return self.test_dataloader()\n\n    def test_dataloader(self):\n        return DataLoader(self.test_td, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre> <pre><code>X_train_t = torch.tensor(X_train.toarray().astype(np.float32))\ny_train_t = torch.tensor(y_train.to_numpy().astype(np.float32))\nX_test_t = torch.tensor(X_test.toarray().astype(np.float32))\ny_test_t = torch.tensor(y_test.to_numpy().astype(np.float32))\n\ntrain_ds = TensorDataset(X_train_t, y_train_t)\ntest_ds = TensorDataset(X_test_t, y_test_t)\n</code></pre> <pre><code>sentiment_model = SentimentModel(X_train_t.shape[1])\nsentiment_dm = SentimentDataModule(train_ds, test_ds)\nsentiment_logger = CSVLogger('logs', name='sentiment')\nsentiment_trainer = Trainer(\n    deterministic=True, \n    max_epochs=30, \n    logger=sentiment_logger,\n    callbacks=EarlyStopping(monitor='val_acc', patience=5, mode='min')\n)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>sentiment_trainer.fit(sentiment_model, sentiment_dm)\n</code></pre> <pre><code>  | Name           | Type              | Params | Mode \n-------------------------------------------------------------\n0 | loss_fn        | BCEWithLogitsLoss | 0      | train\n1 | train_accuracy | BinaryAccuracy    | 0      | train\n2 | val_accuracy   | BinaryAccuracy    | 0      | train\n3 | test_accuracy  | BinaryAccuracy    | 0      | train\n4 | model          | Sequential        | 147 K  | train\n-------------------------------------------------------------\n147 K     Trainable params\n0         Non-trainable params\n147 K     Total params\n0.590     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n</code></pre> <pre><code>sentiment_trainer.test(sentiment_model, sentiment_dm)\n</code></pre> <pre><code>Testing: |                                                                                                    \u2026\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_acc            0.8461538553237915\n        test_loss           0.5141122341156006\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'test_loss': 0.5141122341156006, 'test_acc': 0.8461538553237915}]\n</code></pre> <pre><code>sentiment_results = pd.read_csv(sentiment_logger.experiment.metrics_file_path)\n\nfig, ax = subplots()\nsentiment_results.dropna(subset='train_acc').plot('epoch', 'train_acc', ax=ax)\nsentiment_results.dropna(subset='val_acc').plot('epoch', 'val_acc', ax=ax)\nbest_val = sentiment_results['val_acc'].max()\nax.axhline(best_val, linestyle='--')\nbest_val\n</code></pre> <pre><code>0.8698225021362305\n</code></pre> <p></p>"},{"location":"Sentiment%20analysis%20with%20SVM%20and%20Binomial%20NB%20on%20Presidential%20Debate/#conclusion","title":"Conclusion","text":"<ul> <li>Deep learning outperform all model with with 87% accuracy at it's peak</li> <li>It hard to interpret deep learning so a simpler model like Logistic Regression should be enough for this task with 86% accuracy</li> <li>I personally prefer simpler model for this task especially SVM for more accuracy and Logistic regression for interpretability. We have'nt tune the hyperparameter yet and the simpler model might be better after tuning because it easier to train.</li> </ul>"},{"location":"Sklearn%20-%20Logistic%20regression%2C%20LDA%2C%20QDA%2C%20KNN%2C%20Naive%20Bayes/","title":"Sklearn   Logistic regression, LDA, QDA, KNN, Naive Bayes","text":"<ol> <li>This question should be answered using the Weekly data set, which is part of the ISLP package. This data is similar in nature to the Smarket data from this chapter\u2019s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010. (a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns? (b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones? (c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression. (d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010). (e) Repeat (d) using LDA. (f) Repeat (d) using QDA. (g) Repeat (d) using KNN with K = 1. (h) Repeat (d) using naive Bayes. (i) Which of these methods appears to provide the best results on this data? (j) Experiment with different combinations of predictors, includ- ing possible transformations and interactions, for each of the methods. Report the variables, method, and associated confu- sion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.</li> </ol> <pre><code>from ISLP import load_data\n</code></pre> <pre><code>weekly = load_data(\"Weekly\")\nweekly.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1089 entries, 0 to 1088\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   Year       1089 non-null   int64   \n 1   Lag1       1089 non-null   float64 \n 2   Lag2       1089 non-null   float64 \n 3   Lag3       1089 non-null   float64 \n 4   Lag4       1089 non-null   float64 \n 5   Lag5       1089 non-null   float64 \n 6   Volume     1089 non-null   float64 \n 7   Today      1089 non-null   float64 \n 8   Direction  1089 non-null   category\ndtypes: category(1), float64(7), int64(1)\nmemory usage: 69.4 KB\n</code></pre> <pre><code>weekly.describe()\n</code></pre> Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today count 1089.000000 1089.000000 1089.000000 1089.000000 1089.000000 1089.000000 1089.000000 1089.000000 mean 2000.048669 0.150585 0.151079 0.147205 0.145818 0.139893 1.574618 0.149899 std 6.033182 2.357013 2.357254 2.360502 2.360279 2.361285 1.686636 2.356927 min 1990.000000 -18.195000 -18.195000 -18.195000 -18.195000 -18.195000 0.087465 -18.195000 25% 1995.000000 -1.154000 -1.154000 -1.158000 -1.158000 -1.166000 0.332022 -1.154000 50% 2000.000000 0.241000 0.241000 0.241000 0.238000 0.234000 1.002680 0.241000 75% 2005.000000 1.405000 1.409000 1.409000 1.409000 1.405000 2.053727 1.405000 max 2010.000000 12.026000 12.026000 12.026000 12.026000 12.026000 9.328214 12.026000 <pre><code>import seaborn as sns\nsns.pairplot(weekly, hue=\"Direction\")\n</code></pre> <pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7224845a0d40&gt;\n</code></pre> <pre><code>weekly_x = weekly.drop(columns=[\"Direction\", \"Year\", \"Today\"])\nweekly_y = weekly[\"Direction\"]\n</code></pre> <pre><code>weekly_x.corr()\n</code></pre> Lag1 Lag2 Lag3 Lag4 Lag5 Volume Lag1 1.000000 -0.074853 0.058636 -0.071274 -0.008183 -0.064951 Lag2 -0.074853 1.000000 -0.075721 0.058382 -0.072499 -0.085513 Lag3 0.058636 -0.075721 1.000000 -0.075396 0.060657 -0.069288 Lag4 -0.071274 0.058382 -0.075396 1.000000 -0.075675 -0.061075 Lag5 -0.008183 -0.072499 0.060657 -0.075675 1.000000 -0.058517 Volume -0.064951 -0.085513 -0.069288 -0.061075 -0.058517 1.000000 <pre><code>from sklearn.linear_model import LogisticRegression\nfrom ISLP import confusion_table\n</code></pre> <pre><code>lr = LogisticRegression(C=10e10, solver='liblinear')\nlr.fit(weekly_x, weekly_y)\n</code></pre> <pre>LogisticRegression(C=100000000000.0, solver='liblinear')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression(C=100000000000.0, solver='liblinear')</pre> <pre><code>lr.coef_\n</code></pre> <pre><code>array([[-0.04126692,  0.05843996, -0.01606072, -0.02778961, -0.01447045,\n        -0.02274015]])\n</code></pre> <pre><code>lr_pred = lr.predict(weekly_x)\nconfusion_table(lr_pred, weekly_y)\n</code></pre> Truth Down Up Predicted Down 54 48 Up 430 557 <pre><code>(lr_pred == weekly_y).mean()\n</code></pre> <pre><code>0.5610651974288338\n</code></pre> <pre><code>train = weekly[\"Year\"] &lt;= 2008\n</code></pre> <pre><code>X_train = (weekly[[\"Lag2\"]])[train]\ny_train = (weekly[\"Direction\"])[train]\nX_test = (weekly[[\"Lag2\"]])[~train]\ny_test = (weekly[\"Direction\"])[~train]\n</code></pre> <pre><code>lr = LogisticRegression(C=10e10, solver='liblinear')\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\nconfusion_table(lr_pred, y_test)\n</code></pre> Truth Down Up Predicted Down 9 5 Up 34 56 <pre><code>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n</code></pre> <pre><code>lda = LinearDiscriminantAnalysis()\nlda.fit(X_train, y_train)\nlda_pred = lda.predict(X_test)\nconfusion_table(lda_pred, y_test)\n</code></pre> Truth Down Up Predicted Down 9 5 Up 34 56 <pre><code>qda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train, y_train)\nqda_pred = qda.predict(X_test)\nconfusion_table(qda_pred, y_test)\n</code></pre> Truth Down Up Predicted Down 0 0 Up 43 61 <pre><code>nb = GaussianNB()\nnb.fit(X_train, y_train)\nnb_pred = nb.predict(X_test)\nconfusion_table(nb_pred, y_test)\n</code></pre> Truth Down Up Predicted Down 0 0 Up 43 61 <pre><code>for i in range(1, 10):\n    print(i)\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    knn_pred = knn.predict(X_test)\n    print(confusion_table(knn_pred, y_test))\n    print()\n</code></pre> <pre><code>1\nTruth      Down  Up\nPredicted          \nDown         22  32\nUp           21  29\n\n2\nTruth      Down  Up\nPredicted          \nDown         31  44\nUp           12  17\n\n3\nTruth      Down  Up\nPredicted          \nDown         16  19\nUp           27  42\n\n4\nTruth      Down  Up\nPredicted          \nDown         26  27\nUp           17  34\n\n5\nTruth      Down  Up\nPredicted          \nDown         16  21\nUp           27  40\n\n6\nTruth      Down  Up\nPredicted          \nDown         20  28\nUp           23  33\n\n7\nTruth      Down  Up\nPredicted          \nDown         16  20\nUp           27  41\n\n8\nTruth      Down  Up\nPredicted          \nDown         21  25\nUp           22  36\n\n9\nTruth      Down  Up\nPredicted          \nDown         17  20\nUp           26  41\n</code></pre> <pre><code>weekly[[\"Today\", \"Direction\"]]\n</code></pre> Today Direction 0 -0.270 Down 1 -2.576 Down 2 3.514 Up 3 0.712 Up 4 1.178 Up ... ... ... 1084 2.969 Up 1085 1.281 Up 1086 0.283 Up 1087 1.034 Up 1088 0.069 Up <p>1089 rows \u00d7 2 columns</p> <pre><code>\n</code></pre>"},{"location":"Tree%2C%20Bagging%2C%20Boosting%20with%20Hyperparameter%20tuning/","title":"Tree, Bagging, Boosting with Hyperparameter tuning","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection as skm\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, accuracy_score\n\nfrom ISLP import load_data , confusion_table\nfrom ISLP.models import ModelSpec as MS\nfrom ISLP.bart import BART\n</code></pre> <ol> <li>in Section 8.3.3, we applied random forests to the Boston data using max_features = 6 and using n_estimators = 100 and n_estimators = 500 . Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for max_features and n_estimators. You can model your plot after Figure 8.10. Describe the results obtained.</li> </ol> <pre><code>boston = load_data(\"Boston\")\nboston\n</code></pre> crim zn indus chas nox rm age dis rad tax ptratio lstat medv 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0 0.573 6.593 69.1 2.4786 1 273 21.0 9.67 22.4 502 0.04527 0.0 11.93 0 0.573 6.120 76.7 2.2875 1 273 21.0 9.08 20.6 503 0.06076 0.0 11.93 0 0.573 6.976 91.0 2.1675 1 273 21.0 5.64 23.9 504 0.10959 0.0 11.93 0 0.573 6.794 89.3 2.3889 1 273 21.0 6.48 22.0 505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273 21.0 7.88 11.9 <p>506 rows \u00d7 13 columns</p> <pre><code>X = boston.drop(columns=[\"medv\"])\ny = boston[\"medv\"]\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, shuffle=True, random_state=0)\n</code></pre> <pre><code>rf = RandomForestRegressor()\nparams = {'n_estimators': np.arange(50, 350, 50), 'max_features': [1, X_train.shape[1], \"sqrt\", \"log2\"]}\nmodel = skm.GridSearchCV(rf, param_grid=params, verbose=1)\nmodel.fit(X_train, y_train)\n</code></pre> <pre><code>Fitting 5 folds for each of 24 candidates, totalling 120 fits\n</code></pre> <pre>GridSearchCV(estimator=RandomForestRegressor(),\n             param_grid={'max_features': [1, 12, 'sqrt', 'log2'],\n                         'n_estimators': array([ 50, 100, 150, 200, 250, 300])},\n             verbose=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(estimator=RandomForestRegressor(),\n             param_grid={'max_features': [1, 12, 'sqrt', 'log2'],\n                         'n_estimators': array([ 50, 100, 150, 200, 250, 300])},\n             verbose=1)</pre> best_estimator_: RandomForestRegressor<pre>RandomForestRegressor(max_features='log2', n_estimators=50)</pre> RandomForestRegressor?Documentation for RandomForestRegressor<pre>RandomForestRegressor(max_features='log2', n_estimators=50)</pre> <pre><code>df = pd.DataFrame(model.cv_results_[\"params\"])\ndf[\"r2\"] = model.cv_results_[\"mean_test_score\"]\ngrp = df.groupby(\"max_features\")\n</code></pre> <pre><code>_, ax = plt.subplots()\nfor name, group in df.groupby('max_features'):\n    ax.plot(group[\"n_estimators\"], group[\"r2\"], label=name, marker='o')\n\nplt.title('Mean Test Score vs. n_estimators (Grouped by max_features)')\nplt.xlabel('n_estimators')\nplt.ylabel('mean_test_score')\nplt.legend(title='max_features')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n</code></pre> <p></p> <pre><code>pred = model.best_estimator_.predict(X_test)\nr2_score(y_test, pred), mean_squared_error(y_test, pred)\n</code></pre> <pre><code>(0.7790723677094152, 16.749676711462456)\n</code></pre> <ol> <li>in the lab, a classification tree was applied to the Carseats data set af- ter converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable</li> <li>(a) Split the data set into a training set and a test set.</li> <li>(b) Fit a regression tree to the training set. Plot the tree, and inter- pret the results. What test MSE do you obtain?</li> <li>(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?</li> <li>(d) Use the bagging approach in order to analyze this data. What</li> <li>test MSE do you obtain? Use the feature_importance_ values to determine which variables are most important.</li> <li>(e) Use random forests to analyze this data. What test MSE do you obtain? Use the feature_importance_ values to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.</li> <li>(f) Now analyze the data using BART, and report your results.</li> </ol> <pre><code>carseats = load_data(\"Carseats\")\ncarseats.head()\n</code></pre> Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US 0 9.50 138 73 11 276 120 Bad 42 17 Yes Yes 1 11.22 111 48 16 260 83 Good 65 10 Yes Yes 2 10.06 113 35 10 269 80 Medium 59 12 Yes Yes 3 7.40 117 100 4 466 97 Medium 55 14 Yes Yes 4 4.15 141 64 3 340 128 Bad 38 13 Yes No <pre><code>y = carseats[\"Sales\"]\nX = pd.get_dummies(carseats.drop(columns=\"Sales\"), columns=[\"ShelveLoc\", \"Urban\", \"US\"], drop_first=True)\nX.head()\n</code></pre> CompPrice Income Advertising Population Price Age Education ShelveLoc_Good ShelveLoc_Medium Urban_Yes US_Yes 0 138 73 11 276 120 42 17 False False True True 1 111 48 16 260 83 65 10 True False True True 2 113 35 10 269 80 59 12 False True True True 3 117 100 4 466 97 55 14 False True True True 4 141 64 3 340 128 38 13 False False True False <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=0, shuffle=True)\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=0)\n</code></pre> <pre><code>dtr = DecisionTreeRegressor(random_state=0)\ndtr.fit(X_train, y_train)\ndtr_pred = dtr.predict(X_test)\nmean_squared_error(y_test, dtr_pred), r2_score(y_test, dtr_pred)\n</code></pre> <pre><code>(6.007378000000001, 0.18420298947019487)\n</code></pre> <pre><code>ccp = dtr.cost_complexity_pruning_path(X_train, y_train)['ccp_alphas']\npruned = skm.GridSearchCV(dtr, param_grid={'ccp_alpha': ccp}, cv=kfold, scoring=\"neg_mean_squared_error\")\npruned.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(random_state=0),\n             param_grid={'ccp_alpha': array([0.00000000e+00, 2.50000000e-07, 1.00000000e-06, 1.00000000e-06,\n       2.25000000e-06, 2.25000000e-06, 4.00000000e-06, 4.00000000e-06,\n       9.00000000e-06, 9.00000000e-06, 9.00000000e-06, 9.00000000e-06,\n       1.22500000e-05, 1.60000000e-0...\n       6.04006667e-02, 6.37144802e-02, 6.85004135e-02, 7.09593750e-02,\n       7.10987292e-02, 7.40173444e-02, 7.45467149e-02, 7.81200795e-02,\n       1.05294704e-01, 1.27332857e-01, 1.65842886e-01, 1.81486201e-01,\n       1.86376688e-01, 2.52891440e-01, 2.59843794e-01, 2.67189360e-01,\n       3.46654183e-01, 3.76660787e-01, 6.13656686e-01, 6.43168488e-01,\n       1.07979677e+00, 1.53159950e+00])},\n             scoring='neg_mean_squared_error')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(random_state=0),\n             param_grid={'ccp_alpha': array([0.00000000e+00, 2.50000000e-07, 1.00000000e-06, 1.00000000e-06,\n       2.25000000e-06, 2.25000000e-06, 4.00000000e-06, 4.00000000e-06,\n       9.00000000e-06, 9.00000000e-06, 9.00000000e-06, 9.00000000e-06,\n       1.22500000e-05, 1.60000000e-0...\n       6.04006667e-02, 6.37144802e-02, 6.85004135e-02, 7.09593750e-02,\n       7.10987292e-02, 7.40173444e-02, 7.45467149e-02, 7.81200795e-02,\n       1.05294704e-01, 1.27332857e-01, 1.65842886e-01, 1.81486201e-01,\n       1.86376688e-01, 2.52891440e-01, 2.59843794e-01, 2.67189360e-01,\n       3.46654183e-01, 3.76660787e-01, 6.13656686e-01, 6.43168488e-01,\n       1.07979677e+00, 1.53159950e+00])},\n             scoring='neg_mean_squared_error')</pre> best_estimator_: DecisionTreeRegressor<pre>DecisionTreeRegressor(ccp_alpha=0.07401734444444534, random_state=0)</pre> DecisionTreeRegressor?Documentation for DecisionTreeRegressor<pre>DecisionTreeRegressor(ccp_alpha=0.07401734444444534, random_state=0)</pre> <pre><code>pruned_pred = pruned.best_estimator_.predict(X_test)\nmean_squared_error(y_test, pruned_pred), r2_score(y_test, pruned_pred)\n</code></pre> <pre><code>(5.377231455417805, 0.2697763739427288)\n</code></pre> <pre><code>dtr.get_n_leaves(), pruned.best_estimator_.get_n_leaves()\n</code></pre> <pre><code>(200, 18)\n</code></pre> <pre><code>_, ax = plt.subplots()\nax.plot(ccp, pruned.cv_results_[\"mean_test_score\"])\nax.axvline(pruned.best_params_[\"ccp_alpha\"]), pruned.best_params_[\"ccp_alpha\"]\n</code></pre> <pre><code>(&lt;matplotlib.lines.Line2D at 0x7f6d054b07a0&gt;, 0.07401734444444534)\n</code></pre> <p></p> <pre><code>params = {\"max_features\": [X_train.shape[1], \"sqrt\", \"log2\"], \"n_estimators\": [50, 100, 200, 500]}\nrf = skm.GridSearchCV(RandomForestRegressor(), param_grid=params, cv=kfold, scoring=\"neg_mean_squared_error\")\nrf.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=RandomForestRegressor(),\n             param_grid={'max_features': [11, 'sqrt', 'log2'],\n                         'n_estimators': [50, 100, 200, 500]},\n             scoring='neg_mean_squared_error')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=RandomForestRegressor(),\n             param_grid={'max_features': [11, 'sqrt', 'log2'],\n                         'n_estimators': [50, 100, 200, 500]},\n             scoring='neg_mean_squared_error')</pre> best_estimator_: RandomForestRegressor<pre>RandomForestRegressor(max_features=11, n_estimators=500)</pre> RandomForestRegressor?Documentation for RandomForestRegressor<pre>RandomForestRegressor(max_features=11, n_estimators=500)</pre> <pre><code>rf_pred = rf.best_estimator_.predict(X_test)\nmean_squared_error(y_test, rf_pred), r2_score(y_test, rf_pred)\n</code></pre> <pre><code>(2.656156527654005, 0.6392961197448597)\n</code></pre> <pre><code>df = pd.DataFrame(rf.cv_results_[\"params\"])\ndf[\"neg_mean_squared_error\"] = rf.cv_results_[\"mean_test_score\"]\ngrp = df.groupby(\"max_features\")\n\n_, ax = plt.subplots()\nfor name, group in df.groupby(\"max_features\"):\n    ax.plot(group[\"n_estimators\"], group[\"neg_mean_squared_error\"], label=name, marker='o')\n\nplt.legend()\nplt.title('Negative MSE vs n_estimators')\nplt.xlabel('n_estimators')\nplt.ylabel('neg mse')\nplt.legend(title='max_features')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <pre><code>pd.DataFrame({\"importance\": rf.best_estimator_.feature_importances_}, index=X.columns).sort_values(by=\"importance\", ascending=False)\n</code></pre> importance Price 0.334300 ShelveLoc_Good 0.154064 Age 0.113329 CompPrice 0.087943 ShelveLoc_Medium 0.080999 Income 0.068796 Population 0.058244 Advertising 0.057147 Education 0.032739 US_Yes 0.007296 Urban_Yes 0.005143 <ol> <li>We now use boosting to predict Salary in the Hitters data set. (a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries. (b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations. (c) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter \u03bb. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis. (d) Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis. (e) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6. (f) Which variables appear to be the most important predictors in the boosted model? (g) Now apply bagging to the training set. What is the test set MSE for this approach?</li> </ol> <pre><code>hitters = load_data(\"Hitters\")\nhitters.head()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague 0 293 66 1 30 29 14 1 293 66 1 30 29 14 A E 446 33 20 NaN A 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N 4 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N <pre><code>hitters.describe()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks PutOuts Assists Errors Salary count 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.00000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 263.000000 mean 380.928571 101.024845 10.770186 50.909938 48.027950 38.742236 7.444099 2648.68323 717.571429 69.490683 358.795031 330.118012 260.239130 288.937888 106.913043 8.040373 535.925882 std 153.404981 46.454741 8.709037 26.024095 26.166895 21.639327 4.926087 2324.20587 654.472627 86.266061 334.105886 333.219617 267.058085 280.704614 136.854876 6.368359 451.118681 min 16.000000 1.000000 0.000000 0.000000 0.000000 0.000000 1.000000 19.00000 4.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 67.500000 25% 255.250000 64.000000 4.000000 30.250000 28.000000 22.000000 4.000000 816.75000 209.000000 14.000000 100.250000 88.750000 67.250000 109.250000 7.000000 3.000000 190.000000 50% 379.500000 96.000000 8.000000 48.000000 44.000000 35.000000 6.000000 1928.00000 508.000000 37.500000 247.000000 220.500000 170.500000 212.000000 39.500000 6.000000 425.000000 75% 512.000000 137.000000 16.000000 69.000000 64.750000 53.000000 11.000000 3924.25000 1059.250000 90.000000 526.250000 426.250000 339.250000 325.000000 166.000000 11.000000 750.000000 max 687.000000 238.000000 40.000000 130.000000 121.000000 105.000000 24.000000 14053.00000 4256.000000 548.000000 2165.000000 1659.000000 1566.000000 1378.000000 492.000000 32.000000 2460.000000 <pre><code>hitters = hitters.dropna()\nX = pd.get_dummies(hitters.drop(columns=[\"Salary\"]), columns=X.select_dtypes(include=[\"category\"]).columns)\ny = np.log(hitters[\"Salary\"])\n\nn_train = 200\nX_train = X[:n_train]\nX_test = X[n_train:]\ny_train = y[:n_train]\ny_test = y[n_train:]\n</code></pre> <pre><code>alphas = 10**np.linspace(0, -5, 10)\n_, ax = plt.subplots()\nax.plot(np.linspace(0, -5, 10), alphas, marker=\"o\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x7f6d0393fad0&gt;]\n</code></pre> <p></p> <pre><code>gbr = skm.GridSearchCV(GradientBoostingRegressor(n_estimators=1000), param_grid={\"learning_rate\": alphas}, cv=kfold, scoring=\"neg_mean_squared_error\")\ngbr.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=GradientBoostingRegressor(n_estimators=1000),\n             param_grid={'learning_rate': array([1.00000000e+00, 2.78255940e-01, 7.74263683e-02, 2.15443469e-02,\n       5.99484250e-03, 1.66810054e-03, 4.64158883e-04, 1.29154967e-04,\n       3.59381366e-05, 1.00000000e-05])},\n             scoring='neg_mean_squared_error')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=GradientBoostingRegressor(n_estimators=1000),\n             param_grid={'learning_rate': array([1.00000000e+00, 2.78255940e-01, 7.74263683e-02, 2.15443469e-02,\n       5.99484250e-03, 1.66810054e-03, 4.64158883e-04, 1.29154967e-04,\n       3.59381366e-05, 1.00000000e-05])},\n             scoring='neg_mean_squared_error')</pre> best_estimator_: GradientBoostingRegressor<pre>GradientBoostingRegressor(learning_rate=0.005994842503189409, n_estimators=1000)</pre> GradientBoostingRegressor?Documentation for GradientBoostingRegressor<pre>GradientBoostingRegressor(learning_rate=0.005994842503189409, n_estimators=1000)</pre> <pre><code>gbr_pred = gbr.best_estimator_.predict(X_test)\nmean_squared_error(y_test, gbr_pred), r2_score(y_test, gbr_pred)\n</code></pre> <pre><code>(0.21235784885823164, 0.6705746637944561)\n</code></pre> <pre><code>_, ax = plt.subplots()\nax.plot(alphas, gbr.cv_results_[\"mean_test_score\"])\nax.axvline(gbr.best_params_[\"learning_rate\"], linestyle='--'), gbr.best_params_[\"learning_rate\"]\n</code></pre> <pre><code>(&lt;matplotlib.lines.Line2D at 0x7f6cfe04e6c0&gt;, 0.005994842503189409)\n</code></pre> <p></p> <pre><code>caravan = load_data(\"Caravan\")\ncaravan.head()\n</code></pre> MOSTYPE MAANTHUI MGEMOMV MGEMLEEF MOSHOOFD MGODRK MGODPR MGODOV MGODGE MRELGE ... APERSONG AGEZONG AWAOREG ABRAND AZEILPL APLEZIER AFIETS AINBOED ABYSTAND Purchase 0 33 1 3 2 8 0 5 1 3 7 ... 0 0 0 1 0 0 0 0 0 No 1 37 1 2 2 8 1 4 1 4 6 ... 0 0 0 1 0 0 0 0 0 No 2 37 1 2 2 8 0 4 2 4 3 ... 0 0 0 1 0 0 0 0 0 No 3 9 1 3 3 3 2 3 2 4 5 ... 0 0 0 1 0 0 0 0 0 No 4 40 1 4 2 10 1 4 1 4 7 ... 0 0 0 1 0 0 0 0 0 No <p>5 rows \u00d7 86 columns</p> <pre><code>X = caravan.drop(columns=[\"Purchase\"])\ny = caravan[\"Purchase\"]\n\nn_train = 1000\nX_train = X[:n_train]\nX_test = X[n_train:]\ny_train = y[:n_train]\ny_test = y[n_train:]\n</code></pre> <pre><code>gbr = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01)\ngbr.fit(X_train, y_train)\n</code></pre> <pre>GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFitted<pre>GradientBoostingClassifier(learning_rate=0.01, n_estimators=1000)</pre> <pre><code>gbr_pred = np.where(gbr.predict_proba(X_test)[:,1] &gt; 0.2, 'Yes', 'No')\nconfusion_matrix(y_test, gbr_pred), accuracy_score(y_test, gbr_pred)\n</code></pre> <pre><code>(array([[4334,  199],\n        [ 251,   38]]),\n 0.9066777270841975)\n</code></pre> <pre><code>gbr_pred = np.where(gbr.predict_proba(X_test)[:,1] &gt; 0.5, 'Yes', 'No')\nconfusion_matrix(y_test, gbr_pred), accuracy_score(y_test, gbr_pred)\n</code></pre> <pre><code>(array([[4489,   44],\n        [ 275,   14]]),\n 0.933844877644131)\n</code></pre> <pre><code>38/(199+38),14/(14+44)\n</code></pre> <pre><code>(0.16033755274261605, 0.2413793103448276)\n</code></pre> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_pred = np.where(lr.predict_proba(X_test)[:,1] &gt; 0.2, 'Yes', 'No')\nconfusion_matrix(y_test, lr_pred), accuracy_score(y_test, lr_pred)\n</code></pre> <pre><code>/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n(array([[4305,  228],\n        [ 240,   49]]),\n 0.9029448361675654)\n</code></pre> <pre><code>knn = skm.GridSearchCV(KNeighborsClassifier(), param_grid={'n_neighbors':range(1,11)}, scoring='precision')\nknn.fit(X_train, y_train)\n</code></pre> <pre><code>/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:960: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n    scores = scorer(estimator, X_test, y_test, **score_params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 288, in __call__\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n    y_pred = method_caller(\n             ^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n    result, _ = _get_response_values(\n                ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n    raise ValueError(\nValueError: pos_label=1 is not a valid label: It should be one of ['No' 'Yes']\n\n  warnings.warn(\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n  warnings.warn(\n</code></pre> <pre>GridSearchCV(estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': range(1, 11)}, scoring='precision')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': range(1, 11)}, scoring='precision')</pre> best_estimator_: KNeighborsClassifier<pre>KNeighborsClassifier(n_neighbors=1)</pre> KNeighborsClassifier?Documentation for KNeighborsClassifier<pre>KNeighborsClassifier(n_neighbors=1)</pre> <pre><code>knn_pred = knn.best_estimator_.predict(X_test)\nconfusion_matrix(y_test, knn_pred), accuracy_score(y_test, knn_pred)\n</code></pre> <pre><code>(array([[4262,  271],\n        [ 262,   27]]),\n 0.8894649523019494)\n</code></pre> <pre><code>49/(228+49), 27/(271+27)\n</code></pre> <pre><code>(0.17689530685920576, 0.09060402684563758)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"about/","title":"\ud83d\udc4b Muhammad Luqman Hakim","text":"<p>\ud83c\udf93 Computer Science Student | \ud83e\udd16 Machine Learning Enthusiast \ud83d\udccd IPB University | \ud83d\udca1 Lifelong Learner  </p> <p>Welcome to my Machine Learning Journey \u2014 a space where I document the projects, lessons, and experiments I've explored as I deepen my understanding of data-driven systems.</p> <p>My focus lies in: - Building intelligent systems using neural networks and deep learning. - Solving real-world problems with predictive models. - Bridging theory and practice through hands-on experimentation.</p>"},{"location":"about/#highlights","title":"\ud83d\udd0d Highlights","text":"<ul> <li>\ud83d\udcc8 Applied time series models for financial data forecasting.</li> <li>\ud83d\udcdd Conducted sentiment analysis using NLP and TF-IDF/Embeddings.</li> <li>\ud83e\udde0 Built Simple Machine Learning but using Neural Network.</li> </ul>"},{"location":"about/#contact","title":"\ud83d\udceb Contact","text":"<ul> <li>\ud83d\udce7 Email: hakim.luqman.muhammad@gmail.com </li> <li>\ud83d\udcac WhatsApp: wa.me/6285778598244</li> </ul>"},{"location":"z_ISLP%20demo10/","title":"z ISLP demo10","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport sklearn.model_selection as skm\nfrom matplotlib.pyplot import subplots\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\nfrom ISLP import load_data\n</code></pre> <pre><code>import torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import RMSprop\n\nfrom torchmetrics import MeanAbsoluteError, R2Score\nfrom torchmetrics.classification import MulticlassAccuracy, BinaryAccuracy\nfrom torchinfo import summary\nfrom torchvision.io import read_image\n\nfrom torchvision.datasets import CIFAR100, MNIST\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision.transforms import Resize, Normalize, CenterCrop, ToTensor\n</code></pre> <pre><code>import pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n</code></pre> <pre><code>from ISLP.torch import SimpleDataModule, SimpleModule, ErrorTracker, rec_num_workers\nfrom ISLP.torch.imdb import load_lookup, load_tensor, load_sparse, load_sequential\nfrom glob import glob\nimport json\n</code></pre> <pre><code>hitters = load_data(\"Hitters\")\nhitters.head()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague 0 293 66 1 30 29 14 1 293 66 1 30 29 14 A E 446 33 20 NaN A 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N 4 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N <pre><code>hitters = hitters.dropna()\nhitters = pd.get_dummies(hitters, columns=hitters.select_dtypes(include=[\"category\"]).columns, drop_first=True)\nX = hitters.drop(columns=[\"Salary\"])\ny = hitters[\"Salary\"]\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, shuffle=True, random_state=0)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n</code></pre> <pre><code>lr = LinearRegression().fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\nnp.fabs(lr_pred-y_test).mean()\n</code></pre> <pre><code>265.87464986111604\n</code></pre> <pre><code>n = X_train.shape[0]\nlam_max = np.fabs(X_train.T.dot(y_train - y_train.mean())).max() / n\nparam_grid = {'alpha': np.exp(np.linspace (0, np.log (0.01) , 100))* lam_max}\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=0)\n</code></pre> <pre><code>lasso = skm.GridSearchCV(Lasso(), param_grid=param_grid, cv=kfold, scoring=\"neg_mean_absolute_error\")\nlasso.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Lasso(),\n             param_grid={'alpha': array([216.42356927, 206.58678403, 197.19709587, 188.23418352,\n       179.67864937, 171.51197745, 163.71649337, 156.27532608,\n       149.17237132, 142.39225682, 135.92030899, 129.74252118,\n       123.84552335, 118.21655318, 112.84342839, 107.71452041,\n       102.81872922,  98.14545929,  93.684596...\n         9.15307838,   8.73705684,   8.33994413,   7.96088079,\n         7.59904648,   7.25365808,   6.92396813,   6.60926309,\n         6.30886188,   6.02211438,   5.74839998,   5.48712633,\n         5.23772797,   4.99966515,   4.77242265,   4.55550868,\n         4.34845378,   4.15080984,   3.96214913,   3.78206334,\n         3.61016272,   3.44607525,   3.28944582,   3.13993543,\n         2.99722052,   2.86099222,   2.73095571,   2.60682955,\n         2.48834513,   2.375246  ,   2.2672874 ,   2.16423569])},\n             scoring='neg_mean_absolute_error')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=Lasso(),\n             param_grid={'alpha': array([216.42356927, 206.58678403, 197.19709587, 188.23418352,\n       179.67864937, 171.51197745, 163.71649337, 156.27532608,\n       149.17237132, 142.39225682, 135.92030899, 129.74252118,\n       123.84552335, 118.21655318, 112.84342839, 107.71452041,\n       102.81872922,  98.14545929,  93.684596...\n         9.15307838,   8.73705684,   8.33994413,   7.96088079,\n         7.59904648,   7.25365808,   6.92396813,   6.60926309,\n         6.30886188,   6.02211438,   5.74839998,   5.48712633,\n         5.23772797,   4.99966515,   4.77242265,   4.55550868,\n         4.34845378,   4.15080984,   3.96214913,   3.78206334,\n         3.61016272,   3.44607525,   3.28944582,   3.13993543,\n         2.99722052,   2.86099222,   2.73095571,   2.60682955,\n         2.48834513,   2.375246  ,   2.2672874 ,   2.16423569])},\n             scoring='neg_mean_absolute_error')</pre> best_estimator_: Lasso<pre>Lasso(alpha=9.153078381962859)</pre> Lasso?Documentation for Lasso<pre>Lasso(alpha=9.153078381962859)</pre> <pre><code>lasso_pred = lasso.best_estimator_.predict(X_test)\nnp.fabs(lasso_pred-y_test).mean()\n</code></pre> <pre><code>254.4028106605372\n</code></pre> <pre><code>y.describe()\n</code></pre> <pre><code>count     263.000000\nmean      535.925882\nstd       451.118681\nmin        67.500000\n25%       190.000000\n50%       425.000000\n75%       750.000000\nmax      2460.000000\nName: Salary, dtype: float64\n</code></pre> <pre><code>class HittersModel(nn.Module):\n    def __init__(self, input_size):\n        super(HittersModel, self).__init__()\n        self.flatten = nn.Flatten()\n        self.sequential = nn.Sequential(\n            nn.Linear(input_size, 50),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(50, 1)\n        )\n\n    def forward(self, X):\n        X = self.flatten(X)\n        return torch.flatten(self.sequential(X))\n</code></pre> <pre><code>hitters_model = HittersModel(X_train.shape[1])\nsummary(hitters_model)\n</code></pre> <pre><code>=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nHittersModel                             --\n\u251c\u2500Flatten: 1-1                           --\n\u251c\u2500Sequential: 1-2                        --\n\u2502    \u2514\u2500Linear: 2-1                       1,000\n\u2502    \u2514\u2500ReLU: 2-2                         --\n\u2502    \u2514\u2500Dropout: 2-3                      --\n\u2502    \u2514\u2500Linear: 2-4                       51\n=================================================================\nTotal params: 1,051\nTrainable params: 1,051\nNon-trainable params: 0\n=================================================================\n</code></pre> <pre><code>X_train_t = torch.tensor(X_train.astype(np.float32))\ny_train_t = torch.tensor(y_train.to_numpy().astype(np.float32))\nhit_train = TensorDataset(X_train_t, y_train_t)\n\nX_test_t = torch.tensor(X_test.astype(np.float32))\ny_test_t = torch.tensor(y_test.to_numpy().astype(np.float32))\nhit_test = TensorDataset(X_test_t, y_test_t)\n</code></pre> <pre><code>hit_dm = SimpleDataModule(hit_train, hit_test, batch_size=32, num_workers=min(4, rec_num_workers()), validation=hit_test)\nhit_module = SimpleModule.regression(hitters_model, metrics ={'mae':MeanAbsoluteError()})\nhit_logger = CSVLogger('logs', name='hitters')\n</code></pre> <pre><code>hit_trainer = Trainer(deterministic=True, max_epochs=50, log_every_n_steps=5, logger=hit_logger, callbacks=[ErrorTracker()])\nhit_trainer.fit(hit_module , datamodule=hit_dm)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n  | Name  | Type         | Params | Mode \n-----------------------------------------------\n0 | model | HittersModel | 1.1 K  | train\n1 | loss  | MSELoss      | 0      | train\n-----------------------------------------------\n1.1 K     Trainable params\n0         Non-trainable params\n1.1 K     Total params\n0.004     Total estimated model params size (MB)\n8         Modules in train mode\n0         Modules in eval mode\n\n`Trainer.fit` stopped: `max_epochs=50` reached.\n</code></pre> <pre><code>hit_trainer.test(hit_module, datamodule=hit_dm)\n</code></pre> <pre><code>Testing: |                                                | 0/? [00:00&lt;?, ?it/s]\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_loss              135539.171875\n        test_mae             271.5210876464844\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n[{'test_loss': 135539.171875, 'test_mae': 271.5210876464844}]\n</code></pre> <pre><code>hit_results = pd.read_csv(hit_logger.experiment.metrics_file_path)\n</code></pre> <pre><code>def summary_plot(results, ax, col='loss', valid_legend='Validation', training_legend='Training', ylabel='Loss', fontsize =20):\n    for (column, color , label) in zip([f'train_{col}_epoch', f'valid_{col}'], ['black','red'], [training_legend, valid_legend]):\n        results.plot(x='epoch', y=column, label=label, marker='o', color=color , ax=ax)\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel(ylabel)\n        return ax\n</code></pre> <pre><code>fig , ax = subplots (1, 1, figsize =(6, 6))\nax = summary_plot(hit_results, ax , col='mae', ylabel='MAE', valid_legend='Validation (=Test)')\nax.set_ylim ([0, 400])\nax.set_xticks(np.linspace (0, 50, 11).astype(int));\n</code></pre> <pre><code>hitters_model.eval()\npreds = hit_module(X_test_t)\ntorch.abs(y_test_t - preds).mean()\n</code></pre> <pre><code>tensor(271.5211, grad_fn=&lt;MeanBackward0&gt;)\n</code></pre> <pre><code>(mnist_train, mnist_test) = [MNIST(root='data', train=train, download=True, transform=ToTensor()) for train in [True, False]]\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.91M/9.91M [00:12&lt;00:00, 766kB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.9k/28.9k [00:00&lt;00:00, 70.4kB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.65M/1.65M [00:07&lt;00:00, 220kB/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54k/4.54k [00:00&lt;00:00, 9.59MB/s]\n</code></pre> <pre><code>class MNISTModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.train_acc = MulticlassAccuracy(num_classes=10)\n        self.val_acc = MulticlassAccuracy(num_classes=10)\n        self.layer1 = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(28*28, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n        )\n        self.model = nn.Sequential(\n            self.layer1, \n            self.layer2,\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y)\n        acc = self.train_acc(preds, y)\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y)\n        acc = self.val_acc(preds, y)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", acc, on_step=False,on_epoch=True, prog_bar=True)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.001)\n</code></pre> <pre><code>class MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, train, test, batch_size=32, validation=None):\n        super().__init__()\n        self.train = train\n        self.test = test\n        self.val = validation or self.test\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        return DataLoader(self.train, batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return DataLoader(self.val, batch_size=self.batch_size)\n\n</code></pre> <pre><code>mnist_dm = MNISTDataModule(mnist_train, mnist_test, batch_size=256)\nmnist_model = MNISTModel()\nmnist_logger = CSVLogger('logs', name='MNIST')\n</code></pre> <pre><code>for idx , (X_ ,Y_) in enumerate(mnist_dm.train_dataloader()):\n    print('X: ', X_.shape)\n    print('Y: ', Y_.shape)\n    if idx &gt;= 1:\n        break\n\nsummary(mnist_model, input_data=X_, col_names=['input_size', 'output_size', 'num_params'])\n</code></pre> <pre><code>X:  torch.Size([256, 1, 28, 28])\nY:  torch.Size([256])\nX:  torch.Size([256, 1, 28, 28])\nY:  torch.Size([256])\n\n===================================================================================================================\nLayer (type:depth-idx)                   Input Shape               Output Shape              Param #\n===================================================================================================================\nMNISTModel                               [256, 1, 28, 28]          [256, 10]                 --\n\u251c\u2500Sequential: 1-1                        [256, 1, 28, 28]          [256, 10]                 --\n\u2502    \u2514\u2500Sequential: 2-1                   [256, 1, 28, 28]          [256, 256]                --\n\u2502    \u2502    \u2514\u2500Flatten: 3-1                 [256, 1, 28, 28]          [256, 784]                --\n\u2502    \u2502    \u2514\u2500Linear: 3-2                  [256, 784]                [256, 256]                200,960\n\u2502    \u2502    \u2514\u2500ReLU: 3-3                    [256, 256]                [256, 256]                --\n\u2502    \u2502    \u2514\u2500Dropout: 3-4                 [256, 256]                [256, 256]                --\n\u2502    \u2514\u2500Sequential: 2-2                   [256, 256]                [256, 128]                --\n\u2502    \u2502    \u2514\u2500Linear: 3-5                  [256, 256]                [256, 128]                32,896\n\u2502    \u2502    \u2514\u2500ReLU: 3-6                    [256, 128]                [256, 128]                --\n\u2502    \u2502    \u2514\u2500Dropout: 3-7                 [256, 128]                [256, 128]                --\n\u2502    \u2514\u2500Linear: 2-3                       [256, 128]                [256, 10]                 1,290\n===================================================================================================================\nTotal params: 235,146\nTrainable params: 235,146\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 60.20\n===================================================================================================================\nInput size (MB): 0.80\nForward/backward pass size (MB): 0.81\nParams size (MB): 0.94\nEstimated Total Size (MB): 2.55\n===================================================================================================================\n</code></pre> <pre><code>mnist_trainer = Trainer(deterministic=True, max_epochs=30, logger=mnist_logger)\nmnist_trainer.fit(mnist_model, datamodule=mnist_dm)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n  | Name      | Type               | Params | Mode \n---------------------------------------------------------\n0 | loss_fn   | CrossEntropyLoss   | 0      | train\n1 | train_acc | MulticlassAccuracy | 0      | train\n2 | val_acc   | MulticlassAccuracy | 0      | train\n3 | layer1    | Sequential         | 200 K  | train\n4 | layer2    | Sequential         | 32.9 K | train\n5 | model     | Sequential         | 235 K  | train\n---------------------------------------------------------\n235 K     Trainable params\n0         Non-trainable params\n235 K     Total params\n0.941     Total estimated model params size (MB)\n14        Modules in train mode\n0         Modules in eval mode\n\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n`Trainer.fit` stopped: `max_epochs=30` reached.\n</code></pre> <pre><code>\n</code></pre> <pre><code>mnist_results = pd.read_csv(mnist_logger.experiment.metrics_file_path)\nfig, ax = subplots()\nmnist_results.dropna(subset=['val_acc']).plot(x='epoch', y='val_acc', ax=ax)\nmnist_results.dropna(subset=['train_acc']).plot(x='epoch', y='train_acc', ax=ax)\n</code></pre> <pre><code>&lt;Axes: xlabel='epoch'&gt;\n</code></pre> <pre><code>cifar_train, cifar_test = [CIFAR100(root='data', train=train, download=True) for train in [True, False]]\n</code></pre> <pre><code>cifar_train.data.shape\n</code></pre> <pre><code>(50000, 32, 32, 3)\n</code></pre> <pre><code>transform = ToTensor()\ncifar_train_X = torch.stack ([ transform(x) for x in cifar_train.data])\ncifar_test_X = torch.stack ([ transform(x) for x in cifar_test.data])\ncifar_train = TensorDataset(cifar_train_X, torch.tensor(cifar_train.targets))\ncifar_test = TensorDataset(cifar_test_X, torch.tensor(cifar_test.targets))\n</code></pre> <pre><code>class BuildingBlock(pl.LightningModule):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(\n                in_channels=in_channels, \n                out_channels=out_channels, \n                kernel_size=(3, 3), padding='same'\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2))\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass CIFARModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.accuracy = MulticlassAccuracy(num_classes=100)\n        self.learning_rate = 0.001\n        self.conv_sizes = [\n            (3, 32), # (32, 16, 16)\n            (32, 64), # (64, 8, 8)\n            (64, 128), # (128, 4, 4)\n            (128, 256) # (256, 2, 2)\n        ]\n        self.conv = nn.Sequential(\n            *[BuildingBlock(in_channel, out_channel) for in_channel, out_channel in self.conv_sizes]\n        )\n        self.dense = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.5),\n            nn.Linear(2*2*256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 100)\n        )\n\n    def forward(self, x):\n        conv = self.conv(x)\n        out = self.dense(conv)\n        return out\n\n    def step(self, batch, batch_idx, t='train'):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y)\n        acc = self.accuracy(preds, y)\n        self.log(f'{t}_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(f'{t}_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'train')\n\n    def validation_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'val')\n\n    def test_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'test')\n\n    def configure_optimizers(self):\n        return torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n\nclass CIFARDataModule(pl.LightningDataModule):\n    def __init__(self, train_t, val_t, batch_size=32):\n        super().__init__()\n        self.train_t = train_t\n        self.val_t = val_t\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        return DataLoader(self.train_t, batch_size=self.batch_size, num_workers=8)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_t, batch_size=self.batch_size, num_workers=8)\n\n    def test_dataloader(self):\n        return self.val_dataloader(batch_size=self.batch_size)\n</code></pre> <pre><code>cifar_model = CIFARModel()\ncifar_dm = CIFARDataModule(cifar_train, cifar_test, batch_size=128)\ncifar_logger = CSVLogger('logs', name='CIFAR100')\ncifar_trainer = Trainer(max_epochs=30, logger=cifar_logger)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>cifar_trainer.fit(cifar_model, datamodule=cifar_dm)\n</code></pre> <pre><code>  | Name     | Type               | Params | Mode \n--------------------------------------------------------\n0 | loss_fn  | CrossEntropyLoss   | 0      | train\n1 | accuracy | MulticlassAccuracy | 0      | train\n2 | conv     | Sequential         | 388 K  | train\n3 | dense    | Sequential         | 576 K  | train\n--------------------------------------------------------\n964 K     Trainable params\n0         Non-trainable params\n964 K     Total params\n3.858     Total estimated model params size (MB)\n29        Modules in train mode\n0         Modules in eval mode\n\n`Trainer.fit` stopped: `max_epochs=30` reached.\n</code></pre> <pre><code>fig, ax = subplots()\ncifar_results = pd.read_csv(cifar_logger.experiment.metrics_file_path)\ncifar_results.dropna(subset=['val_accuracy']).plot(x='epoch', y='val_accuracy', ax=ax)\ncifar_results.dropna(subset=['train_accuracy']).plot(x='epoch', y='train_accuracy', ax=ax)\n</code></pre> <pre><code>&lt;Axes: xlabel='epoch'&gt;\n</code></pre> <pre><code>for i, (X_, Y_) in enumerate(cifar_dm.train_dataloader()):\n    # print(X_)\n    # print(Y_)\n    if i &gt;= 1:\n        break\n\ncifar_model = CIFARModel ()\nsummary(cifar_model,input_data=X_ , col_names =['input_size','output_size', 'num_params'])\n</code></pre> <pre><code>===================================================================================================================\nLayer (type:depth-idx)                   Input Shape               Output Shape              Param #\n===================================================================================================================\nCIFARModel                               [32, 3, 32, 32]           [32, 100]                 --\n\u251c\u2500Sequential: 1-1                        [32, 3, 32, 32]           [32, 256, 2, 2]           --\n\u2502    \u2514\u2500BuildingBlock: 2-1                [32, 3, 32, 32]           [32, 32, 16, 16]          --\n\u2502    \u2502    \u2514\u2500Sequential: 3-1              [32, 3, 32, 32]           [32, 32, 16, 16]          896\n\u2502    \u2514\u2500BuildingBlock: 2-2                [32, 32, 16, 16]          [32, 64, 8, 8]            --\n\u2502    \u2502    \u2514\u2500Sequential: 3-2              [32, 32, 16, 16]          [32, 64, 8, 8]            18,496\n\u2502    \u2514\u2500BuildingBlock: 2-3                [32, 64, 8, 8]            [32, 128, 4, 4]           --\n\u2502    \u2502    \u2514\u2500Sequential: 3-3              [32, 64, 8, 8]            [32, 128, 4, 4]           73,856\n\u2502    \u2514\u2500BuildingBlock: 2-4                [32, 128, 4, 4]           [32, 256, 2, 2]           --\n\u2502    \u2502    \u2514\u2500Sequential: 3-4              [32, 128, 4, 4]           [32, 256, 2, 2]           295,168\n\u251c\u2500Sequential: 1-2                        [32, 256, 2, 2]           [32, 100]                 --\n\u2502    \u2514\u2500Flatten: 2-5                      [32, 256, 2, 2]           [32, 1024]                --\n\u2502    \u2514\u2500Dropout: 2-6                      [32, 1024]                [32, 1024]                --\n\u2502    \u2514\u2500Linear: 2-7                       [32, 1024]                [32, 512]                 524,800\n\u2502    \u2514\u2500ReLU: 2-8                         [32, 512]                 [32, 512]                 --\n\u2502    \u2514\u2500Linear: 2-9                       [32, 512]                 [32, 100]                 51,300\n===================================================================================================================\nTotal params: 964,516\nTrainable params: 964,516\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 501.70\n===================================================================================================================\nInput size (MB): 0.39\nForward/backward pass size (MB): 15.89\nParams size (MB): 3.86\nEstimated Total Size (MB): 20.14\n===================================================================================================================\n</code></pre> <pre><code>max_num_workers =10\n(imdb_train, imdb_test) = load_tensor(root='data/IMDB')\nimdb_dm = SimpleDataModule(imdb_train, imdb_test, validation =2000, num_workers=min(6, max_num_workers), batch_size =512)\n</code></pre> <pre><code>Retrieving \"IMDB_X_test.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\nRetrieving \"IMDB_X_train.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\nRetrieving \"IMDB_Y_test.npy\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\nRetrieving \"IMDB_Y_train.npy\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n</code></pre> <pre><code>class IMDBModel(pl.LightningModule):\n    def __init__(self, input_size):\n        super().__init__()\n        self.learning_rate = 0.001\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = BinaryAccuracy()\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_size, 16),\n            nn.ReLU(),\n            nn.Linear(16, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n    def step(self, batch, batch_idx, t='train'):\n        x, y = batch\n        preds = self(x)\n        loss = self.loss_fn(preds, y.unsqueeze(1).float())\n        acc = self.accuracy(preds, y.unsqueeze(1).float())\n        self.log(f'{t}_accuracy', acc, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(f'{t}_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'train')\n\n    def validation_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'val')\n\n    def test_step(self, batch, batch_idx):\n        return self.step(batch, batch_idx, 'test')\n\n    def configure_optimizers(self):\n        return torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n\nclass IMDBDataModule(pl.LightningDataModule):\n    def __init__(self, train_t, val_t, batch_size=32):\n        super().__init__()\n        self.train_t = train_t\n        self.val_t = val_t\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        return DataLoader(self.train_t, batch_size=self.batch_size, num_workers=8)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_t, batch_size=self.batch_size, num_workers=8)\n\n    def test_dataloader(self):\n        return self.val_dataloader(batch_size=self.batch_size)\n</code></pre> <pre><code>imdb_model = IMDBModel(imdb_train.tensors[0].size()[1])\nimdb_dm = IMDBDataModule(imdb_train, imdb_test)\nimdb_logger = CSVLogger(\"logs\", name=\"imdb\")\nimdb_trainer = Trainer(\n    deterministic=True, \n    logger=imdb_logger, \n    max_epochs=30, \n    callbacks=[EarlyStopping(monitor='val_accuracy', mode='min', patience=5)]\n)\n</code></pre> <pre><code>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n</code></pre> <pre><code>imdb_trainer.fit(imdb_model, datamodule=imdb_dm)\n</code></pre> <pre><code>  | Name     | Type              | Params | Mode \n-------------------------------------------------------\n0 | loss_fn  | BCEWithLogitsLoss | 0      | train\n1 | accuracy | BinaryAccuracy    | 0      | train\n2 | model    | Sequential        | 160 K  | train\n-------------------------------------------------------\n160 K     Trainable params\n0         Non-trainable params\n160 K     Total params\n0.641     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n</code></pre> <pre><code>fig, ax = subplots()\nimdb_results = pd.read_csv(imdb_logger.experiment.metrics_file_path)\nimdb_results.dropna(subset=['val_accuracy']).plot(x='epoch', y='val_accuracy', ax=ax)\nimdb_results.dropna(subset=['train_accuracy']).plot(x='epoch', y='train_accuracy', ax=ax)\nax.set_ylim([0, 1.1])\n\n</code></pre> <pre><code>(0.0, 1.1)\n</code></pre> <pre><code>(imdb_seq_train ,\nimdb_seq_test) = load_sequential(root='data/IMDB')\npadded_sample = np.asarray(imdb_seq_train.tensors [0][0])\nsample_review = padded_sample[padded_sample &gt; 0][:12]\nsample_review [:12]\n</code></pre> <pre><code>Retrieving \"IMDB_S_train.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\nRetrieving \"IMDB_S_test.tensor.gz\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n\narray([   1,   14,   22,   16,   43,  530,  973, 1622, 1385,   65,  458,\n       4468], dtype=int32)\n</code></pre> <pre><code>lookup = load_lookup(root='data/IMDB')\n' '.join(lookup[i] for i in sample_review)\n</code></pre> <pre><code>Retrieving \"IMDB_word_index.pkl\" from \"http://imdb.jtaylor.su.domains/jtaylor/data/\".\n\n\"&lt;START&gt; this film was just brilliant casting location scenery story direction everyone's\"\n</code></pre> <pre><code>imdb_seq_test.tensors[0][0]\n</code></pre> <pre><code>tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           1,  591,  202,   14,   31,    6,  717,   10,   10,    2,    2,    5,\n           4,  360,    7,    4,  177, 5760,  394,  354,    4,  123,    9, 1035,\n        1035, 1035,   10,   10,   13,   92,  124,   89,  488, 7944,  100,   28,\n        1668,   14,   31,   23,   27, 7479,   29,  220,  468,    8,  124,   14,\n         286,  170,    8,  157,   46,    5,   27,  239,   16,  179,    2,   38,\n          32,   25, 7944,  451,  202,   14,    6,  717], dtype=torch.int32)\n</code></pre> <pre><code>nn.Embedding?\n</code></pre> <pre><code>\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpadding_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnorm_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0m_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0m_freeze\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-&gt;\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\nThis module is often used to store word embeddings and retrieve them using indices.\nThe input to the module is a list of indices, and the output is the corresponding\nword embeddings.\n\nArgs:\n    num_embeddings (int): size of the dictionary of embeddings\n    embedding_dim (int): the size of each embedding vector\n    padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;\n                                 therefore, the embedding vector at :attr:`padding_idx` is not updated during training,\n                                 i.e. it remains as a fixed \"pad\". For a newly constructed Embedding,\n                                 the embedding vector at :attr:`padding_idx` will default to all zeros,\n                                 but can be updated to another value to be used as the padding vector.\n    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n                                is renormalized to have norm :attr:`max_norm`.\n    norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n    scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of\n                                            the words in the mini-batch. Default ``False``.\n    sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n                             See Notes for more details regarding sparse gradients.\n\nAttributes:\n    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n                     initialized from :math:`\\mathcal{N}(0, 1)`\n\nShape:\n    - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract\n    - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n\n.. note::\n    Keep in mind that only a limited number of optimizers support\n    sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n    :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n\n.. note::\n    When :attr:`max_norm` is not ``None``, :class:`Embedding`'s forward method will modify the\n    :attr:`weight` tensor in-place. Since tensors needed for gradient computations cannot be\n    modified in-place, performing a differentiable operation on ``Embedding.weight`` before\n    calling :class:`Embedding`'s forward method requires cloning ``Embedding.weight`` when\n    :attr:`max_norm` is not ``None``. For example::\n\n        n, d, m = 3, 5, 7\n        embedding = nn.Embedding(n, d, max_norm=1.0)\n        W = torch.randn((m, d), requires_grad=True)\n        idx = torch.tensor([1, 2])\n        a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n        b = embedding(idx) @ W.t()  # modifies weight in-place\n        out = (a.unsqueeze(0) + b.unsqueeze(1))\n        loss = out.sigmoid().prod()\n        loss.backward()\n\nExamples::\n\n    &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3\n    &gt;&gt;&gt; embedding = nn.Embedding(10, 3)\n    &gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n    &gt;&gt;&gt; input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n    &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n    &gt;&gt;&gt; embedding(input)\n    tensor([[[-0.0251, -1.6902,  0.7172],\n             [-0.6431,  0.0748,  0.6969],\n             [ 1.4970,  1.3448, -0.9685],\n             [-0.3677, -2.7265, -0.1685]],\n\n            [[ 1.4970,  1.3448, -0.9685],\n             [ 0.4362, -0.4004,  0.9400],\n             [-0.6431,  0.0748,  0.6969],\n             [ 0.9124, -2.3616,  1.1151]]])\n\n    &gt;&gt;&gt; # example with padding_idx\n    &gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)\n    &gt;&gt;&gt; input = torch.LongTensor([[0, 2, 0, 5]])\n    &gt;&gt;&gt; embedding(input)\n    tensor([[[ 0.0000,  0.0000,  0.0000],\n             [ 0.1535, -2.0309,  0.9315],\n             [ 0.0000,  0.0000,  0.0000],\n             [-0.1655,  0.9897,  0.0635]]])\n\n    &gt;&gt;&gt; # example of changing `pad` vector\n    &gt;&gt;&gt; padding_idx = 0\n    &gt;&gt;&gt; embedding = nn.Embedding(3, 3, padding_idx=padding_idx)\n    &gt;&gt;&gt; embedding.weight\n    Parameter containing:\n    tensor([[ 0.0000,  0.0000,  0.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n    &gt;&gt;&gt; with torch.no_grad():\n    ...     embedding.weight[padding_idx] = torch.ones(3)\n    &gt;&gt;&gt; embedding.weight\n    Parameter containing:\n    tensor([[ 1.0000,  1.0000,  1.0000],\n            [-0.7895, -0.7089, -0.0364],\n            [ 0.6778,  0.5803,  0.2678]], requires_grad=True)\n\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[0;31mFile:\u001b[0m           ~/Lab/islp/venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     Embedding, Embedding\n</code></pre> <pre><code>nn.LSTM?\n</code></pre> <pre><code>\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \n__init__(input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False,proj_size=0,device=None,dtype=None)\n\nApply a multi-layer long short-term memory (LSTM) RNN to an input sequence.\nFor each element in the input sequence, each layer computes the following\nfunction:\n\n.. math::\n    \\begin{array}{ll} \\\\\n        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n        h_t = o_t \\odot \\tanh(c_t) \\\\\n    \\end{array}\n\nwhere :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\nstate at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{t-1}`\nis the hidden state of the layer at time `t-1` or the initial hidden\nstate at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,\n:math:`o_t` are the input, forget, cell, and output gates, respectively.\n:math:`\\sigma` is the sigmoid function, and :math:`\\odot` is the Hadamard product.\n\nIn a multilayer LSTM, the input :math:`x^{(l)}_t` of the :math:`l` -th layer\n(:math:`l \\ge 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by\ndropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random\nvariable which is :math:`0` with probability :attr:`dropout`.\n\nIf ``proj_size &gt; 0`` is specified, LSTM with projections will be used. This changes\nthe LSTM cell in the following way. First, the dimension of :math:`h_t` will be changed from\n``hidden_size`` to ``proj_size`` (dimensions of :math:`W_{hi}` will be changed accordingly).\nSecond, the output hidden state of each layer will be multiplied by a learnable projection\nmatrix: :math:`h_t = W_{hr}h_t`. Note that as a consequence of this, the output\nof LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact\ndimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.\n\nArgs:\n    input_size: The number of expected features in the input `x`\n    hidden_size: The number of features in the hidden state `h`\n    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n        would mean stacking two LSTMs together to form a `stacked LSTM`,\n        with the second LSTM taking in outputs of the first LSTM and\n        computing the final results. Default: 1\n    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n        Default: ``True``\n    batch_first: If ``True``, then the input and output tensors are provided\n        as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n        Note that this does not apply to hidden or cell states. See the\n        Inputs/Outputs sections below for details.  Default: ``False``\n    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n        LSTM layer except the last layer, with dropout probability equal to\n        :attr:`dropout`. Default: 0\n    bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n    proj_size: If ``&gt; 0``, will use LSTM with projections of corresponding size. Default: 0\n\nInputs: input, (h_0, c_0)\n    * **input**: tensor of shape :math:`(L, H_{in})` for unbatched input,\n      :math:`(L, N, H_{in})` when ``batch_first=False`` or\n      :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n      the input sequence.  The input can also be a packed variable length sequence.\n      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n    * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n      initial hidden state for each element in the input sequence.\n      Defaults to zeros if (h_0, c_0) is not provided.\n    * **c_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{cell})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n      initial cell state for each element in the input sequence.\n      Defaults to zeros if (h_0, c_0) is not provided.\n\n    where:\n\n    .. math::\n        \\begin{aligned}\n            N ={} &amp; \\text{batch size} \\\\\n            L ={} &amp; \\text{sequence length} \\\\\n            D ={} &amp; 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n            H_{in} ={} &amp; \\text{input\\_size} \\\\\n            H_{cell} ={} &amp; \\text{hidden\\_size} \\\\\n            H_{out} ={} &amp; \\text{proj\\_size if } \\text{proj\\_size}&gt;0 \\text{ otherwise hidden\\_size} \\\\\n        \\end{aligned}\n\nOutputs: output, (h_n, c_n)\n    * **output**: tensor of shape :math:`(L, D * H_{out})` for unbatched input,\n      :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n      :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n      `(h_t)` from the last layer of the LSTM, for each `t`. If a\n      :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n      will also be a packed sequence. When ``bidirectional=True``, `output` will contain\n      a concatenation of the forward and reverse hidden states at each time step in the sequence.\n    * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n      final hidden state for each element in the sequence. When ``bidirectional=True``,\n      `h_n` will contain a concatenation of the final forward and reverse hidden states, respectively.\n    * **c_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{cell})` for unbatched input or\n      :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n      final cell state for each element in the sequence. When ``bidirectional=True``,\n      `c_n` will contain a concatenation of the final forward and reverse cell states, respectively.\n\nAttributes:\n    weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n        `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size, input_size)` for `k = 0`.\n        Otherwise, the shape is `(4*hidden_size, num_directions * hidden_size)`. If\n        ``proj_size &gt; 0`` was specified, the shape will be\n        `(4*hidden_size, num_directions * proj_size)` for `k &gt; 0`\n    weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n        `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size, hidden_size)`. If ``proj_size &gt; 0``\n        was specified, the shape will be `(4*hidden_size, proj_size)`.\n    bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n        `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n    bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n        `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n    weight_hr_l[k] : the learnable projection weights of the :math:`\\text{k}^{th}` layer\n        of shape `(proj_size, hidden_size)`. Only present when ``proj_size &gt; 0`` was\n        specified.\n    weight_ih_l[k]_reverse: Analogous to `weight_ih_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    weight_hh_l[k]_reverse:  Analogous to `weight_hh_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    bias_ih_l[k]_reverse:  Analogous to `bias_ih_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    bias_hh_l[k]_reverse:  Analogous to `bias_hh_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True``.\n    weight_hr_l[k]_reverse:  Analogous to `weight_hr_l[k]` for the reverse direction.\n        Only present when ``bidirectional=True`` and ``proj_size &gt; 0`` was specified.\n\n.. note::\n    All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n    where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n\n.. note::\n    For bidirectional LSTMs, forward and backward are directions 0 and 1 respectively.\n    Example of splitting the output layers when ``batch_first=False``:\n    ``output.view(seq_len, batch, num_directions, hidden_size)``.\n\n.. note::\n    For bidirectional LSTMs, `h_n` is not equivalent to the last element of `output`; the\n    former contains the final forward and reverse hidden states, while the latter contains the\n    final forward hidden state and the initial reverse hidden state.\n\n.. note::\n    ``batch_first`` argument is ignored for unbatched inputs.\n\n.. note::\n    ``proj_size`` should be smaller than ``hidden_size``.\n\n.. include:: ../cudnn_rnn_determinism.rst\n\n.. include:: ../cudnn_persistent_rnn.rst\n\nExamples::\n\n    &gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)\n    &gt;&gt;&gt; input = torch.randn(5, 3, 10)\n    &gt;&gt;&gt; h0 = torch.randn(2, 3, 20)\n    &gt;&gt;&gt; c0 = torch.randn(2, 3, 20)\n    &gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))\n\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[0;31mFile:\u001b[0m           ~/Lab/islp/venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m\n</code></pre> <pre><code>nn.*\n</code></pre>"},{"location":"z_ISLP%20demo4/","title":"z ISLP demo4","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom matplotlib.pyplot import subplots\nimport statsmodels.api as sm\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS, summarize)\n</code></pre> <pre><code>from ISLP import confusion_table\nfrom ISLP.models import contrast\nfrom sklearn.discriminant_analysis import \\\n(LinearDiscriminantAnalysis as LDA ,\nQuadraticDiscriminantAnalysis as QDA)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n</code></pre> <pre><code>Smarket = load_data('Smarket')\nSmarket\n</code></pre> Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction 0 2001 0.381 -0.192 -2.624 -1.055 5.010 1.19130 0.959 Up 1 2001 0.959 0.381 -0.192 -2.624 -1.055 1.29650 1.032 Up 2 2001 1.032 0.959 0.381 -0.192 -2.624 1.41120 -0.623 Down 3 2001 -0.623 1.032 0.959 0.381 -0.192 1.27600 0.614 Up 4 2001 0.614 -0.623 1.032 0.959 0.381 1.20570 0.213 Up ... ... ... ... ... ... ... ... ... ... 1245 2005 0.422 0.252 -0.024 -0.584 -0.285 1.88850 0.043 Up 1246 2005 0.043 0.422 0.252 -0.024 -0.584 1.28581 -0.955 Down 1247 2005 -0.955 0.043 0.422 0.252 -0.024 1.54047 0.130 Up 1248 2005 0.130 -0.955 0.043 0.422 0.252 1.42236 -0.298 Down 1249 2005 -0.298 0.130 -0.955 0.043 0.422 1.38254 -0.489 Down <p>1250 rows \u00d7 9 columns</p> <pre><code>Smarket.drop(columns=[\"Direction\"]).corr()\n</code></pre> Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Year 1.000000 0.029700 0.030596 0.033195 0.035689 0.029788 0.539006 0.030095 Lag1 0.029700 1.000000 -0.026294 -0.010803 -0.002986 -0.005675 0.040910 -0.026155 Lag2 0.030596 -0.026294 1.000000 -0.025897 -0.010854 -0.003558 -0.043383 -0.010250 Lag3 0.033195 -0.010803 -0.025897 1.000000 -0.024051 -0.018808 -0.041824 -0.002448 Lag4 0.035689 -0.002986 -0.010854 -0.024051 1.000000 -0.027084 -0.048414 -0.006900 Lag5 0.029788 -0.005675 -0.003558 -0.018808 -0.027084 1.000000 -0.022002 -0.034860 Volume 0.539006 0.040910 -0.043383 -0.041824 -0.048414 -0.022002 1.000000 0.014592 Today 0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860 0.014592 1.000000 <pre><code>Smarket.plot(y='Volume')\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code>allvars = Smarket.drop(columns=[\"Today\", \"Year\", \"Direction\"])\ndesign = MS(allvars)\nX = design.fit_transform(Smarket)\ny = Smarket.Direction == \"Up\"\nglm = sm.GLM(y, X, family=sm.families.Binomial())\nresults =  glm.fit()\nsummarize(results)\n</code></pre> coef std err z P&gt;|z| intercept -0.1260 0.241 -0.523 0.601 Lag1 -0.0731 0.050 -1.457 0.145 Lag2 -0.0423 0.050 -0.845 0.398 Lag3 0.0111 0.050 0.222 0.824 Lag4 0.0094 0.050 0.187 0.851 Lag5 0.0103 0.050 0.208 0.835 Volume 0.1354 0.158 0.855 0.392 <pre><code>probs = results.predict()\nprobs\n</code></pre> <pre><code>array([0.50708413, 0.48146788, 0.48113883, ..., 0.5392683 , 0.52611829,\n       0.51791656])\n</code></pre> <pre><code>labels = np.array([\"Up\" if n &gt; 0.5 else \"Down\" for n in probs])\nlabels\n</code></pre> <pre><code>array(['Up', 'Down', 'Down', ..., 'Up', 'Up', 'Up'], dtype='&lt;U4')\n</code></pre> <pre><code>confusion_table(labels, Smarket.Direction)\n</code></pre> Truth Down Up Predicted Down 145 141 Up 457 507 <pre><code>train = (Smarket[\"Year\"] &lt; 2005)\nSmarket_train = Smarket[train]\nSmarket_test = Smarket[~train]\n</code></pre> <pre><code>X_train, X_test = X[train], X[~train]\ny_train, y_test = y[train], y[~train]\ntrain_glm = sm.GLM(y_train, X_train, family=sm.families.Binomial())\nresults = train_glm.fit()\nprobs = results.predict(exog=X_test)\n</code></pre> <pre><code>probs\n</code></pre> <pre><code>998     0.528220\n999     0.515669\n1000    0.522652\n1001    0.513854\n1002    0.498334\n          ...   \n1245    0.483637\n1246    0.506048\n1247    0.516658\n1248    0.516124\n1249    0.508072\nLength: 252, dtype: float64\n</code></pre> <pre><code>labels = np.array([True if n &gt; 0.5 else False for n in probs])\nconfusion_table(labels, y_test)\n</code></pre> Truth False True Predicted False 77 97 True 34 44 <pre><code>X_train = (Smarket[train])[[\"Lag1\", \"Lag2\"]]\ny_train = (Smarket[train]).Direction == \"Up\"\nX_test = (Smarket[~train])[[\"Lag1\", \"Lag2\"]]\ny_test = (Smarket[~train]).Direction == \"Up\"\n</code></pre> <pre><code>lda = LDA(store_covariance=True)\nlda.fit(X_train, y_train)\n</code></pre> <pre>LinearDiscriminantAnalysis(store_covariance=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearDiscriminantAnalysis?Documentation for LinearDiscriminantAnalysisiFitted<pre>LinearDiscriminantAnalysis(store_covariance=True)</pre> <pre><code>lda.means_, lda.priors_\n</code></pre> <pre><code>(array([[ 0.04279022,  0.03389409],\n        [-0.03954635, -0.03132544]]),\n array([0.49198397, 0.50801603]))\n</code></pre> <pre><code>lda_pred = lda.predict(X_test)\n</code></pre> <pre><code>confusion_table(lda_pred, y_test)\n</code></pre> Truth False True Predicted False 35 35 True 76 106 <pre><code>lda_prob = lda.predict_proba(X_test)\n</code></pre> <pre><code>\n</code></pre> <pre><code>0.5202349505356155\n</code></pre> <pre><code>qda = QDA(store_covariance=True)\n</code></pre> <pre><code>qda.fit(X_train, y_train)\n</code></pre> <pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.QuadraticDiscriminantAnalysis?Documentation for QuadraticDiscriminantAnalysisiFitted<pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre> <pre><code>qda.means_, qda.priors_\n</code></pre> <pre><code>(array([[ 0.04279022,  0.03389409],\n        [-0.03954635, -0.03132544]]),\n array([0.49198397, 0.50801603]))\n</code></pre> <pre><code>qda.covariance_\n</code></pre> <pre><code>[array([[ 1.50662277, -0.03924806],\n        [-0.03924806,  1.53559498]]),\n array([[ 1.51700576, -0.02787349],\n        [-0.02787349,  1.49026815]])]\n</code></pre> <pre><code>(qda.predict(X_test) == y_test).mean()\n</code></pre> <pre><code>0.5992063492063492\n</code></pre> <pre><code>NB = GaussianNB()\nNB.fit(X_train, y_train)\n</code></pre> <pre>GaussianNB()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNB?Documentation for GaussianNBiFitted<pre>GaussianNB()</pre> <pre><code>NB.theta_, NB.var_\n</code></pre> <pre><code>(array([[ 0.04279022,  0.03389409],\n        [-0.03954635, -0.03132544]]),\n array([[1.50355429, 1.53246749],\n        [1.51401364, 1.48732877]]))\n</code></pre> <pre><code>(NB.predict(X_test) == y_test).mean()\n</code></pre> <pre><code>0.5952380952380952\n</code></pre> <pre><code>knn1 = KNeighborsClassifier(n_neighbors=1)\nknn1.fit(X_train, y_train)\n(knn1.predict(X_test) == y_test).mean()\n</code></pre> <pre>KNeighborsClassifier(n_neighbors=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted<pre>KNeighborsClassifier(n_neighbors=1)</pre> <pre><code>knn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X_train, y_train)\n(knn3.predict(X_test) == y_test).mean()\n</code></pre> <pre><code>0.5317460317460317\n</code></pre> <pre><code>Caravan = load_data('Caravan')\nPurchase = Caravan.Purchase\nPurchase.value_counts()\n</code></pre> <pre><code>Purchase\nNo     5474\nYes     348\nName: count, dtype: int64\n</code></pre> <pre><code>caravan_df = Caravan.drop(columns=[\"Purchase\"])\nscaler = StandardScaler(copy=True)\n</code></pre> <pre><code>scaler.fit(caravan_df)\nX_std = scaler.transform(caravan_df)\n</code></pre> <pre><code>pd.DataFrame(X_std, columns=caravan_df.columns).std()\n</code></pre> <pre><code>MOSTYPE     1.000086\nMAANTHUI    1.000086\nMGEMOMV     1.000086\nMGEMLEEF    1.000086\nMOSHOOFD    1.000086\n              ...   \nAZEILPL     1.000086\nAPLEZIER    1.000086\nAFIETS      1.000086\nAINBOED     1.000086\nABYSTAND    1.000086\nLength: 85, dtype: float64\n</code></pre> <pre><code>(X_train, X_test, y_train, y_test) = train_test_split(caravan_df, Purchase, test_size=1000, random_state=0)\n</code></pre> <pre><code>knn1 = KNeighborsClassifier(n_neighbors=1)\nknn1.fit(X_train, y_train)\n(knn1.predict(X_test) == y_test).mean()\n</code></pre> <pre><code>0.879\n</code></pre> <pre><code>knn3 = KNeighborsClassifier(n_neighbors=3)\nknn3.fit(X_train, y_train)\n(knn3.predict(X_test) == y_test).mean()\n</code></pre> <pre><code>0.923\n</code></pre> <pre><code>confusion_table(knn1.predict(X_test), y_test)\n</code></pre> Truth No Yes Predicted No 874 62 Yes 59 5 <pre><code>logit = LogisticRegression(C=1e10, solver='liblinear')\nlogit.fit(X_train, y_train)\nlogit_pred = logit.predict(X_test)\nconfusion_table(logit_pred, y_test)\n</code></pre> Truth No Yes Predicted No 931 67 Yes 2 0 <pre><code>logit = LogisticRegression(C=1e10 , solver='liblinear')\nlogit.fit(X_train , y_train)\nlogit_pred = logit.predict_proba(X_test)\nlogit_labels = np.where(logit_pred [:,1] &gt; 0.15, 'Yes', 'No')\nconfusion_table(logit_labels , y_test)\n</code></pre> Truth No Yes Predicted No 876 49 Yes 57 18 <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"z_ISLP%20demo6/","title":"z ISLP demo6","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn.linear_model as skl\nimport sklearn.model_selection as skm\n\nfrom ISLP import load_data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom matplotlib.pyplot import subplots\n</code></pre> <pre><code>hits = load_data('Hitters')\nhits.info()\nhits.head()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 322 entries, 0 to 321\nData columns (total 20 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   AtBat      322 non-null    int64   \n 1   Hits       322 non-null    int64   \n 2   HmRun      322 non-null    int64   \n 3   Runs       322 non-null    int64   \n 4   RBI        322 non-null    int64   \n 5   Walks      322 non-null    int64   \n 6   Years      322 non-null    int64   \n 7   CAtBat     322 non-null    int64   \n 8   CHits      322 non-null    int64   \n 9   CHmRun     322 non-null    int64   \n 10  CRuns      322 non-null    int64   \n 11  CRBI       322 non-null    int64   \n 12  CWalks     322 non-null    int64   \n 13  League     322 non-null    category\n 14  Division   322 non-null    category\n 15  PutOuts    322 non-null    int64   \n 16  Assists    322 non-null    int64   \n 17  Errors     322 non-null    int64   \n 18  Salary     263 non-null    float64 \n 19  NewLeague  322 non-null    category\ndtypes: category(3), float64(1), int64(16)\nmemory usage: 44.2 KB\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague 0 293 66 1 30 29 14 1 293 66 1 30 29 14 A E 446 33 20 NaN A 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N 4 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N <pre><code>hits.describe()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks PutOuts Assists Errors Salary count 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.00000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 263.000000 mean 380.928571 101.024845 10.770186 50.909938 48.027950 38.742236 7.444099 2648.68323 717.571429 69.490683 358.795031 330.118012 260.239130 288.937888 106.913043 8.040373 535.925882 std 153.404981 46.454741 8.709037 26.024095 26.166895 21.639327 4.926087 2324.20587 654.472627 86.266061 334.105886 333.219617 267.058085 280.704614 136.854876 6.368359 451.118681 min 16.000000 1.000000 0.000000 0.000000 0.000000 0.000000 1.000000 19.00000 4.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 67.500000 25% 255.250000 64.000000 4.000000 30.250000 28.000000 22.000000 4.000000 816.75000 209.000000 14.000000 100.250000 88.750000 67.250000 109.250000 7.000000 3.000000 190.000000 50% 379.500000 96.000000 8.000000 48.000000 44.000000 35.000000 6.000000 1928.00000 508.000000 37.500000 247.000000 220.500000 170.500000 212.000000 39.500000 6.000000 425.000000 75% 512.000000 137.000000 16.000000 69.000000 64.750000 53.000000 11.000000 3924.25000 1059.250000 90.000000 526.250000 426.250000 339.250000 325.000000 166.000000 11.000000 750.000000 max 687.000000 238.000000 40.000000 130.000000 121.000000 105.000000 24.000000 14053.00000 4256.000000 548.000000 2165.000000 1659.000000 1566.000000 1378.000000 492.000000 32.000000 2460.000000 <pre><code>hits = hits.dropna()\nsns.pairplot(hits, hue=\"Salary\")\n</code></pre> <pre><code>&lt;seaborn.axisgrid.PairGrid at 0x7cfb80e084a0&gt;\n</code></pre> <pre><code>kfold = skm.KFold(n_splits=5, shuffle=True, random_state=0)\nX = hits.drop(columns=[\"Salary\", \"League\", \"Division\", \"NewLeague\"])\ny = hits[\"Salary\"]\n</code></pre> <pre><code>lr = LinearRegression()\nmodels = []\n\nbest_preds = []\nbest_mse = []\nfor _ in range(len(X.columns)):\n    mse_col = []\n    mse_col_name = []\n    for col in X.columns:\n        if col in best_preds:\n            continue\n\n        tmp_mse = []\n        for (train_id, test_id) in kfold.split(y):\n            X_train, y_train = X[[col] + best_preds].iloc[train_id], y.iloc[train_id]\n            X_test, y_test = X[[col] + best_preds].iloc[test_id], y.iloc[test_id]\n\n            lr.fit(X_train, y_train)\n            lr_pred = lr.predict(X_test)\n\n            mse = ((lr_pred - y_test)**2).mean()\n            tmp_mse.append(mse)\n\n        mse_col.append(np.mean(tmp_mse))\n        mse_col_name.append(col)\n\n    best_col_id = np.argmin(mse_col)\n    best_preds.append(mse_col_name[best_col_id]) \n    best_mse.append(mse_col[best_col_id])\n</code></pre> <pre><code>best_preds\n</code></pre> <pre><code>['CRBI',\n 'Hits',\n 'PutOuts',\n 'AtBat',\n 'Walks',\n 'CWalks',\n 'CRuns',\n 'CHmRun',\n 'RBI',\n 'Errors',\n 'CHits',\n 'Assists',\n 'HmRun',\n 'Runs',\n 'Years',\n 'CAtBat']\n</code></pre> <pre><code>best_n = np.argmin(best_mse)\n</code></pre> <pre><code>best_n_preds = best_preds[:best_n+1]\n</code></pre> <pre><code>\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X[best_n_preds], y)\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\n((lr_pred - y_test)**2).mean()\n</code></pre> <pre><code>107105.26189115838\n</code></pre> <pre><code>fig, ax = subplots()\nax.scatter(y_test, lr_pred - y_test)\nax.set_xlabel(\"Salary\")\nax.set_ylabel(\"Salary error\")\n</code></pre> <pre><code>Text(0, 0.5, 'Salary error')\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"z_ISLP%20demo6_2/","title":"z ISLP demo6 2","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn.linear_model as skl\nimport sklearn.model_selection as skm\n\nfrom ISLP import load_data\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import mean_squared_error as MSE, r2_score as R2\nfrom matplotlib.pyplot import subplots\n</code></pre> <pre><code>hits = load_data('Hitters')\nhits.info()\nhits.head()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 322 entries, 0 to 321\nData columns (total 20 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   AtBat      322 non-null    int64   \n 1   Hits       322 non-null    int64   \n 2   HmRun      322 non-null    int64   \n 3   Runs       322 non-null    int64   \n 4   RBI        322 non-null    int64   \n 5   Walks      322 non-null    int64   \n 6   Years      322 non-null    int64   \n 7   CAtBat     322 non-null    int64   \n 8   CHits      322 non-null    int64   \n 9   CHmRun     322 non-null    int64   \n 10  CRuns      322 non-null    int64   \n 11  CRBI       322 non-null    int64   \n 12  CWalks     322 non-null    int64   \n 13  League     322 non-null    category\n 14  Division   322 non-null    category\n 15  PutOuts    322 non-null    int64   \n 16  Assists    322 non-null    int64   \n 17  Errors     322 non-null    int64   \n 18  Salary     263 non-null    float64 \n 19  NewLeague  322 non-null    category\ndtypes: category(3), float64(1), int64(16)\nmemory usage: 44.2 KB\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague 0 293 66 1 30 29 14 1 293 66 1 30 29 14 A E 446 33 20 NaN A 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N 4 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N <pre><code>hits.describe()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks PutOuts Assists Errors Salary count 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.00000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 263.000000 mean 380.928571 101.024845 10.770186 50.909938 48.027950 38.742236 7.444099 2648.68323 717.571429 69.490683 358.795031 330.118012 260.239130 288.937888 106.913043 8.040373 535.925882 std 153.404981 46.454741 8.709037 26.024095 26.166895 21.639327 4.926087 2324.20587 654.472627 86.266061 334.105886 333.219617 267.058085 280.704614 136.854876 6.368359 451.118681 min 16.000000 1.000000 0.000000 0.000000 0.000000 0.000000 1.000000 19.00000 4.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 67.500000 25% 255.250000 64.000000 4.000000 30.250000 28.000000 22.000000 4.000000 816.75000 209.000000 14.000000 100.250000 88.750000 67.250000 109.250000 7.000000 3.000000 190.000000 50% 379.500000 96.000000 8.000000 48.000000 44.000000 35.000000 6.000000 1928.00000 508.000000 37.500000 247.000000 220.500000 170.500000 212.000000 39.500000 6.000000 425.000000 75% 512.000000 137.000000 16.000000 69.000000 64.750000 53.000000 11.000000 3924.25000 1059.250000 90.000000 526.250000 426.250000 339.250000 325.000000 166.000000 11.000000 750.000000 max 687.000000 238.000000 40.000000 130.000000 121.000000 105.000000 24.000000 14053.00000 4256.000000 548.000000 2165.000000 1659.000000 1566.000000 1378.000000 492.000000 32.000000 2460.000000 <pre><code>hits = hits.dropna()\nsns.pairplot(hits, hue=\"Salary\")\n</code></pre> <pre><code>&lt;seaborn.axisgrid.PairGrid at 0x72846cfd77d0&gt;\n</code></pre> <pre><code>for col in [\"League\", \"Division\", \"NewLeague\"]:\n    hits[col] = [1 if val == hits[col].iloc[0] else 0 for val in hits[col]]\n\nX = hits.drop(columns=[\"Salary\"])\ny = hits[\"Salary\"]\nlambdas = 10**np.linspace(8, -2, 100)\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.1, random_state=0)\nkfold = skm.KFold(n_splits=5, shuffle=True, random_state=0)\n</code></pre> <pre><code>model = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('ridge', skl.RidgeCV(alphas=lambdas, store_cv_results=True))\n    ]\n)\n</code></pre> <pre><code>model.fit(X_train, y_train)\nmodel_pred = model.predict(X_test)\nMSE(model_pred, y_test), R2(model_pred, y_test)\n</code></pre> <pre><code>(99295.86827979995, 0.4448133917882484)\n</code></pre> <pre><code>ridgecv = model.named_steps['ridge']\nerr_path = ridgecv.cv_results_.mean(0)\n\nfig, ax = subplots()\nax.plot(-np.log10(lambdas), err_path)\nax.axvline(-np.log10(ridgecv.alpha_), c='k', ls='--')\nax.set_ylim ([50000 ,250000])\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross -validated MSE', fontsize=20);\n</code></pre> <pre><code>&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\l'\n/tmp/ipykernel_31051/3379189933.py:8: SyntaxWarning: invalid escape sequence '\\l'\n  ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n</code></pre> <pre><code>ridgecv.coef_, ridgecv.alpha_\n</code></pre> <pre><code>(array([-2.43878762e+02,  2.60488628e+02, -1.03873887e-01, -1.94743085e+01,\n        -2.47831048e+00,  1.31955900e+02, -4.79502021e+01, -9.65621490e+01,\n         1.12083526e+02,  9.42043348e+01,  2.26196846e+02,  6.47739865e+01,\n        -1.56211768e+02,  2.56043840e+01, -6.22373727e+01,  7.81558996e+01,\n         4.86000393e+01, -2.90786464e+01, -7.98183554e+00]),\n 2.6560877829466896)\n</code></pre> <pre><code>model = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('lasso', skl.LassoCV(alphas=lambdas))\n    ]\n)\n</code></pre> <pre><code>model.fit(X_train, y_train)\nmodel_pred = model.predict(X_test)\nMSE(model_pred, y_test), R2(model_pred, y_test)\n</code></pre> <pre><code>(100781.91070990668, 0.43806280005181397)\n</code></pre> <pre><code>lassocv = model.named_steps['lasso']\nerr_path = lassocv.mse_path_.mean(1)\n\nfig, ax = subplots()\nax.plot(-np.log10(lambdas), err_path)\nax.axvline(-np.log10(lassocv.alpha_), c='k', ls='--')\nax.set_ylim ([50000 ,250000])\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross -validated MSE', fontsize=20);\n</code></pre> <pre><code>&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:8: SyntaxWarning: invalid escape sequence '\\l'\n/tmp/ipykernel_31051/1786576076.py:8: SyntaxWarning: invalid escape sequence '\\l'\n  ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n</code></pre> <pre><code>lassocv.coef_, lassocv.alpha_\n</code></pre> <pre><code>(array([-285.68139176,  298.36986021,   -0.        ,  -16.64102469,\n           0.        ,  132.59353528,  -39.83682438,   -0.        ,\n           0.        ,  100.62310668,  270.53870113,   33.4568784 ,\n        -168.6634666 ,   17.83798661,  -59.0206921 ,   78.97330083,\n          41.10553871,  -19.38100744,    0.        ]),\n 2.104904144512022)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"z_ISLP%20demo6_3/","title":"z ISLP demo6 3","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn.linear_model as skl\nimport sklearn.model_selection as skm\n\nfrom ISLP import load_data\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import mean_squared_error as MSE, r2_score as R2\nfrom matplotlib.pyplot import subplots\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\n</code></pre> <pre><code>hits = load_data('Hitters')\nhits.info()\nhits.head()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 322 entries, 0 to 321\nData columns (total 20 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   AtBat      322 non-null    int64   \n 1   Hits       322 non-null    int64   \n 2   HmRun      322 non-null    int64   \n 3   Runs       322 non-null    int64   \n 4   RBI        322 non-null    int64   \n 5   Walks      322 non-null    int64   \n 6   Years      322 non-null    int64   \n 7   CAtBat     322 non-null    int64   \n 8   CHits      322 non-null    int64   \n 9   CHmRun     322 non-null    int64   \n 10  CRuns      322 non-null    int64   \n 11  CRBI       322 non-null    int64   \n 12  CWalks     322 non-null    int64   \n 13  League     322 non-null    category\n 14  Division   322 non-null    category\n 15  PutOuts    322 non-null    int64   \n 16  Assists    322 non-null    int64   \n 17  Errors     322 non-null    int64   \n 18  Salary     263 non-null    float64 \n 19  NewLeague  322 non-null    category\ndtypes: category(3), float64(1), int64(16)\nmemory usage: 44.2 KB\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks League Division PutOuts Assists Errors Salary NewLeague 0 293 66 1 30 29 14 1 293 66 1 30 29 14 A E 446 33 20 NaN A 1 315 81 7 24 38 39 14 3449 835 69 321 414 375 N W 632 43 10 475.0 N 2 479 130 18 66 72 76 3 1624 457 63 224 266 263 A W 880 82 14 480.0 A 3 496 141 20 65 78 37 11 5628 1575 225 828 838 354 N E 200 11 3 500.0 N 4 321 87 10 39 42 30 2 396 101 12 48 46 33 N E 805 40 4 91.5 N <pre><code>hits.describe()\n</code></pre> AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks PutOuts Assists Errors Salary count 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.00000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 322.000000 263.000000 mean 380.928571 101.024845 10.770186 50.909938 48.027950 38.742236 7.444099 2648.68323 717.571429 69.490683 358.795031 330.118012 260.239130 288.937888 106.913043 8.040373 535.925882 std 153.404981 46.454741 8.709037 26.024095 26.166895 21.639327 4.926087 2324.20587 654.472627 86.266061 334.105886 333.219617 267.058085 280.704614 136.854876 6.368359 451.118681 min 16.000000 1.000000 0.000000 0.000000 0.000000 0.000000 1.000000 19.00000 4.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 0.000000 67.500000 25% 255.250000 64.000000 4.000000 30.250000 28.000000 22.000000 4.000000 816.75000 209.000000 14.000000 100.250000 88.750000 67.250000 109.250000 7.000000 3.000000 190.000000 50% 379.500000 96.000000 8.000000 48.000000 44.000000 35.000000 6.000000 1928.00000 508.000000 37.500000 247.000000 220.500000 170.500000 212.000000 39.500000 6.000000 425.000000 75% 512.000000 137.000000 16.000000 69.000000 64.750000 53.000000 11.000000 3924.25000 1059.250000 90.000000 526.250000 426.250000 339.250000 325.000000 166.000000 11.000000 750.000000 max 687.000000 238.000000 40.000000 130.000000 121.000000 105.000000 24.000000 14053.00000 4256.000000 548.000000 2165.000000 1659.000000 1566.000000 1378.000000 492.000000 32.000000 2460.000000 <pre><code>hits = hits.dropna()\n# sns.pairplot(hits, hue=\"Salary\")\n</code></pre> <pre><code>for col in [\"League\", \"Division\", \"NewLeague\"]:\n    hits[col] = [1 if val == hits[col].iloc[0] else 0 for val in hits[col]]\n\nX = hits.drop(columns=[\"Salary\"])\ny = hits[\"Salary\"]\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.1, random_state=0, shuffle=True)\nkfold = skm.KFold(n_splits=5, shuffle=True, random_state=0)\n</code></pre> <pre><code>model = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('pca', PCA(n_components=2)),\n        ('ridge', skl.LinearRegression())\n    ]\n)\n</code></pre> <pre><code>model.fit(X_train, y_train)\nmodel_pred = model.predict(X_test)\nMSE(model_pred, y_test), R2(model_pred, y_test)\n</code></pre> <pre><code>(115168.19165112973, 0.08221912624753136)\n</code></pre> <pre><code>pca = model.named_steps['pca']\npca.explained_variance_ratio_, pca.components_\n</code></pre> <pre><code>(array([0.37711794, 0.22135976]),\n array([[ 0.20115935,  0.20029646,  0.19933906,  0.19554293,  0.23273539,\n          0.19688827,  0.28400759,  0.33247928,  0.33328639,  0.32069048,\n          0.33919628,  0.34243474,  0.3174305 , -0.03407424, -0.01843857,\n          0.08955967,  0.01826129,  0.01172887, -0.01779337],\n        [ 0.37847546,  0.3715308 ,  0.2414967 ,  0.37768547,  0.31485639,\n          0.24561844, -0.26393601, -0.19810171, -0.18845451, -0.11530701,\n         -0.17523351, -0.16877619, -0.19326804, -0.12451746, -0.01483749,\n          0.15513448,  0.15113156,  0.17792085, -0.09869745]]))\n</code></pre> <pre><code>param = {\"pca__n_components\": range(1, 20)}\ngrid = skm.GridSearchCV(model, param, cv=kfold, scoring='neg_mean_squared_error')\ngrid.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('pca', PCA(n_components=2)),\n                                       ('ridge', LinearRegression())]),\n             param_grid={'pca__n_components': range(1, 20)},\n             scoring='neg_mean_squared_error')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('pca', PCA(n_components=2)),\n                                       ('ridge', LinearRegression())]),\n             param_grid={'pca__n_components': range(1, 20)},\n             scoring='neg_mean_squared_error')</pre> best_estimator_: Pipeline<pre>Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=17)),\n                ('ridge', LinearRegression())])</pre> StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> PCA?Documentation for PCA<pre>PCA(n_components=17)</pre> LinearRegression?Documentation for LinearRegression<pre>LinearRegression()</pre> <pre><code>err = grid.cv_results_['mean_test_score']\nn_comp = param['pca__n_components']\n\nfig, ax = subplots()\nax.errorbar(n_comp, -err, yerr=grid.cv_results_['std_test_score'] / np.sqrt(19))\nax.set_xlabel('n components')\nax.set_ylabel('mean val score')\nax.set_xticks(n_comp[::2]);\n</code></pre> <pre><code>pls = PLSRegression(scale=True)\nparam = {'n_components': range(1, 20)} \ngrid = skm.GridSearchCV(pls, param, cv=kfold, scoring='neg_mean_squared_error')\n</code></pre> <pre><code>grid.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=PLSRegression(),\n             param_grid={'n_components': range(1, 20)},\n             scoring='neg_mean_squared_error')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n             estimator=PLSRegression(),\n             param_grid={'n_components': range(1, 20)},\n             scoring='neg_mean_squared_error')</pre> best_estimator_: PLSRegression<pre>PLSRegression(n_components=11)</pre> PLSRegression?Documentation for PLSRegression<pre>PLSRegression(n_components=11)</pre> <pre><code>err = grid.cv_results_['mean_test_score']\n\nfig, ax = subplots()\nax.errorbar(n_comp, -err, yerr=grid.cv_results_['std_test_score'] / np.sqrt(19))\nax.set_xlabel('n components')\nax.set_ylabel('mean val score')\nax.set_xticks(n_comp[::2]);\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"z_ISLP%20demo8/","title":"z ISLP demo8","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection as skm\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_text\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\n\nfrom statsmodels.datasets import get_rdataset\nfrom ISLP import load_data , confusion_table\nfrom ISLP.models import ModelSpec as MS\nfrom ISLP.bart import BART\n</code></pre> <pre><code>Carseats = load_data('Carseats')\nCarseats\n</code></pre> Sales CompPrice Income Advertising Population Price ShelveLoc Age Education Urban US 0 9.50 138 73 11 276 120 Bad 42 17 Yes Yes 1 11.22 111 48 16 260 83 Good 65 10 Yes Yes 2 10.06 113 35 10 269 80 Medium 59 12 Yes Yes 3 7.40 117 100 4 466 97 Medium 55 14 Yes Yes 4 4.15 141 64 3 340 128 Bad 38 13 Yes No ... ... ... ... ... ... ... ... ... ... ... ... 395 12.57 138 108 17 203 128 Good 33 14 Yes Yes 396 6.14 139 23 3 37 120 Medium 55 11 No Yes 397 7.41 162 26 12 368 159 Medium 40 18 Yes Yes 398 5.94 100 79 7 284 95 Bad 50 12 Yes Yes 399 9.71 134 37 0 27 120 Good 49 16 Yes Yes <p>400 rows \u00d7 11 columns</p> <pre><code>X = Carseats.drop(columns=[\"Sales\"])\nX = pd.get_dummies(X, columns=[\"ShelveLoc\", \"Urban\", \"US\"], drop_first=True)\nhigh_sales = Carseats.Sales &gt; 8\n</code></pre> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, high_sales, random_state=0)\n</code></pre> <pre><code>dtc = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=0)\ndtc.fit(X_train, y_train)\ndtc_pred = dtc.predict(X_test)\naccuracy_score(y_test, dtc_pred)\n</code></pre> <pre><code>0.74\n</code></pre> <pre><code>ax = plt.subplots()[1]\nplot_tree(dtc, feature_names=X.columns, ax=ax)\n</code></pre> <pre><code>[Text(0.4583333333333333, 0.875, 'Price &lt;= 92.5\\nentropy = 0.969\\nsamples = 300\\nvalue = [181.0, 119.0]'),\n Text(0.25, 0.625, 'ShelveLoc_Good &lt;= 0.5\\nentropy = 0.722\\nsamples = 45\\nvalue = [9, 36]'),\n Text(0.35416666666666663, 0.75, 'True  '),\n Text(0.16666666666666666, 0.375, 'Income &lt;= 56.0\\nentropy = 0.845\\nsamples = 33\\nvalue = [9, 24]'),\n Text(0.08333333333333333, 0.125, 'entropy = 0.954\\nsamples = 8\\nvalue = [5, 3]'),\n Text(0.25, 0.125, 'entropy = 0.634\\nsamples = 25\\nvalue = [4, 21]'),\n Text(0.3333333333333333, 0.375, 'entropy = 0.0\\nsamples = 12\\nvalue = [0, 12]'),\n Text(0.6666666666666666, 0.625, 'Advertising &lt;= 6.5\\nentropy = 0.91\\nsamples = 255\\nvalue = [172, 83]'),\n Text(0.5625, 0.75, '  False'),\n Text(0.5, 0.375, 'CompPrice &lt;= 144.5\\nentropy = 0.678\\nsamples = 134\\nvalue = [110, 24]'),\n Text(0.4166666666666667, 0.125, 'entropy = 0.534\\nsamples = 115\\nvalue = [101, 14]'),\n Text(0.5833333333333334, 0.125, 'entropy = 0.998\\nsamples = 19\\nvalue = [9, 10]'),\n Text(0.8333333333333334, 0.375, 'ShelveLoc_Good &lt;= 0.5\\nentropy = 1.0\\nsamples = 121\\nvalue = [62.0, 59.0]'),\n Text(0.75, 0.125, 'entropy = 0.952\\nsamples = 94\\nvalue = [59, 35]'),\n Text(0.9166666666666666, 0.125, 'entropy = 0.503\\nsamples = 27\\nvalue = [3, 24]')]\n</code></pre> <p></p> <pre><code>X_train, X_test, y_train, y_test = skm.train_test_split(X, high_sales, test_size=0.5, random_state=0)\ndtc = DecisionTreeClassifier(criterion=\"entropy\")\ndtc.fit(X_train, y_train)\n</code></pre> <pre>DecisionTreeClassifier(criterion='entropy')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted<pre>DecisionTreeClassifier(criterion='entropy')</pre> <pre><code>ccp = dtc.cost_complexity_pruning_path(X_train, y_train)\nparams = {'ccp_alpha': ccp.ccp_alphas}\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=0)\nmodel = skm.GridSearchCV(dtc, param_grid=params, cv=kfold)\nmodel.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=DecisionTreeClassifier(criterion='entropy'),\n             param_grid={'ccp_alpha': array([0.        , 0.01622556, 0.0171946 , 0.0180482 , 0.0180482 ,\n       0.01991688, 0.02012073, 0.02070855, 0.02193427, 0.0219518 ,\n       0.02220877, 0.02274806, 0.02417233, 0.02588672, 0.02714959,\n       0.02735525, 0.02900052, 0.02906078, 0.03209543, 0.04499252,\n       0.06236632, 0.10024835])})</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=DecisionTreeClassifier(criterion='entropy'),\n             param_grid={'ccp_alpha': array([0.        , 0.01622556, 0.0171946 , 0.0180482 , 0.0180482 ,\n       0.01991688, 0.02012073, 0.02070855, 0.02193427, 0.0219518 ,\n       0.02220877, 0.02274806, 0.02417233, 0.02588672, 0.02714959,\n       0.02735525, 0.02900052, 0.02906078, 0.03209543, 0.04499252,\n       0.06236632, 0.10024835])})</pre> best_estimator_: DecisionTreeClassifier<pre>DecisionTreeClassifier(ccp_alpha=0.018048202372184057, criterion='entropy')</pre> DecisionTreeClassifier?Documentation for DecisionTreeClassifier<pre>DecisionTreeClassifier(ccp_alpha=0.018048202372184057, criterion='entropy')</pre> <pre><code>model.best_estimator_.get_n_leaves(), dtc.get_n_leaves()\nmodel.cv_results_\n</code></pre> <pre><code>{'mean_fit_time': array([0.00433464, 0.00290234, 0.00236988, 0.00263145, 0.00240963,\n        0.00240662, 0.00256596, 0.00232456, 0.00255721, 0.00245774,\n        0.00244524, 0.00223739, 0.00230029, 0.0025923 , 0.00253756,\n        0.00245504, 0.00255251, 0.00247173, 0.0027205 , 0.00255225,\n        0.00256662, 0.00268097]),\n 'std_fit_time': array([0.00297806, 0.00059483, 0.00034463, 0.00059222, 0.00040597,\n        0.00036248, 0.00050071, 0.00021018, 0.00043207, 0.00046749,\n        0.0004039 , 0.00018706, 0.00025085, 0.00050736, 0.00041083,\n        0.00048615, 0.00056722, 0.00048864, 0.00056649, 0.00058765,\n        0.00053271, 0.00049404]),\n 'mean_score_time': array([0.0029093 , 0.00165648, 0.00122352, 0.00149612, 0.00130613,\n        0.00134208, 0.00132575, 0.00129905, 0.00140755, 0.00133898,\n        0.00135987, 0.00122116, 0.00148056, 0.00144806, 0.00151777,\n        0.00139794, 0.00143487, 0.00138805, 0.00145013, 0.00146112,\n        0.0013844 , 0.00153246]),\n 'std_score_time': array([0.00257493, 0.00037032, 0.00013903, 0.0004048 , 0.00034704,\n        0.00033574, 0.00020024, 0.00019787, 0.0002398 , 0.00033398,\n        0.00035035, 0.00018485, 0.00036048, 0.00055823, 0.00056115,\n        0.00039527, 0.0003866 , 0.00035872, 0.00040886, 0.00039925,\n        0.00038439, 0.00036823]),\n 'param_ccp_alpha': masked_array(data=[0.0, 0.016225562489182655, 0.017194601396443954,\n                    0.018048202372184057, 0.018048202372184057,\n                    0.01991687712104144, 0.020120733983535546,\n                    0.020708547250381463, 0.021934274536246717,\n                    0.021951797627815944, 0.022208773417819985,\n                    0.022748055645587552, 0.024172334280683237,\n                    0.025886720366496196, 0.02714958965089037,\n                    0.027355254741740692, 0.029000519903033605,\n                    0.029060777786737286, 0.03209542720150803,\n                    0.04499251649788086, 0.062366324849087884,\n                    0.10024834625148749],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False],\n        fill_value=1e+20),\n 'params': [{'ccp_alpha': 0.0},\n  {'ccp_alpha': 0.016225562489182655},\n  {'ccp_alpha': 0.017194601396443954},\n  {'ccp_alpha': 0.018048202372184057},\n  {'ccp_alpha': 0.018048202372184057},\n  {'ccp_alpha': 0.01991687712104144},\n  {'ccp_alpha': 0.020120733983535546},\n  {'ccp_alpha': 0.020708547250381463},\n  {'ccp_alpha': 0.021934274536246717},\n  {'ccp_alpha': 0.021951797627815944},\n  {'ccp_alpha': 0.022208773417819985},\n  {'ccp_alpha': 0.022748055645587552},\n  {'ccp_alpha': 0.024172334280683237},\n  {'ccp_alpha': 0.025886720366496196},\n  {'ccp_alpha': 0.02714958965089037},\n  {'ccp_alpha': 0.027355254741740692},\n  {'ccp_alpha': 0.029000519903033605},\n  {'ccp_alpha': 0.029060777786737286},\n  {'ccp_alpha': 0.03209542720150803},\n  {'ccp_alpha': 0.04499251649788086},\n  {'ccp_alpha': 0.062366324849087884},\n  {'ccp_alpha': 0.10024834625148749}],\n 'split0_test_score': array([0.7 , 0.7 , 0.75, 0.7 , 0.75, 0.75, 0.7 , 0.7 , 0.7 , 0.7 , 0.7 ,\n        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7 , 0.7 , 0.7 , 0.65]),\n 'split1_test_score': array([0.75, 0.7 , 0.7 , 0.8 , 0.8 , 0.75, 0.85, 0.75, 0.8 , 0.75, 0.75,\n        0.8 , 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.7 , 0.85, 0.75, 0.85]),\n 'split2_test_score': array([0.75, 0.75, 0.7 , 0.65, 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.7 ,\n        0.7 , 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.9 , 0.8 ]),\n 'split3_test_score': array([0.6 , 0.65, 0.6 , 0.8 , 0.8 , 0.75, 0.75, 0.75, 0.75, 0.7 , 0.75,\n        0.75, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.6 , 0.6 , 0.45]),\n 'split4_test_score': array([0.6 , 0.55, 0.6 , 0.65, 0.6 , 0.55, 0.6 , 0.6 , 0.6 , 0.6 , 0.6 ,\n        0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.6 , 0.55, 0.55, 0.55]),\n 'split5_test_score': array([0.7 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 ,\n        0.8 , 0.8 , 0.8 , 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.7 , 0.5 ]),\n 'split6_test_score': array([0.75, 0.65, 0.6 , 0.65, 0.65, 0.65, 0.65, 0.65, 0.6 , 0.6 , 0.6 ,\n        0.6 , 0.6 , 0.6 , 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.6 , 0.55]),\n 'split7_test_score': array([0.65, 0.75, 0.7 , 0.75, 0.75, 0.7 , 0.75, 0.7 , 0.7 , 0.7 , 0.7 ,\n        0.75, 0.65, 0.7 , 0.65, 0.7 , 0.65, 0.65, 0.7 , 0.75, 0.7 , 0.7 ]),\n 'split8_test_score': array([0.7 , 0.7 , 0.75, 0.7 , 0.7 , 0.7 , 0.7 , 0.75, 0.75, 0.75, 0.75,\n        0.75, 0.75, 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.6 , 0.6 , 0.45]),\n 'split9_test_score': array([0.75, 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.8 , 0.75, 0.75, 0.75,\n        0.75, 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.65, 0.65, 0.6 ]),\n 'mean_test_score': array([0.695, 0.705, 0.7  , 0.73 , 0.735, 0.715, 0.73 , 0.72 , 0.715,\n        0.705, 0.71 , 0.725, 0.71 , 0.71 , 0.695, 0.7  , 0.695, 0.695,\n        0.68 , 0.675, 0.675, 0.61 ]),\n 'std_test_score': array([0.05678908, 0.07228416, 0.07416198, 0.06403124, 0.06726812,\n        0.07088723, 0.07141428, 0.06      , 0.06726812, 0.06103278,\n        0.06244998, 0.06800735, 0.08      , 0.07681146, 0.0820061 ,\n        0.08062258, 0.0820061 , 0.0820061 , 0.06      , 0.09552487,\n        0.09552487, 0.13190906]),\n 'rank_test_score': array([15, 11, 13,  2,  1,  6,  3,  5,  6, 12,  9,  4,  8,  9, 15, 13, 15,\n        15, 19, 20, 20, 22], dtype=int32)}\n</code></pre> <pre><code>accuracy_score(y_test, model.best_estimator_.predict(X_test))\n</code></pre> <pre><code>0.705\n</code></pre> <pre><code>DecisionTreeClassifier?\n</code></pre> <pre><code>\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msplitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nA decision tree classifier.\n\nRead more in the :ref:`User Guide &lt;tree&gt;`.\n\nParameters\n----------\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n\nsplitter : {\"best\", \"random\"}, default=\"best\"\n    The strategy used to choose the split at each node. Supported\n    strategies are \"best\" to choose the best split and \"random\" to choose\n    the best random split.\n\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\n\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n\n    .. versionchanged:: 0.18\n       Added float values for fractions.\n\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\n\nmax_features : int, float or {\"sqrt\", \"log2\"}, default=None\n    The number of features to consider when looking for the best split:\n\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at\n      each split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n\n    .. note::\n\n        The search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\nrandom_state : int, RandomState instance or None, default=None\n    Controls the randomness of the estimator. The features are always\n    randomly permuted at each split, even if ``splitter`` is set to\n    ``\"best\"``. When ``max_features &lt; n_features``, the algorithm will\n    select ``max_features`` at random at each split before finding the best\n    split among them. But the best found split may vary across different\n    runs, even if ``max_features=n_features``. That is the case, if the\n    improvement of the criterion is identical for several splits and one\n    split has to be selected at random. To obtain a deterministic behaviour\n    during fitting, ``random_state`` has to be fixed to an integer.\n    See :term:`Glossary &lt;random_state&gt;` for details.\n\nmax_leaf_nodes : int, default=None\n    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\n\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n\n    The weighted impurity decrease equation is the following::\n\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n\n    .. versionadded:: 0.19\n\nclass_weight : dict, list of dict or \"balanced\", default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If None, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n\n    For multi-output, the weights of each column of y will be multiplied.\n\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\n\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details. See\n    :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n    for an example of such pruning.\n\n    .. versionadded:: 0.22\n\nmonotonic_cst : array-like of int of shape (n_features), default=None\n    Indicates the monotonicity constraint to enforce on each feature.\n      - 1: monotonic increase\n      - 0: no constraint\n      - -1: monotonic decrease\n\n    If monotonic_cst is None, no constraints are applied.\n\n    Monotonicity constraints are not supported for:\n      - multiclass classifications (i.e. when `n_classes &gt; 2`),\n      - multioutput classifications (i.e. when `n_outputs_ &gt; 1`),\n      - classifications trained on data with missing values.\n\n    The constraints hold over the probability of the positive class.\n\n    Read more in the :ref:`User Guide &lt;monotonic_cst_gbdt&gt;`.\n\n    .. versionadded:: 1.4\n\nAttributes\n----------\nclasses_ : ndarray of shape (n_classes,) or list of ndarray\n    The classes labels (single output problem),\n    or a list of arrays of class labels (multi-output problem).\n\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance [4]_.\n\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\n\nmax_features_ : int\n    The inferred value of max_features.\n\nn_classes_ : int or list of int\n    The number of classes (for single output problems),\n    or a list containing the number of classes for each\n    output (for multi-output problems).\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\n\ntree_ : Tree instance\n    The underlying Tree object. Please refer to\n    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n    for basic usage of these attributes.\n\nSee Also\n--------\nDecisionTreeRegressor : A decision tree regressor.\n\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\n\nThe :meth:`predict` method operates using the :func:`numpy.argmax`\nfunction on the outputs of :meth:`predict_proba`. This means that in\ncase the highest predicted probabilities are tied, the classifier will\npredict the tied class with the lowest index in :term:`classes_`.\n\nReferences\n----------\n\n.. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n\n.. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n\n.. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n       Learning\", Springer, 2009.\n\n.. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from sklearn.model_selection import cross_val_score\n&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier\n&gt;&gt;&gt; clf = DecisionTreeClassifier(random_state=0)\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; cross_val_score(clf, iris.data, iris.target, cv=10)\n...                             # doctest: +SKIP\n...\narray([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n\u001b[0;31mFile:\u001b[0m           ~/Lab/islp/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py\n\u001b[0;31mType:\u001b[0m           ABCMeta\n\u001b[0;31mSubclasses:\u001b[0m     ExtraTreeClassifier\n</code></pre> <pre><code>skm.KFold?\n</code></pre> <pre><code>\u001b[0;31mInit signature:\u001b[0m \u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nK-Fold cross-validator.\n\nProvides train/test indices to split data in train/test sets. Split\ndataset into k consecutive folds (without shuffling by default).\n\nEach fold is then used once as a validation while the k - 1 remaining\nfolds form the training set.\n\nRead more in the :ref:`User Guide &lt;k_fold&gt;`.\n\nFor visualisation of cross-validation behaviour and\ncomparison between common scikit-learn split methods\nrefer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n\nParameters\n----------\nn_splits : int, default=5\n    Number of folds. Must be at least 2.\n\n    .. versionchanged:: 0.22\n        ``n_splits`` default value changed from 3 to 5.\n\nshuffle : bool, default=False\n    Whether to shuffle the data before splitting into batches.\n    Note that the samples within each split will not be shuffled.\n\nrandom_state : int, RandomState instance or None, default=None\n    When `shuffle` is True, `random_state` affects the ordering of the\n    indices, which controls the randomness of each fold. Otherwise, this\n    parameter has no effect.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary &lt;random_state&gt;`.\n\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.model_selection import KFold\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n&gt;&gt;&gt; y = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; kf = KFold(n_splits=2)\n&gt;&gt;&gt; kf.get_n_splits(X)\n2\n&gt;&gt;&gt; print(kf)\nKFold(n_splits=2, random_state=None, shuffle=False)\n&gt;&gt;&gt; for i, (train_index, test_index) in enumerate(kf.split(X)):\n...     print(f\"Fold {i}:\")\n...     print(f\"  Train: index={train_index}\")\n...     print(f\"  Test:  index={test_index}\")\nFold 0:\n  Train: index=[2 3]\n  Test:  index=[0 1]\nFold 1:\n  Train: index=[0 1]\n  Test:  index=[2 3]\n\nNotes\n-----\nThe first ``n_samples % n_splits`` folds have size\n``n_samples // n_splits + 1``, other folds have size\n``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n\nRandomized CV splitters may return different results for each call of\nsplit. You can make the results identical by setting `random_state`\nto an integer.\n\nSee Also\n--------\nStratifiedKFold : Takes class information into account to avoid building\n    folds with imbalanced class distributions (for binary or multiclass\n    classification tasks).\n\nGroupKFold : K-fold iterator variant with non-overlapping groups.\n\nRepeatedKFold : Repeats K-Fold n times.\n\u001b[0;31mFile:\u001b[0m           ~/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py\n\u001b[0;31mType:\u001b[0m           ABCMeta\n\u001b[0;31mSubclasses:\u001b[0m\n</code></pre> <pre><code>skm.GridSearchCV?\n</code></pre> <pre><code>\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2*n_jobs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nExhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"score_samples\", \"predict\", \"predict_proba\",\n\"decision_function\", \"transform\" and \"inverse_transform\" if they are\nimplemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated grid-search over a parameter grid.\n\nRead more in the :ref:`User Guide &lt;grid_search&gt;`.\n\nParameters\n----------\nestimator : estimator object\n    This is assumed to implement the scikit-learn estimator interface.\n    Either estimator needs to provide a ``score`` function,\n    or ``scoring`` must be passed.\n\nparam_grid : dict or list of dictionaries\n    Dictionary with parameters names (`str`) as keys and lists of\n    parameter settings to try as values, or a list of such\n    dictionaries, in which case the grids spanned by each dictionary\n    in the list are explored. This enables searching over any sequence\n    of parameter settings.\n\nscoring : str, callable, list, tuple or dict, default=None\n    Strategy to evaluate the performance of the cross-validated model on\n    the test set.\n\n    If `scoring` represents a single score, one can use:\n\n    - a single string (see :ref:`scoring_parameter`);\n    - a callable (see :ref:`scoring_callable`) that returns a single value.\n\n    If `scoring` represents multiple scores, one can use:\n\n    - a list or tuple of unique strings;\n    - a callable returning a dictionary where the keys are the metric\n      names and the values are the metric scores;\n    - a dictionary with metric names as keys and callables as values.\n\n    See :ref:`multimetric_grid_search` for an example.\n\nn_jobs : int, default=None\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`\n    for more details.\n\n    .. versionchanged:: v0.20\n       `n_jobs` default changed from 1 to None\n\nrefit : bool, str, or callable, default=True\n    Refit an estimator using the best found parameters on the whole\n    dataset.\n\n    For multiple metric evaluation, this needs to be a `str` denoting the\n    scorer that would be used to find the best parameters for refitting\n    the estimator at the end.\n\n    Where there are considerations other than maximum score in\n    choosing a best estimator, ``refit`` can be set to a function which\n    returns the selected ``best_index_`` given ``cv_results_``. In that\n    case, the ``best_estimator_`` and ``best_params_`` will be set\n    according to the returned ``best_index_`` while the ``best_score_``\n    attribute will not be available.\n\n    The refitted estimator is made available at the ``best_estimator_``\n    attribute and permits using ``predict`` directly on this\n    ``GridSearchCV`` instance.\n\n    Also for multiple metric evaluation, the attributes ``best_index_``,\n    ``best_score_`` and ``best_params_`` will only be available if\n    ``refit`` is set and all of them will be determined w.r.t this specific\n    scorer.\n\n    See ``scoring`` parameter to know more about multiple metric\n    evaluation.\n\n    See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n    to see how to design a custom selection strategy using a callable\n    via `refit`.\n\n    .. versionchanged:: 0.20\n        Support for callable added.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used. These splitters are instantiated\n    with `shuffle=False` so the splits will be the same across calls.\n\n    Refer :ref:`User Guide &lt;cross_validation&gt;` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nverbose : int\n    Controls the verbosity: the higher, the more messages.\n\n    - &gt;1 : the computation time for each fold and parameter candidate is\n      displayed;\n    - &gt;2 : the score is also displayed;\n    - &gt;3 : the fold and candidate parameter indexes are also displayed\n      together with the starting time of the computation.\n\npre_dispatch : int, or str, default='2*n_jobs'\n    Controls the number of jobs that get dispatched during parallel\n    execution. Reducing this number can be useful to avoid an\n    explosion of memory consumption when more jobs get dispatched\n    than CPUs can process. This parameter can be:\n\n    - None, in which case all the jobs are immediately created and spawned. Use\n      this for lightweight and fast-running jobs, to avoid delays due to on-demand\n      spawning of the jobs\n    - An int, giving the exact number of total jobs that are spawned\n    - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n\nerror_score : 'raise' or numeric, default=np.nan\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised. If a numeric value is given,\n    FitFailedWarning is raised. This parameter does not affect the refit\n    step, which will always raise the error.\n\nreturn_train_score : bool, default=False\n    If ``False``, the ``cv_results_`` attribute will not include training\n    scores.\n    Computing training scores is used to get insights on how different\n    parameter settings impact the overfitting/underfitting trade-off.\n    However computing the scores on the training set can be computationally\n    expensive and is not strictly required to select the parameters that\n    yield the best generalization performance.\n\n    .. versionadded:: 0.19\n\n    .. versionchanged:: 0.21\n        Default value was changed from ``True`` to ``False``\n\nAttributes\n----------\ncv_results_ : dict of numpy (masked) ndarrays\n    A dict with keys as column headers and values as columns, that can be\n    imported into a pandas ``DataFrame``.\n\n    For instance the below given table\n\n    +------------+-----------+------------+-----------------+---+---------+\n    |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n    +============+===========+============+=================+===+=========+\n    |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n    +------------+-----------+------------+-----------------+---+---------+\n    |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n    +------------+-----------+------------+-----------------+---+---------+\n\n    will be represented by a ``cv_results_`` dict of::\n\n        {\n        'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                     mask = [False False False False]...)\n        'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                    mask = [ True  True False False]...),\n        'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                     mask = [False False  True  True]...),\n        'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n        'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n        'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n        'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n        'rank_test_score'    : [2, 4, 3, 1],\n        'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n        'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n        'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n        'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n        'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n        'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n        'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n        'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n        'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n        }\n\n    NOTE\n\n    The key ``'params'`` is used to store a list of parameter\n    settings dicts for all the parameter candidates.\n\n    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n    ``std_score_time`` are all in seconds.\n\n    For multi-metric evaluation, the scores for all the scorers are\n    available in the ``cv_results_`` dict at the keys ending with that\n    scorer's name (``'_&lt;scorer_name&gt;'``) instead of ``'_score'`` shown\n    above. ('split0_test_precision', 'mean_train_precision' etc.)\n\nbest_estimator_ : estimator\n    Estimator that was chosen by the search, i.e. estimator\n    which gave highest score (or smallest loss if specified)\n    on the left out data. Not available if ``refit=False``.\n\n    See ``refit`` parameter for more information on allowed values.\n\nbest_score_ : float\n    Mean cross-validated score of the best_estimator\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\n    This attribute is not available if ``refit`` is a function.\n\nbest_params_ : dict\n    Parameter setting that gave the best results on the hold out data.\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\nbest_index_ : int\n    The index (of the ``cv_results_`` arrays) which corresponds to the best\n    candidate parameter setting.\n\n    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n    the parameter setting for the best model, that gives the highest\n    mean score (``search.best_score_``).\n\n    For multi-metric evaluation, this is present only if ``refit`` is\n    specified.\n\nscorer_ : function or a dict\n    Scorer function used on the held out data to choose the best\n    parameters for the model.\n\n    For multi-metric evaluation, this attribute holds the validated\n    ``scoring`` dict which maps the scorer key to the scorer callable.\n\nn_splits_ : int\n    The number of cross-validation splits (folds/iterations).\n\nrefit_time_ : float\n    Seconds used for refitting the best model on the whole dataset.\n\n    This is present only if ``refit`` is not False.\n\n    .. versionadded:: 0.20\n\nmultimetric_ : bool\n    Whether or not the scorers compute several metrics.\n\nclasses_ : ndarray of shape (n_classes,)\n    The classes labels. This is present only if ``refit`` is specified and\n    the underlying estimator is a classifier.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `n_features_in_` when fit.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Only defined if\n    `best_estimator_` is defined (see the documentation for the `refit`\n    parameter for more details) and that `best_estimator_` exposes\n    `feature_names_in_` when fit.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nParameterGrid : Generates all the combinations of a hyperparameter grid.\ntrain_test_split : Utility function to split the data into a development\n    set usable for fitting a GridSearchCV instance and an evaluation set\n    for its final evaluation.\nsklearn.metrics.make_scorer : Make a scorer from a performance metric or\n    loss function.\n\nNotes\n-----\nThe parameters selected are those that maximize the score of the left out\ndata, unless an explicit score is passed in which case it is used instead.\n\nIf `n_jobs` was set to a value higher than one, the data is copied for each\npoint in the grid (and not `n_jobs` times). This is done for efficiency\nreasons if individual jobs take very little time, but may raise errors if\nthe dataset is large and not enough memory is available.  A workaround in\nthis case is to set `pre_dispatch`. Then, the memory is copied only\n`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\nn_jobs`.\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn import svm, datasets\n&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV\n&gt;&gt;&gt; iris = datasets.load_iris()\n&gt;&gt;&gt; parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n&gt;&gt;&gt; svc = svm.SVC()\n&gt;&gt;&gt; clf = GridSearchCV(svc, parameters)\n&gt;&gt;&gt; clf.fit(iris.data, iris.target)\nGridSearchCV(estimator=SVC(),\n             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n&gt;&gt;&gt; sorted(clf.cv_results_.keys())\n['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n 'param_C', 'param_kernel', 'params',...\n 'rank_test_score', 'split0_test_score',...\n 'split2_test_score', ...\n 'std_fit_time', 'std_score_time', 'std_test_score']\n\u001b[0;31mFile:\u001b[0m           ~/Lab/islp/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py\n\u001b[0;31mType:\u001b[0m           ABCMeta\n\u001b[0;31mSubclasses:\u001b[0m\n</code></pre> <pre><code>dtc.cost_complexity_pruning_path?\n</code></pre> <pre><code>\u001b[0;31mSignature:\u001b[0m \u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_complexity_pruning_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nCompute the pruning path during Minimal Cost-Complexity Pruning.\n\nSee :ref:`minimal_cost_complexity_pruning` for details on the pruning\nprocess.\n\nParameters\n----------\nX : {array-like, sparse matrix} of shape (n_samples, n_features)\n    The training input samples. Internally, it will be converted to\n    ``dtype=np.float32`` and if a sparse matrix is provided\n    to a sparse ``csc_matrix``.\n\ny : array-like of shape (n_samples,) or (n_samples, n_outputs)\n    The target values (class labels) as integers or strings.\n\nsample_weight : array-like of shape (n_samples,), default=None\n    Sample weights. If None, then samples are equally weighted. Splits\n    that would create child nodes with net zero or negative weight are\n    ignored while searching for a split in each node. Splits are also\n    ignored if they would result in any single class carrying a\n    negative weight in either child node.\n\nReturns\n-------\nccp_path : :class:`~sklearn.utils.Bunch`\n    Dictionary-like object, with the following attributes.\n\n    ccp_alphas : ndarray\n        Effective alphas of subtree during pruning.\n\n    impurities : ndarray\n        Sum of the impurities of the subtree leaves for the\n        corresponding alpha value in ``ccp_alphas``.\n\u001b[0;31mFile:\u001b[0m      ~/Lab/islp/venv/lib/python3.12/site-packages/sklearn/tree/_classes.py\n\u001b[0;31mType:\u001b[0m      method\n</code></pre> <pre><code>ccp = dtc.cost_complexity_pruning_path(X_train, y_train)\n\n</code></pre> <pre><code>boston = load_data(\"Boston\")\nboston\n</code></pre> crim zn indus chas nox rm age dis rad tax ptratio lstat medv 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 5.33 36.2 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 501 0.06263 0.0 11.93 0 0.573 6.593 69.1 2.4786 1 273 21.0 9.67 22.4 502 0.04527 0.0 11.93 0 0.573 6.120 76.7 2.2875 1 273 21.0 9.08 20.6 503 0.06076 0.0 11.93 0 0.573 6.976 91.0 2.1675 1 273 21.0 5.64 23.9 504 0.10959 0.0 11.93 0 0.573 6.794 89.3 2.3889 1 273 21.0 6.48 22.0 505 0.04741 0.0 11.93 0 0.573 6.030 80.8 2.5050 1 273 21.0 7.88 11.9 <p>506 rows \u00d7 13 columns</p> <pre><code>X = boston.drop(columns=[\"medv\"])\ny = boston[\"medv\"]\nX_train, X_test, y_train, y_test = skm.train_test_split(X, y, test_size=0.5, random_state=0)\n# scaler = StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)\n</code></pre> <pre><code>dtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\nnp.mean((y_test-dtr.predict(X_test))**2), dtr.get_n_leaves()\n</code></pre> <pre><code>(26.542569169960473, 238)\n</code></pre> <pre><code>ccp = dtr.cost_complexity_pruning_path(X_train, y_train)\nparams = {'ccp_alpha': ccp.ccp_alphas}\nkfold = skm.KFold(n_splits=10, shuffle=True, random_state=0)\nmodel = skm.GridSearchCV(dtr, param_grid=params, cv=kfold, scoring='neg_mean_squared_error')\nmodel.fit(X_train, y_train)\n</code></pre> <pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(),\n             param_grid={'ccp_alpha': array([0.00000000e+00, 4.49355090e-16, 1.79742036e-15, 1.97628458e-05,\n       1.97628458e-05, 1.97628458e-05, 1.97628458e-05, 1.97628458e-05,\n       1.97628458e-05, 1.97628458e-05, 1.97628458e-05, 1.97628459e-05,\n       1.97628459e-05, 1.97628459e-05, 1.97628459e-...\n       1.74563061e-01, 1.84099944e-01, 1.90909989e-01, 1.98409938e-01,\n       2.11783048e-01, 2.21983060e-01, 2.65387258e-01, 2.80502541e-01,\n       2.95232962e-01, 4.96709706e-01, 6.35314537e-01, 6.61084017e-01,\n       7.90843215e-01, 8.81586298e-01, 9.20701196e-01, 1.19620193e+00,\n       2.08525213e+00, 2.91359587e+00, 5.48209786e+00, 8.19739880e+00,\n       1.58561557e+01, 4.59185361e+01])},\n             scoring='neg_mean_squared_error')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=KFold(n_splits=10, random_state=0, shuffle=True),\n             estimator=DecisionTreeRegressor(),\n             param_grid={'ccp_alpha': array([0.00000000e+00, 4.49355090e-16, 1.79742036e-15, 1.97628458e-05,\n       1.97628458e-05, 1.97628458e-05, 1.97628458e-05, 1.97628458e-05,\n       1.97628458e-05, 1.97628458e-05, 1.97628458e-05, 1.97628459e-05,\n       1.97628459e-05, 1.97628459e-05, 1.97628459e-...\n       1.74563061e-01, 1.84099944e-01, 1.90909989e-01, 1.98409938e-01,\n       2.11783048e-01, 2.21983060e-01, 2.65387258e-01, 2.80502541e-01,\n       2.95232962e-01, 4.96709706e-01, 6.35314537e-01, 6.61084017e-01,\n       7.90843215e-01, 8.81586298e-01, 9.20701196e-01, 1.19620193e+00,\n       2.08525213e+00, 2.91359587e+00, 5.48209786e+00, 8.19739880e+00,\n       1.58561557e+01, 4.59185361e+01])},\n             scoring='neg_mean_squared_error')</pre> best_estimator_: DecisionTreeRegressor<pre>DecisionTreeRegressor(ccp_alpha=0.17052700922263658)</pre> DecisionTreeRegressor?Documentation for DecisionTreeRegressor<pre>DecisionTreeRegressor(ccp_alpha=0.17052700922263658)</pre> <pre><code>np.mean((y_test-model.best_estimator_.predict(X_test))**2), model.best_estimator_.get_n_leaves()\n</code></pre> <pre><code>(22.419123269550663, 25)\n</code></pre> <pre><code>_, ax = plt.subplots()\nplot_tree(model.best_estimator_, feature_names=X.columns, ax=ax)\n</code></pre> <pre><code>[Text(0.49193548387096775, 0.9444444444444444, 'lstat &lt;= 7.81\\nsquared_error = 92.491\\nsamples = 253\\nvalue = 23.049'),\n Text(0.24193548387096775, 0.8333333333333334, 'rm &lt;= 7.435\\nsquared_error = 84.345\\nsamples = 86\\nvalue = 32.492'),\n Text(0.36693548387096775, 0.8888888888888888, 'True  '),\n Text(0.12903225806451613, 0.7222222222222222, 'dis &lt;= 1.485\\nsquared_error = 41.903\\nsamples = 68\\nvalue = 28.978'),\n Text(0.0967741935483871, 0.6111111111111112, 'squared_error = 0.0\\nsamples = 3\\nvalue = 50.0'),\n Text(0.16129032258064516, 0.6111111111111112, 'rm &lt;= 6.556\\nsquared_error = 22.499\\nsamples = 65\\nvalue = 28.008'),\n Text(0.12903225806451613, 0.5, 'squared_error = 2.618\\nsamples = 25\\nvalue = 23.748'),\n Text(0.1935483870967742, 0.5, 'rm &lt;= 7.104\\nsquared_error = 16.496\\nsamples = 40\\nvalue = 30.67'),\n Text(0.16129032258064516, 0.3888888888888889, 'age &lt;= 51.1\\nsquared_error = 12.905\\nsamples = 30\\nvalue = 29.307'),\n Text(0.0967741935483871, 0.2777777777777778, 'ptratio &lt;= 18.85\\nsquared_error = 7.328\\nsamples = 18\\nvalue = 30.978'),\n Text(0.06451612903225806, 0.16666666666666666, 'indus &lt;= 6.66\\nsquared_error = 5.454\\nsamples = 14\\nvalue = 31.9'),\n Text(0.03225806451612903, 0.05555555555555555, 'squared_error = 2.601\\nsamples = 12\\nvalue = 32.617'),\n Text(0.0967741935483871, 0.05555555555555555, 'squared_error = 1.0\\nsamples = 2\\nvalue = 27.6'),\n Text(0.12903225806451613, 0.16666666666666666, 'squared_error = 0.493\\nsamples = 4\\nvalue = 27.75'),\n Text(0.22580645161290322, 0.2777777777777778, 'rad &lt;= 2.5\\nsquared_error = 10.798\\nsamples = 12\\nvalue = 26.8'),\n Text(0.1935483870967742, 0.16666666666666666, 'squared_error = 4.562\\nsamples = 5\\nvalue = 24.38'),\n Text(0.25806451612903225, 0.16666666666666666, 'dis &lt;= 3.341\\nsquared_error = 8.082\\nsamples = 7\\nvalue = 28.529'),\n Text(0.22580645161290322, 0.05555555555555555, 'squared_error = 1.878\\nsamples = 5\\nvalue = 30.16'),\n Text(0.2903225806451613, 0.05555555555555555, 'squared_error = 0.302\\nsamples = 2\\nvalue = 24.45'),\n Text(0.22580645161290322, 0.3888888888888889, 'squared_error = 4.964\\nsamples = 10\\nvalue = 34.76'),\n Text(0.3548387096774194, 0.7222222222222222, 'ptratio &lt;= 15.4\\nsquared_error = 21.812\\nsamples = 18\\nvalue = 45.767'),\n Text(0.2903225806451613, 0.6111111111111112, 'nox &lt;= 0.416\\nsquared_error = 5.79\\nsamples = 11\\nvalue = 48.636'),\n Text(0.25806451612903225, 0.5, 'squared_error = 0.0\\nsamples = 1\\nvalue = 42.3'),\n Text(0.3225806451612903, 0.5, 'squared_error = 1.952\\nsamples = 10\\nvalue = 49.27'),\n Text(0.41935483870967744, 0.6111111111111112, 'age &lt;= 44.35\\nsquared_error = 13.714\\nsamples = 7\\nvalue = 41.257'),\n Text(0.3870967741935484, 0.5, 'squared_error = 1.749\\nsamples = 3\\nvalue = 44.833'),\n Text(0.45161290322580644, 0.5, 'squared_error = 5.902\\nsamples = 4\\nvalue = 38.575'),\n Text(0.7419354838709677, 0.8333333333333334, 'lstat &lt;= 15.0\\nsquared_error = 27.121\\nsamples = 167\\nvalue = 18.186'),\n Text(0.6169354838709677, 0.8888888888888888, '  False'),\n Text(0.6129032258064516, 0.7222222222222222, 'rm &lt;= 6.53\\nsquared_error = 12.255\\nsamples = 87\\nvalue = 21.566'),\n Text(0.5483870967741935, 0.6111111111111112, 'ptratio &lt;= 18.65\\nsquared_error = 6.248\\nsamples = 74\\nvalue = 20.784'),\n Text(0.5161290322580645, 0.5, 'tax &lt;= 208.0\\nsquared_error = 6.745\\nsamples = 31\\nvalue = 21.855'),\n Text(0.4838709677419355, 0.3888888888888889, 'squared_error = 2.56\\nsamples = 2\\nvalue = 28.0'),\n Text(0.5483870967741935, 0.3888888888888889, 'squared_error = 4.25\\nsamples = 29\\nvalue = 21.431'),\n Text(0.5806451612903226, 0.5, 'squared_error = 4.466\\nsamples = 43\\nvalue = 20.012'),\n Text(0.6774193548387096, 0.6111111111111112, 'ptratio &lt;= 19.3\\nsquared_error = 23.169\\nsamples = 13\\nvalue = 26.015'),\n Text(0.6451612903225806, 0.5, 'rm &lt;= 6.947\\nsquared_error = 10.304\\nsamples = 10\\nvalue = 27.98'),\n Text(0.6129032258064516, 0.3888888888888889, 'squared_error = 5.648\\nsamples = 7\\nvalue = 26.429'),\n Text(0.6774193548387096, 0.3888888888888889, 'squared_error = 2.447\\nsamples = 3\\nvalue = 31.6'),\n Text(0.7096774193548387, 0.5, 'squared_error = 10.302\\nsamples = 3\\nvalue = 19.467'),\n Text(0.8709677419354839, 0.7222222222222222, 'dis &lt;= 1.918\\nsquared_error = 17.363\\nsamples = 80\\nvalue = 14.511'),\n Text(0.8064516129032258, 0.6111111111111112, 'tax &lt;= 551.5\\nsquared_error = 13.188\\nsamples = 36\\nvalue = 11.672'),\n Text(0.7741935483870968, 0.5, 'squared_error = 3.129\\nsamples = 9\\nvalue = 15.756'),\n Text(0.8387096774193549, 0.5, 'lstat &lt;= 19.645\\nsquared_error = 9.131\\nsamples = 27\\nvalue = 10.311'),\n Text(0.8064516129032258, 0.3888888888888889, 'squared_error = 4.756\\nsamples = 5\\nvalue = 13.8'),\n Text(0.8709677419354839, 0.3888888888888889, 'crim &lt;= 30.201\\nsquared_error = 6.73\\nsamples = 22\\nvalue = 9.518'),\n Text(0.8387096774193549, 0.2777777777777778, 'squared_error = 5.059\\nsamples = 18\\nvalue = 10.217'),\n Text(0.9032258064516129, 0.2777777777777778, 'squared_error = 2.172\\nsamples = 4\\nvalue = 6.375'),\n Text(0.9354838709677419, 0.6111111111111112, 'crim &lt;= 0.593\\nsquared_error = 8.79\\nsamples = 44\\nvalue = 16.834'),\n Text(0.9032258064516129, 0.5, 'squared_error = 5.269\\nsamples = 16\\nvalue = 19.363'),\n Text(0.967741935483871, 0.5, 'squared_error = 5.061\\nsamples = 28\\nvalue = 15.389')]\n</code></pre> <p></p> <pre><code>bag = RandomForestRegressor(max_features=X_train.shape[1], random_state=0)\nbag.fit(X_train, y_train)\nnp.mean((y_test - bag.predict(X_test))**2)\n</code></pre> <pre><code>15.998719418972334\n</code></pre> <pre><code>rf = RandomForestRegressor(max_features=\"sqrt\", random_state=0)\nrf.fit(X_train, y_train)\nnp.mean((y_test - rf.predict(X_test))**2)\n</code></pre> <pre><code>15.54812255731225\n</code></pre> <pre><code>pd.DataFrame({\"importance\": rf.feature_importances_}, index=X_train.columns).sort_values(by=\"importance\")\n\n</code></pre> importance chas 0.008253 rad 0.010506 zn 0.017884 age 0.034596 tax 0.040578 indus 0.048895 dis 0.055616 crim 0.068045 nox 0.073336 ptratio 0.076813 rm 0.275036 lstat 0.290442 <pre><code>gbr = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.01, random_state=0)\ngbr.fit(X_train, y_train)\nnp.mean((y_test-gbr.predict(X_test))**2)\n</code></pre> <pre><code>15.118799219930267\n</code></pre> <pre><code>mse = np.zeros_like(gbr.train_score_)\nfor i, y_ in enumerate(gbr.staged_predict(X_test)):\n    mse[i] = np.mean((y_test-y_)**2)\n\n_, ax = plt.subplots()\nax.plot(np.arange(len(mse)), gbr.train_score_, 'b', label=\"train\")\nax.plot(np.arange(len(mse)), mse, 'r', label=\"test\")\nax.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7eff603da810&gt;\n</code></pre> <p></p> <pre><code>pd.DataFrame({\"importance\": gbr.feature_importances_}, index=X_train.columns).sort_values(by=\"importance\")\n</code></pre> importance zn 0.000377 chas 0.001123 rad 0.002500 indus 0.004539 nox 0.009962 tax 0.012848 age 0.013178 crim 0.029443 ptratio 0.044015 dis 0.062999 rm 0.278958 lstat 0.540057 <pre><code>gbr?\n</code></pre> <pre><code>Object `gbr` not found.\n</code></pre> <pre><code>\n</code></pre>"},{"location":"z_ISLP%20demo9/","title":"z ISLP demo9","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection as skm\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, RocCurveDisplay\nfrom ISLP import load_data\nfrom ISLP.svm import plot as plot_svm\n</code></pre> <pre><code>rng = np.random.default_rng(1)\nX = rng.standard_normal((50, 2))\ny = np.array ([ -1]*25+[1]*25)\nX[y==1] += 1\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize=(4,4))\nax.scatter(X[:,0], X[:,1], c=y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x74990817da30&gt;\n</code></pre> <pre><code>svc = SVC(C=0.1, kernel='linear')\nsvc.fit(X, y)\nplot_svm(X, y, svc)\n</code></pre> <pre>SVC(C=0.1, kernel='linear')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted<pre>SVC(C=0.1, kernel='linear')</pre> <pre><code>svc.intercept_, svc.coef_\n</code></pre> <pre><code>(array([-0.68316926]), array([[0.71529828, 0.45433662]]))\n</code></pre> <pre><code>X[y==1] += 1.9;\nfig , ax = plt.subplots(figsize =(4 ,4))\nax.scatter(X[:,0], X[:,1], c=y);\n</code></pre> <pre><code>svc = SVC(C=10000, kernel='linear')\nsvc.fit(X, y)\nplot_svm(X, y, svc)\n</code></pre> <pre><code>X = rng.standard_normal ((200 , 2))\nX[:100] += 2\nX[100:150] -= 2\ny = np.array ([1]*150+[2]*50)\n</code></pre> <pre><code>fig , ax = plt.subplots(figsize =(8 ,8))\nax.scatter(X[:,0],X[:,1],c=y)\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x749907838aa0&gt;\n</code></pre> <pre><code>(X_train, X_test, y_train , y_test) = skm.train_test_split(X,y,test_size =0.5,random_state =0)\nsvm_rbf = SVC(kernel=\"rbf\", gamma=1, C=1)\nsvm_rbf.fit(X_train , y_train)\n</code></pre> <pre>SVC(C=1, gamma=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted<pre>SVC(C=1, gamma=1)</pre> <pre><code>fig , ax = plt.subplots(figsize =(8 ,8))\nplot_svm(X_train ,\ny_train ,\nsvm_rbf ,\nax=ax)\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize =(8 ,8))\nRocCurveDisplay.from_estimator(svm_rbf,X_train,y_train,name='Training ',color='r',ax=ax)\n</code></pre> <pre><code>&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7499075e0500&gt;\n</code></pre> <pre><code>rng = np.random.default_rng (123)\nX = np.vstack ([X, rng.standard_normal ((50, 2))])\ny = np.hstack ([y, [0]*50])\nX[y==0 ,1] += 2\nfig , ax = plt.subplots(figsize =(8 ,8))\nax.scatter(X[:,0], X[:,1], c=y);\n</code></pre> <pre><code>svm_rbf_3 = SVC(kernel=\"rbf\",\nC=10,\ngamma=1,\ndecision_function_shape='ovo');\nsvm_rbf_3.fit(X, y)\nfig , ax = plt.subplots(figsize =(8 ,8))\nplot_svm(X,\ny,\nsvm_rbf_3 ,\nax=ax)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"z_applied4_14_CV/","title":"z applied4 14 CV","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\n</code></pre> <pre><code>from ISLP import load_data, confusion_table\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n</code></pre> <pre><code>auto = load_data(\"Auto\")\nauto = auto.reset_index()\nauto\n</code></pre> name mpg cylinders displacement horsepower weight acceleration year origin 0 chevrolet chevelle malibu 18.0 8 307.0 130 3504 12.0 70 1 1 buick skylark 320 15.0 8 350.0 165 3693 11.5 70 1 2 plymouth satellite 18.0 8 318.0 150 3436 11.0 70 1 3 amc rebel sst 16.0 8 304.0 150 3433 12.0 70 1 4 ford torino 17.0 8 302.0 140 3449 10.5 70 1 ... ... ... ... ... ... ... ... ... ... 387 ford mustang gl 27.0 4 140.0 86 2790 15.6 82 1 388 vw pickup 44.0 4 97.0 52 2130 24.6 82 2 389 dodge rampage 32.0 4 135.0 84 2295 11.6 82 1 390 ford ranger 28.0 4 120.0 79 2625 18.6 82 1 391 chevy s-10 31.0 4 119.0 82 2720 19.4 82 1 <p>392 rows \u00d7 9 columns</p> <pre><code>auto['mpg01'] = np.where(auto['mpg'] &gt; auto['mpg'].median(), 1, 0)\n</code></pre> <pre><code>sns.pairplot(auto, hue='mpg01')\n</code></pre> <pre><code>&lt;seaborn.axisgrid.PairGrid at 0x796b727da720&gt;\n</code></pre> <p></p> <pre><code>features = ['displacement']\nX = auto[features]\ny = auto[\"mpg01\"]\nkf = KFold(n_splits=10, shuffle=True, random_state=1)\n# scaler = StandardScaler()\n# scaler.fit(auto[useful_features])\n# X = scaler.transform(auto[useful_features])\n</code></pre> <pre><code>for i in range(1, 10):\n    s = []\n    for train_index, test_index in kf.split(X):\n        X_train, y_train = X.loc[train_index], y.loc[train_index]\n        X_test, y_test = X.loc[test_index], y.loc[test_index]\n        model = make_pipeline(\n            PolynomialFeatures(degree=i, include_bias=False),\n            LogisticRegression(C=1)\n        )\n        model.fit(X_train, y_train)\n        s.append((model.predict(X_test) == y_test).mean())\n    print(i, (np.array(s)).mean())\n</code></pre> <pre><code>1 0.8900641025641025\n2 0.8900641025641025\n3 0.8951923076923076\n4 0.9028846153846155\n5 0.8876282051282051\n6 0.7150641025641026\n7 0.49967948717948724\n8 0.49967948717948724\n9 0.49967948717948724\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"}]}