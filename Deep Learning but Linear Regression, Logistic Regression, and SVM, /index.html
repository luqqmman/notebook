
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Portfolio and learning journal of a machine learning enthusiast">
      
      
        <meta name="author" content="Muhammad Luqman Hakim">
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Deep Learning but Linear Regression, Logistic Regression, and SVM - ML Journey — Muhammad Luqman Hakim</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deep-learning-but-linear-regression-logistic-regression-and-svm" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="ML Journey — Muhammad Luqman Hakim" class="md-header__button md-logo" aria-label="ML Journey — Muhammad Luqman Hakim" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ML Journey — Muhammad Luqman Hakim
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deep Learning but Linear Regression, Logistic Regression, and SVM
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="ML Journey — Muhammad Luqman Hakim" class="md-nav__button md-logo" aria-label="ML Journey — Muhammad Luqman Hakim" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    ML Journey — Muhammad Luqman Hakim
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="deep-learning-but-linear-regression-logistic-regression-and-svm">Deep Learning but Linear Regression, Logistic Regression, and SVM</h1>
<p>This time we will:
- build simple Linear Regression, Logistic Regression, and SVM but using neural network
- comparing the result from sklearn library and our own neural network version of those models
- add hidden layer as 'feature extraction' to see any improvement (and justify the title xd)</p>
<pre><code class="language-python">import pandas as pd
import numpy as np
from matplotlib.pyplot import subplots
from ISLP import load_data
</code></pre>
<pre><code class="language-python">import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from torch.optim import Adam, RMSprop, SGD
from torchmetrics import MeanSquaredError, R2Score
from torchmetrics.classification import BinaryAccuracy, MulticlassAccuracy
from torchinfo import summary

import pytorch_lightning as pl
from pytorch_lightning.trainer import Trainer
from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
</code></pre>
<pre><code class="language-python">import sklearn.model_selection as skm
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, accuracy_score, mean_absolute_error
</code></pre>
<h1 id="dataset-for-regression-and-classification">Dataset for regression and classification</h1>
<p>We will be using hitters dataset from ISLP library which about baseball player game statistic:
- regression: predict salary based on player statistic
- classification: predict if salary is above or below median</p>
<p>Preprocessing:
- remove null value
- handle categorical data with dummy variable
- it IMPORTANT to drop the first category because of perfect multicolinearity
- standarization</p>
<pre><code class="language-python">hitters = load_data('Hitters')
hitters.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AtBat</th>
      <th>Hits</th>
      <th>HmRun</th>
      <th>Runs</th>
      <th>RBI</th>
      <th>Walks</th>
      <th>Years</th>
      <th>CAtBat</th>
      <th>CHits</th>
      <th>CHmRun</th>
      <th>CRuns</th>
      <th>CRBI</th>
      <th>CWalks</th>
      <th>League</th>
      <th>Division</th>
      <th>PutOuts</th>
      <th>Assists</th>
      <th>Errors</th>
      <th>Salary</th>
      <th>NewLeague</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>293</td>
      <td>66</td>
      <td>1</td>
      <td>30</td>
      <td>29</td>
      <td>14</td>
      <td>1</td>
      <td>293</td>
      <td>66</td>
      <td>1</td>
      <td>30</td>
      <td>29</td>
      <td>14</td>
      <td>A</td>
      <td>E</td>
      <td>446</td>
      <td>33</td>
      <td>20</td>
      <td>NaN</td>
      <td>A</td>
    </tr>
    <tr>
      <th>1</th>
      <td>315</td>
      <td>81</td>
      <td>7</td>
      <td>24</td>
      <td>38</td>
      <td>39</td>
      <td>14</td>
      <td>3449</td>
      <td>835</td>
      <td>69</td>
      <td>321</td>
      <td>414</td>
      <td>375</td>
      <td>N</td>
      <td>W</td>
      <td>632</td>
      <td>43</td>
      <td>10</td>
      <td>475.0</td>
      <td>N</td>
    </tr>
    <tr>
      <th>2</th>
      <td>479</td>
      <td>130</td>
      <td>18</td>
      <td>66</td>
      <td>72</td>
      <td>76</td>
      <td>3</td>
      <td>1624</td>
      <td>457</td>
      <td>63</td>
      <td>224</td>
      <td>266</td>
      <td>263</td>
      <td>A</td>
      <td>W</td>
      <td>880</td>
      <td>82</td>
      <td>14</td>
      <td>480.0</td>
      <td>A</td>
    </tr>
    <tr>
      <th>3</th>
      <td>496</td>
      <td>141</td>
      <td>20</td>
      <td>65</td>
      <td>78</td>
      <td>37</td>
      <td>11</td>
      <td>5628</td>
      <td>1575</td>
      <td>225</td>
      <td>828</td>
      <td>838</td>
      <td>354</td>
      <td>N</td>
      <td>E</td>
      <td>200</td>
      <td>11</td>
      <td>3</td>
      <td>500.0</td>
      <td>N</td>
    </tr>
    <tr>
      <th>4</th>
      <td>321</td>
      <td>87</td>
      <td>10</td>
      <td>39</td>
      <td>42</td>
      <td>30</td>
      <td>2</td>
      <td>396</td>
      <td>101</td>
      <td>12</td>
      <td>48</td>
      <td>46</td>
      <td>33</td>
      <td>N</td>
      <td>E</td>
      <td>805</td>
      <td>40</td>
      <td>4</td>
      <td>91.5</td>
      <td>N</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">hitters = hitters.dropna()
hitters = pd.get_dummies(hitters, columns=['League', 'Division', 'NewLeague'], drop_first=True)
hitters.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AtBat</th>
      <th>Hits</th>
      <th>HmRun</th>
      <th>Runs</th>
      <th>RBI</th>
      <th>Walks</th>
      <th>Years</th>
      <th>CAtBat</th>
      <th>CHits</th>
      <th>CHmRun</th>
      <th>CRuns</th>
      <th>CRBI</th>
      <th>CWalks</th>
      <th>PutOuts</th>
      <th>Assists</th>
      <th>Errors</th>
      <th>Salary</th>
      <th>League_N</th>
      <th>Division_W</th>
      <th>NewLeague_N</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>315</td>
      <td>81</td>
      <td>7</td>
      <td>24</td>
      <td>38</td>
      <td>39</td>
      <td>14</td>
      <td>3449</td>
      <td>835</td>
      <td>69</td>
      <td>321</td>
      <td>414</td>
      <td>375</td>
      <td>632</td>
      <td>43</td>
      <td>10</td>
      <td>475.0</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <th>2</th>
      <td>479</td>
      <td>130</td>
      <td>18</td>
      <td>66</td>
      <td>72</td>
      <td>76</td>
      <td>3</td>
      <td>1624</td>
      <td>457</td>
      <td>63</td>
      <td>224</td>
      <td>266</td>
      <td>263</td>
      <td>880</td>
      <td>82</td>
      <td>14</td>
      <td>480.0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>3</th>
      <td>496</td>
      <td>141</td>
      <td>20</td>
      <td>65</td>
      <td>78</td>
      <td>37</td>
      <td>11</td>
      <td>5628</td>
      <td>1575</td>
      <td>225</td>
      <td>828</td>
      <td>838</td>
      <td>354</td>
      <td>200</td>
      <td>11</td>
      <td>3</td>
      <td>500.0</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>4</th>
      <td>321</td>
      <td>87</td>
      <td>10</td>
      <td>39</td>
      <td>42</td>
      <td>30</td>
      <td>2</td>
      <td>396</td>
      <td>101</td>
      <td>12</td>
      <td>48</td>
      <td>46</td>
      <td>33</td>
      <td>805</td>
      <td>40</td>
      <td>4</td>
      <td>91.5</td>
      <td>True</td>
      <td>False</td>
      <td>True</td>
    </tr>
    <tr>
      <th>5</th>
      <td>594</td>
      <td>169</td>
      <td>4</td>
      <td>74</td>
      <td>51</td>
      <td>35</td>
      <td>11</td>
      <td>4408</td>
      <td>1133</td>
      <td>19</td>
      <td>501</td>
      <td>336</td>
      <td>194</td>
      <td>282</td>
      <td>421</td>
      <td>25</td>
      <td>750.0</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="language-python">X_reg = hitters.drop(columns=[&quot;Salary&quot;])
y_reg = hitters[&quot;Salary&quot;]
X_train, X_test, y_train, y_test = skm.train_test_split(X_reg, y_reg, shuffle=True, random_state=0)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
</code></pre>
<h2 id="generic-model-for-our-task">Generic Model for our task</h2>
<p>We will use this model for our task, this is very straightforward model with only input layer and one output unit. The only main thing that we need to think about is the different loss function for Linear Regression, Logistic Regression, and SVM. There is also simple hidden layer with 32 unit that we will compare the performance later.</p>
<pre><code class="language-python">class Model(pl.LightningModule):
    def __init__(self, loss_fn, input_size, hidden_layer=False, lr=0.01):
        super().__init__()
        self.loss_fn = loss_fn
        self.input_size = input_size
        self.lr = lr
        if hidden_layer:
            self.model = nn.Sequential(
                nn.Linear(input_size, 32),
                nn.ReLU(),
                nn.Linear(32, 1)
            )
        else:
            self.model = nn.Sequential(
                nn.Linear(input_size, 1)
            )

    def forward(self, x):
        return self.model(x).squeeze()

    def _shared_step(self, batch, stage):
        x, y = batch
        preds = self(x)
        loss = self.loss_fn(preds, y.float())
        self.log(f'{stage}_loss', loss, on_epoch=True, on_step=False, prog_bar=True)
        return loss

    def training_step(self, batch, batch_idx):
        return self._shared_step(batch, 'train')

    def validation_step(self, batch, batch_idx):
        self._shared_step(batch, 'val')

    def predict_step(self, batch, batch_idx):
        x, _ = batch
        return self(x)

    def configure_optimizers(self):
        return SGD(self.parameters(), lr=self.lr)

class DataModule(pl.LightningDataModule):
    def __init__(self, train_td, val_td, batch_size=64, num_workers=8):
        super().__init__()
        self.train_td = train_td
        self.val_td = val_td
        self.batch_size = batch_size
        self.num_workers=num_workers

    def train_dataloader(self):
        return DataLoader(self.train_td, batch_size=self.batch_size, num_workers=self.num_workers)

    def val_dataloader(self):
        return DataLoader(self.val_td, batch_size=self.batch_size, num_workers=self.num_workers)    
</code></pre>
<h2 id="linear-regression-with-sklearn">Linear Regression with sklearn</h2>
<p>This is the default linear regression model from sklearn. It got MAE about 265 and 0.55 R2 score. For reference the std of our response is around 451 so it did pretty good with MAE almost half of std.</p>
<pre><code class="language-python">lr = LinearRegression()
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
mean_absolute_error(y_test, lr_pred), r2_score(y_test, lr_pred)
</code></pre>
<pre><code>(265.87464986111604, 0.5531784057871476)
</code></pre>
<h2 id="linear-regression-with-our-nn-model">Linear Regression with our NN model</h2>
<p>This is the our NN model which use MSELoss. The result is bad here, the validation and train loss get stuck at some point which adding hidden layer maybe helpful.</p>
<pre><code class="language-python">X_train_t = torch.tensor(X_train.astype(np.float32))
X_test_t = torch.tensor(X_test.astype(np.float32))
y_train_t = torch.tensor(y_train.to_numpy().astype(np.float32))
y_test_t = torch.tensor(y_test.to_numpy().astype(np.float32))

train_lr_ds = TensorDataset(X_train_t, y_train_t)
test_lr_ds = TensorDataset(X_test_t, y_test_t)
</code></pre>
<pre><code class="language-python">lr_loss_fn = nn.MSELoss()
# lr_logger = CSVLogger('logs', name='linear_regression')
lr_model = Model(lr_loss_fn, X_train.shape[1])
lr_dm = DataModule(train_lr_ds, test_lr_ds, batch_size=32)
lr_trainer = Trainer(max_epochs=100)
</code></pre>
<pre><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
</code></pre>
<pre><code class="language-python">lr_trainer.fit(lr_model, lr_dm)
</code></pre>
<pre><code>  | Name    | Type       | Params | Mode 
-----------------------------------------------
0 | loss_fn | MSELoss    | 0      | train
1 | model   | Sequential | 20     | train
-----------------------------------------------
20        Trainable params
0         Non-trainable params
20        Total params
0.000     Total estimated model params size (MB)
3         Modules in train mode
0         Modules in eval mode

/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

`Trainer.fit` stopped: `max_epochs=100` reached.
</code></pre>
<pre><code class="language-python">lr_dl_pred = np.concatenate(lr_trainer.predict(lr_model, lr_dm.val_dataloader()))
mean_absolute_error(y_test, lr_dl_pred), r2_score(y_test, lr_dl_pred)
</code></pre>
<pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]

(314.38981907700054, 0.4528384642986234)
</code></pre>
<h2 id="not-so-linear-regression-with-hidden-layer">Not so 'Linear' Regression with hidden layer</h2>
<p>We will try adding hidden layer here to compromise with bad result that we get previously. I learn about exploding gradient while running this, so i set the learning rate very low but with more epoch. Sometime the loss function get stuck around the same point loss value with our previous model. But this time the model escape that local minimum and get much better result than sklearn model at around 224 MAE and 0.68 R2 score.</p>
<pre><code class="language-python">lr_loss_fn = nn.MSELoss()
lr_model = Model(lr_loss_fn, X_train.shape[1], hidden_layer=True, lr=0.00005)
lr_dm = DataModule(train_lr_ds, test_lr_ds, batch_size=64)
lr_trainer = Trainer(max_epochs=300)
</code></pre>
<pre><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
</code></pre>
<pre><code class="language-python">lr_trainer.fit(lr_model, lr_dm)
</code></pre>
<pre><code>  | Name    | Type       | Params | Mode 
-----------------------------------------------
0 | loss_fn | MSELoss    | 0      | train
1 | model   | Sequential | 673    | train
-----------------------------------------------
673       Trainable params
0         Non-trainable params
673       Total params
0.003     Total estimated model params size (MB)
5         Modules in train mode
0         Modules in eval mode

`Trainer.fit` stopped: `max_epochs=300` reached.
</code></pre>
<pre><code class="language-python">lr_dlh_pred = np.concatenate(lr_trainer.predict(lr_model, lr_dm.val_dataloader()))
mean_absolute_error(y_test, lr_dlh_pred), r2_score(y_test, lr_dlh_pred)
</code></pre>
<pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]

(224.00030996264832, 0.6806740806956346)
</code></pre>
<h2 id="logistic-regression-with-sklearn">Logistic Regression with sklearn</h2>
<p>This time we will change the response to True or False, is the salary is higher or lower than median salary. The sklearn Logistic Regression model already giving a good start at around 81.8% accuracy.</p>
<pre><code class="language-python">X_cls = hitters.drop(columns=[&quot;Salary&quot;])
y_cls = hitters[&quot;Salary&quot;] &gt; hitters[&quot;Salary&quot;].median()
X_train, X_test, y_train, y_test = skm.train_test_split(X_cls, y_cls, shuffle=True, random_state=0)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
</code></pre>
<pre><code class="language-python">lg = LogisticRegression().fit(X_train, y_train)
lg_pred = lg.predict(X_test)
accuracy_score(y_test, lg_pred)
</code></pre>
<pre><code>0.8181818181818182
</code></pre>
<h2 id="logistic-regression-with-our-nn-model">Logistic Regression with our NN model</h2>
<p>We will try to recreate the idea of Logistic Regression model with our NN model. The key is to use this BCEWithLogitLoss which will do sigmoid for us and use binary cross entropy as loss function. We only provide the logits in output unit so to predict our test data we need to do sigmoid first to map the logit range to probability.</p>
<p>The result is a bit better with 83.3% accuracy</p>
<pre><code class="language-python">X_train_t = torch.tensor(X_train.astype(np.float32), dtype=torch.float32)
X_test_t = torch.tensor(X_test.astype(np.float32), dtype=torch.float32)
y_train_t = torch.tensor(y_train.to_numpy())
y_test_t = torch.tensor(y_test.to_numpy())

train_lg_ds = TensorDataset(X_train_t, y_train_t)
test_lg_ds = TensorDataset(X_test_t, y_test_t)
</code></pre>
<pre><code class="language-python">lg_loss_fn = nn.BCEWithLogitsLoss()
lg_model = Model(lg_loss_fn, X_train.shape[1], lr=0.01)
lg_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)
lg_trainer = Trainer(max_epochs=100)
</code></pre>
<pre><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
</code></pre>
<pre><code class="language-python">lg_trainer.fit(lg_model, lg_dm)
</code></pre>
<pre><code>  | Name    | Type              | Params | Mode 
------------------------------------------------------
0 | loss_fn | BCEWithLogitsLoss | 0      | train
1 | model   | Sequential        | 20     | train
------------------------------------------------------
20        Trainable params
0         Non-trainable params
20        Total params
0.000     Total estimated model params size (MB)
3         Modules in train mode
0         Modules in eval mode

/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

`Trainer.fit` stopped: `max_epochs=100` reached.
</code></pre>
<pre><code class="language-python">lg_dl_pred = np.concatenate(lg_trainer.predict(lg_model, lg_dm.val_dataloader()))
lg_dl_pred = torch.sigmoid(torch.tensor(lg_dl_pred)).numpy()
accuracy_score(y_test, np.where(lg_dl_pred&gt;0.5, 1, 0))
</code></pre>
<pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]

0.8333333333333334
</code></pre>
<h2 id="adding-hidden-layer-to-our-logistic-nn">Adding hidden layer to our Logistic NN</h2>
<p>We will add hidden layer this time and the result is the same as before. We will try to do another method for this classification task, that is using SVM</p>
<pre><code class="language-python">lg_loss_fn = nn.BCEWithLogitsLoss()
lg_metrics = BinaryAccuracy()
lg_logger = CSVLogger('logs', name='logistic_regression')
lg_model = Model(lg_loss_fn, X_train.shape[1], hidden_layer=True, lr=0.01)
lg_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)
lg_trainer = Trainer(max_epochs=100)
</code></pre>
<pre><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
</code></pre>
<pre><code class="language-python">lg_trainer.fit(lg_model, lg_dm)
</code></pre>
<pre><code>  | Name    | Type              | Params | Mode 
------------------------------------------------------
0 | loss_fn | BCEWithLogitsLoss | 0      | train
1 | model   | Sequential        | 673    | train
------------------------------------------------------
673       Trainable params
0         Non-trainable params
673       Total params
0.003     Total estimated model params size (MB)
5         Modules in train mode
0         Modules in eval mode

/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

`Trainer.fit` stopped: `max_epochs=100` reached.
</code></pre>
<pre><code class="language-python">lg_dl_pred = np.concatenate(lg_trainer.predict(lg_model, lg_dm.val_dataloader()))
lg_dl_pred = torch.sigmoid(torch.tensor(lg_dl_pred)).numpy()
accuracy_score(y_test, np.where(lg_dl_pred&gt;0.5, 1, 0))
</code></pre>
<pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]

0.8333333333333334
</code></pre>
<pre><code class="language-python">
</code></pre>
<h2 id="svm-with-sklearn">SVM with sklearn</h2>
<p>Logistic regression for this classification task result around 83%. This time we will take different approach that is using support vector machine to split the feature space to two different region. The sklearn linear SVM result around 80 accuracy.</p>
<pre><code class="language-python">svc = SVC(kernel='linear').fit(X_train, y_train)
svc_pred = svc.predict(X_test)
accuracy_score(y_test, np.where(svc_pred&gt;0.5, 1, 0))
</code></pre>
<pre><code>0.803030303030303
</code></pre>
<h2 id="svm-with-our-nn-model">SVM with our NN model</h2>
<p>We will use our NN to build SVM, but we need to define our own hinge loss as the loss function for this task. One thing that i learn is torch can do backprop even without their nn.loss function. The decision is the same with SMV, if the output is positive then it's above median and vice versa. The result without hidden layer is 81.8% accuracy.</p>
<pre><code class="language-python">def hinge_loss(outputs, labels):
    labels = labels.float()
    labels = labels * 2 - 1
    loss = torch.mean(torch.clamp(1 - outputs.view(-1) * labels, min=0))
    return loss
</code></pre>
<pre><code class="language-python">svm_loss_fn = hinge_loss
svm_model = Model(svm_loss_fn, X_train.shape[1], lr=0.01)
svm_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)
svm_trainer = Trainer(max_epochs=100)
</code></pre>
<pre><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
</code></pre>
<pre><code class="language-python">svm_trainer.fit(svm_model, svm_dm)
</code></pre>
<pre><code>  | Name  | Type       | Params | Mode 
---------------------------------------------
0 | model | Sequential | 20     | train
---------------------------------------------
20        Trainable params
0         Non-trainable params
20        Total params
0.000     Total estimated model params size (MB)
2         Modules in train mode
0         Modules in eval mode

/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

`Trainer.fit` stopped: `max_epochs=100` reached.
</code></pre>
<pre><code class="language-python">svm_dl_pred = np.concatenate(svm_trainer.predict(svm_model, svm_dm.val_dataloader()))
svm_dl_pred = torch.tensor(svm_dl_pred).numpy()
accuracy_score(y_test, np.where(svm_dl_pred&gt;0, 1, 0))
</code></pre>
<pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]

0.8181818181818182
</code></pre>
<h2 id="adding-hidden-layer-to-our-svm-model">Adding hidden layer to our SVM model</h2>
<p>Adding hidden layer in this case doesn't improve our model just like in logistic regression case. Overfitting may be the cause, as we dont do any regularization in our model</p>
<pre><code class="language-python">svm_loss_fn = hinge_loss
svm_model = Model(svm_loss_fn, X_train.shape[1], lr=0.01, hidden_layer=True)
svm_dm = DataModule(train_lg_ds, test_lg_ds, batch_size=32)
svm_trainer = Trainer(max_epochs=100)
</code></pre>
<pre><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
</code></pre>
<pre><code class="language-python">svm_trainer.fit(svm_model, svm_dm)
</code></pre>
<pre><code>  | Name  | Type       | Params | Mode 
---------------------------------------------
0 | model | Sequential | 673    | train
---------------------------------------------
673       Trainable params
0         Non-trainable params
673       Total params
0.003     Total estimated model params size (MB)
4         Modules in train mode
0         Modules in eval mode

/home/luqman/Lab/islp/venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.

`Trainer.fit` stopped: `max_epochs=100` reached.
</code></pre>
<pre><code class="language-python">svm_dl_pred = np.concatenate(svm_trainer.predict(svm_model, svm_dm.val_dataloader()))
svm_dl_pred = torch.tensor(svm_dl_pred).numpy()
accuracy_score(y_test, np.where(svm_dl_pred&gt;0, 1, 0))
</code></pre>
<pre><code>Predicting: |                                             | 0/? [00:00&lt;?, ?it/s]

0.803030303030303
</code></pre>
<pre><code class="language-python">
</code></pre>
<pre><code class="language-python">
</code></pre>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.tabs", "search.highlight", "content.code.annotate", "content.action.edit"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
    
  </body>
</html>